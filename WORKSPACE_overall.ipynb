{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36304898-43e6-4b7d-9419-99b8e4d79d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d7254-cf39-4818-a193-4ea0a62855c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "import wandb\n",
    "import optuna\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "from echo.src.base_objective import BaseObjective\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "\n",
    "from credit.seed import seed_everything\n",
    "from credit.loss import VariableTotalLoss2D\n",
    "from credit.data import ERA5Dataset, ERA5_and_Forcing_Dataset, Dataset_BridgeScaler\n",
    "from credit.transforms import load_transforms\n",
    "from credit.scheduler import load_scheduler, annealed_probability\n",
    "\n",
    "from credit.trainer import Trainer\n",
    "# <-------------- the new pipeline\n",
    "from credit.trainer_new import Trainer as Trainer_New\n",
    "\n",
    "from credit.metrics import LatWeightedMetrics\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.models import load_model\n",
    "from credit.models.checkpoint import (\n",
    "    FSDPOptimizerWrapper,\n",
    "    TorchFSDPCheckpointIO\n",
    ")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04417f3-3024-4288-963d-9f7b09e0c1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7d4819-c421-4a8f-9ddc-4258e50aff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/glade/u/home/ksha/miles-credit/config/example_for_data_checks.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64236dc9-0ab3-4bb5-a309-984d0fdc9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['trainer']['load_weights'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69d9cee-72fa-420a-970c-3a80001bab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_sampler_zscore_only(conf, all_ERA_files, surface_files, diagnostic_files, world_size, rank, is_train, seed=42):\n",
    "\n",
    "    # convert $USER to the actual user name\n",
    "    conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "    # ======================================================== #\n",
    "    # parse intputs\n",
    "    \n",
    "    # file names\n",
    "    varname_upper_air = conf['data']['variables']\n",
    "    \n",
    "    if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "        forcing_files = conf['data']['save_loc_forcing']\n",
    "        varname_forcing = conf['data']['forcing_variables']\n",
    "    else:\n",
    "        forcing_files = None\n",
    "        varname_forcing = None\n",
    "    \n",
    "    if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "        static_files = conf['data']['save_loc_static']\n",
    "        varname_static = conf['data']['static_variables']\n",
    "    else:\n",
    "        static_files = None\n",
    "        varname_static = None\n",
    "    \n",
    "    if surface_files is not None:\n",
    "        varname_surface = conf['data']['surface_variables']\n",
    "    else:\n",
    "        varname_surface = None\n",
    "        \n",
    "    if diagnostic_files is not None:\n",
    "        varname_diagnostic = conf['data']['diagnostic_variables']\n",
    "    else:\n",
    "        varname_diagnostic = None\n",
    "        \n",
    "    # number of previous lead time inputs\n",
    "    history_len = conf[\"data\"][\"history_len\"]\n",
    "    valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "    # number of lead times to forecast\n",
    "    forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "    valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "    \n",
    "    if is_train:\n",
    "        history_len = history_len\n",
    "        forecast_len = forecast_len\n",
    "        # print out training / validation\n",
    "        name = \"training\"\n",
    "    else:\n",
    "        history_len = valid_history_len\n",
    "        forecast_len = valid_forecast_len\n",
    "        name = 'validation'\n",
    "        \n",
    "    # max_forecast_len\n",
    "    if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "        max_forecast_len = None\n",
    "    else:\n",
    "        max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "    # skip_periods\n",
    "    if \"skip_periods\" not in conf[\"data\"]:\n",
    "        skip_periods = None\n",
    "    else:\n",
    "        skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "        \n",
    "    # one_shot\n",
    "    if \"one_shot\" not in conf[\"data\"]:\n",
    "        one_shot = None\n",
    "    else:\n",
    "        one_shot = conf[\"data\"][\"one_shot\"]\n",
    "\n",
    "    # shufle\n",
    "    shuffle = is_train\n",
    "    \n",
    "    # data preprocessing utils\n",
    "    transforms = load_transforms(conf)\n",
    "\n",
    "    # Z-score\n",
    "    dataset = ERA5_and_Forcing_Dataset(\n",
    "        varname_upper_air=varname_upper_air,\n",
    "        varname_surface=varname_surface,\n",
    "        varname_forcing=varname_forcing,\n",
    "        varname_static=varname_static,\n",
    "        varname_diagnostic=varname_diagnostic,\n",
    "        filenames=all_ERA_files,\n",
    "        filename_surface=surface_files,\n",
    "        filename_forcing=forcing_files,\n",
    "        filename_static=static_files,\n",
    "        filename_diagnostic=diagnostic_files,\n",
    "        history_len=history_len,\n",
    "        forecast_len=forecast_len,\n",
    "        skip_periods=skip_periods,\n",
    "        one_shot=one_shot,\n",
    "        max_forecast_len=max_forecast_len,\n",
    "        transform=transforms\n",
    "    )\n",
    "    \n",
    "    # Pytorch sampler\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        seed=seed,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    logging.info(f\" Loaded a {name} ERA dataset, and a distributed sampler (forecast length = {forecast_len + 1})\")\n",
    "\n",
    "    return dataset, sampler\n",
    "\n",
    "\n",
    "def load_model_states_and_optimizer(conf, model, device):\n",
    "\n",
    "    # convert $USER to the actual user name\n",
    "    conf['save_loc'] = save_loc = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "    # training hyperparameters\n",
    "    start_epoch = conf['trainer']['start_epoch']\n",
    "    learning_rate = float(conf['trainer']['learning_rate'])\n",
    "    weight_decay = float(conf['trainer']['weight_decay'])\n",
    "    amp = conf['trainer']['amp']\n",
    "\n",
    "    # load weights falg\n",
    "    load_weights = False if 'load_weights' not in conf['trainer'] else conf['trainer']['load_weights']\n",
    "\n",
    "    #  Load an optimizer, gradient scaler, and learning rate scheduler, the optimizer must come after wrapping model using FSDP\n",
    "    if start_epoch == 0 and not load_weights:  # Loaded after loading model weights when reloading\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
    "        if conf[\"trainer\"][\"mode\"] == \"fsdp\":\n",
    "            optimizer = FSDPOptimizerWrapper(optimizer, model)\n",
    "        scheduler = load_scheduler(optimizer, conf)\n",
    "        scaler = ShardedGradScaler(enabled=amp) if conf[\"trainer\"][\"mode\"] == \"fsdp\" else GradScaler(enabled=amp)\n",
    "\n",
    "    # load optimizer and grad scaler states\n",
    "    else:\n",
    "        ckpt = os.path.join(save_loc, \"checkpoint.pt\")\n",
    "        checkpoint = torch.load(ckpt, map_location=device)\n",
    "\n",
    "        # FSDP checkpoint settings\n",
    "        if conf[\"trainer\"][\"mode\"] == \"fsdp\":\n",
    "            logging.info(f\"Loading FSDP model, optimizer, grad scaler, and learning rate scheduler states from {save_loc}\")\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
    "            optimizer = FSDPOptimizerWrapper(optimizer, model)\n",
    "            checkpoint_io = TorchFSDPCheckpointIO()\n",
    "            checkpoint_io.load_unsharded_model(model, os.path.join(save_loc, \"model_checkpoint.pt\"))\n",
    "            if 'load_optimizer' in conf['trainer'] and conf['trainer']['load_optimizer']:\n",
    "                checkpoint_io.load_unsharded_optimizer(optimizer, os.path.join(save_loc, \"optimizer_checkpoint.pt\"))\n",
    "\n",
    "        else:\n",
    "            # DDP settings\n",
    "            if conf[\"trainer\"][\"mode\"] == \"ddp\":\n",
    "                logging.info(f\"Loading DDP model, optimizer, grad scaler, and learning rate scheduler states from {save_loc}\")\n",
    "                model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            else:\n",
    "                logging.info(f\"Loading model, optimizer, grad scaler, and learning rate scheduler states from {save_loc}\")\n",
    "                model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
    "            if 'load_optimizer' in conf['trainer'] and conf['trainer']['load_optimizer']:\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        scheduler = load_scheduler(optimizer, conf)\n",
    "        scaler = ShardedGradScaler(enabled=amp) if conf[\"trainer\"][\"mode\"] == \"fsdp\" else GradScaler(enabled=amp)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "    # Enable updating the lr if not using a policy\n",
    "    if (conf[\"trainer\"][\"update_learning_rate\"] if \"update_learning_rate\" in conf[\"trainer\"] else False):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    return model, optimizer, scheduler, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6e895-8c02-4c7d-9352-08de3a217227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b1eaff-c240-4918-ba6b-ac538210c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d3f4cb-f128-4186-9ff4-9c823992d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert $USER to the actual user name\n",
    "conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "    setup(rank, world_size, conf[\"trainer\"][\"mode\"])\n",
    "\n",
    "# infer device id from rank\n",
    "\n",
    "device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "# Config settings\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "train_batch_size = conf['trainer']['train_batch_size']\n",
    "valid_batch_size = conf['trainer']['valid_batch_size']\n",
    "thread_workers = conf['trainer']['thread_workers']\n",
    "valid_thread_workers = conf['trainer']['valid_thread_workers'] if 'valid_thread_workers' in conf['trainer'] else thread_workers\n",
    "\n",
    "# get file names\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# <------------------------------------------ std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "\n",
    "    if \"save_loc_surface\" in conf[\"data\"]:\n",
    "        surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "    else:\n",
    "        surface_files = None\n",
    "\n",
    "    if \"save_loc_diagnostic\" in conf[\"data\"]:\n",
    "        diagnostic_files = sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "    else:\n",
    "        diagnostic_files = None\n",
    "\n",
    "\n",
    "# -------------------------------------------------- #\n",
    "# import training / validation years from conf\n",
    "\n",
    "if 'train_years' in conf['data']:\n",
    "    train_years_range = conf['data']['train_years']\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if 'valid_years' in conf['data']:\n",
    "    valid_years_range = conf['data']['valid_years']\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [file for file in all_ERA_files if any(year in file for year in train_years)]\n",
    "valid_files = [file for file in all_ERA_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    train_surface_files = [file for file in surface_files if any(year in file for year in train_years)]\n",
    "    valid_surface_files = [file for file in surface_files if any(year in file for year in valid_years)]\n",
    "    \n",
    "    train_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in train_years)]\n",
    "    valid_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# load dataset and sampler\n",
    "# <----------------------------------- std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    # training set and sampler\n",
    "    train_dataset, train_sampler = load_dataset_and_sampler_zscore_only(conf, \n",
    "                                                                        train_files, \n",
    "                                                                        train_surface_files, \n",
    "                                                                        train_diagnostic_files, \n",
    "                                                                        world_size, rank, is_train=True)\n",
    "    # validation set and sampler\n",
    "    valid_dataset, valid_sampler = load_dataset_and_sampler_zscore_only(conf, \n",
    "                                                                        valid_files, \n",
    "                                                                        valid_surface_files, \n",
    "                                                                        valid_diagnostic_files,\n",
    "                                                                        world_size, rank, is_train=False)\n",
    "else:\n",
    "    train_dataset, train_sampler = load_dataset_and_sampler(conf, train_files, world_size, rank, is_train=True)\n",
    "    valid_dataset, valid_sampler = load_dataset_and_sampler(conf, valid_files, world_size, rank, is_train=False)\n",
    "\n",
    "# setup the dataloder for this process\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if thread_workers > 0 else False,\n",
    "    num_workers=thread_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=valid_sampler,\n",
    "    pin_memory=False,\n",
    "    num_workers=valid_thread_workers,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f895535-b74f-402d-addb-9d4aa9238ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1065ce23-4519-4575-93ca-494ce4c3ed2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'std_new'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['data']['scaler_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28e8ac-5fdd-4596-802f-7bea700428d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8308f38c-04c4-41d9-ad73-175734e01ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_model(conf)\n",
    "\n",
    "# have to send the module to the correct device first\n",
    "\n",
    "m.to(device)\n",
    "# m = torch.compile(m)\n",
    "\n",
    "# Wrap in DDP or FSDP module, or none\n",
    "\n",
    "model = distributed_model_wrapper(conf, m, device)\n",
    "\n",
    "# Load model weights (if any), an optimizer, scheduler, and gradient scaler\n",
    "\n",
    "model, optimizer, scheduler, scaler = load_model_states_and_optimizer(conf, model, device)\n",
    "\n",
    "# Train and validation losses\n",
    "\n",
    "train_criterion = VariableTotalLoss2D(conf)\n",
    "valid_criterion = VariableTotalLoss2D(conf, validation=True)\n",
    "\n",
    "# Optional load stopping probability annealer\n",
    "\n",
    "# Set up some metrics\n",
    "\n",
    "metrics = LatWeightedMetrics(conf)\n",
    "\n",
    "# Initialize a trainer object\n",
    "# <----------------------------------- replace\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    trainer = Trainer_New(model, rank, module=(conf[\"trainer\"][\"mode\"] == \"ddp\"))\n",
    "else:\n",
    "    trainer = Trainer(model, rank, module=(conf[\"trainer\"][\"mode\"] == \"ddp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e3f23-4681-41b4-965f-d135b2b182f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e708fd-7daf-433f-bfd7-da52cca550af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train_loss: 3.152208 train_acc: 0.250551 train_mae: 0.761776 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [01:14<00:00, 14.81s/it]\n",
      "Epoch: 0 valid_loss: 0.650981 valid_acc: 0.389236 valid_mae: 0.650981: 100%|██████████| 5/5 [00:25<00:00,  5.11s/it]\n",
      "Epoch: 1 train_loss: 1.667145 train_acc: 0.401188 train_mae: 0.635709 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:25<00:00,  5.02s/it]\n",
      "Epoch: 1 valid_loss: 0.651842 valid_acc: 0.394498 valid_mae: 0.651842: 100%|██████████| 5/5 [00:26<00:00,  5.39s/it]\n",
      "Epoch: 2 train_loss: 1.456796 train_acc: 0.411350 train_mae: 0.607168 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:28<00:00,  5.70s/it]\n",
      "Epoch: 2 valid_loss: 0.646985 valid_acc: 0.393887 valid_mae: 0.646985: 100%|██████████| 5/5 [00:24<00:00,  4.93s/it]\n",
      "Epoch: 3 train_loss: 1.324081 train_acc: 0.416623 train_mae: 0.594848 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:27<00:00,  5.48s/it]\n",
      "Epoch: 3 valid_loss: 0.642231 valid_acc: 0.390588 valid_mae: 0.642231: 100%|██████████| 5/5 [00:24<00:00,  4.91s/it]\n",
      "Epoch: 4 train_loss: 1.185916 train_acc: 0.419455 train_mae: 0.588465 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:24<00:00,  4.93s/it]\n",
      "Epoch: 4 valid_loss: 0.655386 valid_acc: 0.389987 valid_mae: 0.655386: 100%|██████████| 5/5 [00:24<00:00,  4.83s/it]\n",
      "Epoch: 5 train_loss: 1.147881 train_acc: 0.428035 train_mae: 0.593413 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:25<00:00,  5.08s/it]\n",
      "Epoch: 5 valid_loss: 0.625731 valid_acc: 0.405840 valid_mae: 0.625731: 100%|██████████| 5/5 [00:25<00:00,  5.01s/it]\n",
      "Epoch: 6 train_loss: 1.017160 train_acc: 0.435339 train_mae: 0.561509 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:23<00:00,  4.70s/it]\n",
      "Epoch: 6 valid_loss: 0.626377 valid_acc: 0.413805 valid_mae: 0.626377: 100%|██████████| 5/5 [00:24<00:00,  4.87s/it]\n",
      "Epoch: 7 train_loss: 1.070264 train_acc: 0.436131 train_mae: 0.580767 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:31<00:00,  6.31s/it]\n",
      "Epoch: 7 valid_loss: 0.609124 valid_acc: 0.417989 valid_mae: 0.609124: 100%|██████████| 5/5 [00:25<00:00,  5.08s/it]\n",
      "Epoch: 8 train_loss: 0.965542 train_acc: 0.450041 train_mae: 0.546554 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:27<00:00,  5.49s/it]\n",
      "Epoch: 8 valid_loss: 0.605697 valid_acc: 0.419683 valid_mae: 0.605697: 100%|██████████| 5/5 [00:24<00:00,  4.84s/it]\n",
      "Epoch: 9 train_loss: 0.964217 train_acc: 0.449212 train_mae: 0.556690 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:27<00:00,  5.40s/it]\n",
      "Epoch: 9 valid_loss: 0.600306 valid_acc: 0.423838 valid_mae: 0.600306: 100%|██████████| 5/5 [00:22<00:00,  4.57s/it]\n",
      "Epoch: 10 train_loss: 0.932522 train_acc: 0.451831 train_mae: 0.545402 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:26<00:00,  5.22s/it]\n",
      "Epoch: 10 valid_loss: 0.597413 valid_acc: 0.428753 valid_mae: 0.597413: 100%|██████████| 5/5 [00:23<00:00,  4.68s/it]\n",
      "Epoch: 11 train_loss: 0.965653 train_acc: 0.452807 train_mae: 0.556718 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:23<00:00,  4.64s/it]\n",
      "Epoch: 11 valid_loss: 0.594658 valid_acc: 0.431721 valid_mae: 0.594658: 100%|██████████| 5/5 [00:24<00:00,  4.98s/it]\n",
      "Epoch: 12 train_loss: 0.849527 train_acc: 0.475516 train_mae: 0.521922 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:28<00:00,  5.61s/it]\n",
      "Epoch: 12 valid_loss: 0.592370 valid_acc: 0.438649 valid_mae: 0.592370: 100%|██████████| 5/5 [00:23<00:00,  4.73s/it]\n",
      "Epoch: 13 train_loss: 0.875468 train_acc: 0.460424 train_mae: 0.538830 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:25<00:00,  5.06s/it]\n",
      "Epoch: 13 valid_loss: 0.588401 valid_acc: 0.448443 valid_mae: 0.588401: 100%|██████████| 5/5 [00:24<00:00,  4.86s/it]\n",
      "Epoch: 14 train_loss: 0.861512 train_acc: 0.479767 train_mae: 0.529936 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:27<00:00,  5.43s/it]\n",
      "Epoch: 14 valid_loss: 0.581192 valid_acc: 0.468318 valid_mae: 0.581192: 100%|██████████| 5/5 [00:26<00:00,  5.29s/it]\n",
      "Epoch: 15 train_loss: 0.885826 train_acc: 0.499873 train_mae: 0.540214 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:25<00:00,  5.07s/it]\n",
      "Epoch: 15 valid_loss: 0.575113 valid_acc: 0.482858 valid_mae: 0.575113: 100%|██████████| 5/5 [00:25<00:00,  5.12s/it]\n",
      "Epoch: 16 train_loss: 0.885591 train_acc: 0.503396 train_mae: 0.538823 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:24<00:00,  4.97s/it]\n",
      "Epoch: 16 valid_loss: 0.571639 valid_acc: 0.494851 valid_mae: 0.571639: 100%|██████████| 5/5 [00:24<00:00,  4.89s/it]\n",
      "Epoch: 17 train_loss: 0.821080 train_acc: 0.521203 train_mae: 0.506535 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:25<00:00,  5.05s/it]\n",
      "Epoch: 17 valid_loss: 0.568602 valid_acc: 0.500257 valid_mae: 0.568602: 100%|██████████| 5/5 [00:24<00:00,  4.89s/it]\n",
      "Epoch: 18 train_loss: 0.762350 train_acc: 0.526445 train_mae: 0.485733 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:23<00:00,  4.70s/it]\n",
      "Epoch: 18 valid_loss: 0.568471 valid_acc: 0.502369 valid_mae: 0.568471: 100%|██████████| 5/5 [00:23<00:00,  4.60s/it]\n",
      "Epoch: 19 train_loss: 0.890530 train_acc: 0.519657 train_mae: 0.536459 forecast_len 1.0 lr: 0.001000000000: 100%|██████████| 5/5 [00:22<00:00,  4.43s/it]\n",
      "Epoch: 19 valid_loss: 0.556929 valid_acc: 0.513405 valid_mae: 0.556929: 100%|██████████| 5/5 [00:22<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit(\n",
    "    conf,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    train_criterion,\n",
    "    valid_criterion,\n",
    "    scaler,\n",
    "    scheduler,\n",
    "    metrics,\n",
    "    rollout_scheduler=annealed_probability,\n",
    "    trial=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f676cf-19f9-4255-a087-c29f93d4d39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35964f8b-9fc1-44ad-930a-6ee553ea92d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
