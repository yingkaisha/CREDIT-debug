{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cfc3c8b-d30c-4c5a-bfd9-60bb9cc1b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- #\n",
    "# System\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import warnings\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ---------- #\n",
    "# Numerics\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------- #\n",
    "# AI libs\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "# import wandb\n",
    "\n",
    "# ---------- #\n",
    "# credit\n",
    "from credit.data import *\n",
    "from credit.transforms import load_transforms, Normalize_ERA5_and_Forcing\n",
    "from credit.seed import seed_everything\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.pol_lapdiff_filt import Diffusion_and_Pole_Filter\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "from credit.models.checkpoint import load_model_state\n",
    "from credit.solar import TOADataLoader\n",
    "from credit.output import split_and_reshape, load_metadata, make_xarray, save_netcdf_increment\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad35f680-67a1-4c8b-b741-765c58f499df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old rollout config\n",
    "config_name = '/glade/work/ksha/repos/global/miles-credit/results/wxformer_6h_rollout/model.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061d8918-d799-4535-b532-58693f6f4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447365de-cd75-47e8-83a1-10f1161f323b",
   "metadata": {},
   "source": [
    "## New rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07baada1-105f-42ab-ac39-c8fb00dcfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict_Dataset(torch.utils.data.IterableDataset):\n",
    "    '''\n",
    "    Same as ERA5_and_Forcing_Dataset() but for prediction only\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 conf, \n",
    "                 varname_upper_air,\n",
    "                 varname_surface,\n",
    "                 varname_forcing,\n",
    "                 varname_static,\n",
    "                 filenames,\n",
    "                 filename_surface,\n",
    "                 filename_forcing,\n",
    "                 filename_static,\n",
    "                 fcst_datetime,\n",
    "                 history_len,\n",
    "                 rank,\n",
    "                 world_size,\n",
    "                 transform=None,\n",
    "                 rollout_p=0.0,\n",
    "                 which_forecast=None):\n",
    "        \n",
    "        # ------------------------------------------------------------------------------ #\n",
    "        \n",
    "        ## no diagnostics because they are output only\n",
    "        varname_diagnostic = None\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.transform = transform\n",
    "        self.history_len = history_len\n",
    "        self.fcst_datetime = fcst_datetime\n",
    "        self.which_forecast = which_forecast # <-- got from the old roll-out. Dont know \n",
    "        \n",
    "        # -------------------------------------- #\n",
    "        self.filenames = sorted(filenames) # <---------------- a list of files\n",
    "        self.filename_surface = sorted(filename_surface) # <-- a list of files\n",
    "        self.filename_forcing = filename_forcing # <-- single file\n",
    "        self.filename_static = filename_static # <---- single file\n",
    "        \n",
    "        # -------------------------------------- #\n",
    "        self.varname_upper_air = varname_upper_air\n",
    "        self.varname_surface = varname_surface\n",
    "        self.varname_forcing = varname_forcing\n",
    "        self.varname_static = varname_static\n",
    "\n",
    "        # ====================================== #\n",
    "        # import all upper air zarr files\n",
    "        all_files = []\n",
    "        for fn in self.filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, self.varname_upper_air)\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "        self.all_files = all_files\n",
    "        # ====================================== #\n",
    "\n",
    "        # -------------------------------------- #\n",
    "        # other settings\n",
    "        self.current_epoch = 0\n",
    "        self.rollout_p = rollout_p\n",
    "        \n",
    "        if 'lead_time_periods' in conf['data']:\n",
    "            self.lead_time_periods = conf['data']['lead_time_periods']\n",
    "        else:\n",
    "            self.lead_time_periods = 1\n",
    "        \n",
    "        if 'skip_periods' in conf['data']:\n",
    "            self.skip_periods = conf['data']['skip_periods']\n",
    "        else:\n",
    "            self.skip_periods = 1\n",
    "            \n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "            \n",
    "\n",
    "    def ds_read_and_subset(self, filename, time_start, time_end, varnames):\n",
    "        sliced_x = xr.open_zarr(filename, consolidated=True)\n",
    "        sliced_x = sliced_x.isel(time=slice(time_start, time_end))\n",
    "        sliced_x = drop_var_from_dataset(sliced_x, varnames)\n",
    "        return sliced_x\n",
    "\n",
    "    def load_zarr_as_input(self, file_key, time_key):\n",
    "        # get the needed file from a list of zarr files\n",
    "        # open the zarr file as xr.dataset and subset based on the needed time\n",
    "        \n",
    "        # sliced_x: the final output, starts with an upper air xr.dataset\n",
    "        sliced_x = self.ds_read_and_subset(self.filenames[file_key], \n",
    "                                           time_key, \n",
    "                                           time_key+self.history_len+1, \n",
    "                                           self.varname_upper_air)\n",
    "        # surface variables\n",
    "        if self.varname_surface is not None:\n",
    "            sliced_surface = self.ds_read_and_subset(self.filename_surface[file_key], \n",
    "                                                     time_key, \n",
    "                                                     time_key+self.history_len+1, \n",
    "                                                     self.varname_surface)\n",
    "            # merge surface to sliced_x\n",
    "            sliced_surface['time'] = sliced_x['time']\n",
    "            sliced_x = sliced_x.merge(sliced_surface)\n",
    "            \n",
    "        # forcing / static\n",
    "        if self.filename_forcing is not None:\n",
    "            sliced_forcing = xr.open_dataset(self.filename_forcing)\n",
    "            sliced_forcing = drop_var_from_dataset(sliced_forcing, self.varname_forcing)\n",
    "\n",
    "            # See also `ERA5_and_Forcing_Dataset`\n",
    "            # =============================================================================== #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(np.array(sliced_forcing['time']))\n",
    "            month_day_inputs = extract_month_day_hour(np.array(sliced_x['time']))\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            sliced_forcing = sliced_forcing.isel(time=ind_forcing)\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            sliced_forcing['time'] = sliced_x['time']\n",
    "            # =============================================================================== #\n",
    "            \n",
    "            # merge forcing to sliced_x\n",
    "            sliced_x = sliced_x.merge(sliced_forcing)\n",
    "            \n",
    "        if self.filename_static is not None:\n",
    "            sliced_static = xr.open_dataset(self.filename_static)\n",
    "            sliced_static = drop_var_from_dataset(sliced_static, self.varname_static)\n",
    "            sliced_static = sliced_static.expand_dims(dim={\"time\": len(sliced_x['time'])})\n",
    "            sliced_static['time'] = sliced_x['time']\n",
    "            # merge static to sliced_x\n",
    "            sliced_x = sliced_x.merge(sliced_static)\n",
    "        return sliced_x\n",
    "\n",
    "    \n",
    "    def find_start_stop_indices(self, index):\n",
    "        # convert the first forecasted time to initialization time\n",
    "        # by subtracting the forecast length (assuming 1 step)\n",
    "        # other later forecasted time are viewed as init time directly \n",
    "        # becuase their previous step forecasted time are init times of the later forecasted time\n",
    "        start_time = self.fcst_datetime[index][0] # string\n",
    "        date_object = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        # =========================================================================== #\n",
    "        # <--- !! it MAY NOT work when self.skip_period != 1\n",
    "        shifted_hours = self.lead_time_periods * self.skip_periods * self.history_len\n",
    "        # =========================================================================== #\n",
    "        date_object = date_object - datetime.timedelta(hours=shifted_hours)\n",
    "        self.fcst_datetime[index][0] = date_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # convert all strings to np.datetime64\n",
    "        datetime_objs = [np.datetime64(date) for date in self.fcst_datetime[index]]\n",
    "        start_time, stop_time = [str(datetime_obj) + '.000000000' for datetime_obj in datetime_objs]\n",
    "        self.start_time = np.datetime64(start_time).astype(datetime.datetime)\n",
    "        self.stop_time = np.datetime64(stop_time).astype(datetime.datetime)\n",
    "\n",
    "        info = {}\n",
    "        for idx, dataset in enumerate(self.all_files):\n",
    "            start_time = np.datetime64(dataset['time'].min().values).astype(datetime.datetime)\n",
    "            stop_time = np.datetime64(dataset['time'].max().values).astype(datetime.datetime)\n",
    "            track_start = False\n",
    "            track_stop = False\n",
    "            if start_time <= self.start_time <= stop_time:\n",
    "                # Start time is in this file, use start time index\n",
    "                dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                start_idx = np.searchsorted(dataset, self.start_time)\n",
    "                start_idx = max(0, min(start_idx, len(dataset)-1))\n",
    "                track_start = True\n",
    "            elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                start_idx = 0\n",
    "                track_start = True\n",
    "\n",
    "            if start_time <= self.stop_time <= stop_time:\n",
    "                # Stop time is in this file, use stop time index\n",
    "                if isinstance(dataset, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                stop_idx = np.searchsorted(dataset, self.stop_time)\n",
    "                stop_idx = max(0, min(stop_idx, len(dataset)-1))\n",
    "                track_stop = True\n",
    "\n",
    "            elif start_time < self.stop_time and stop_time >= self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                stop_idx = len(dataset) - 1\n",
    "                track_stop = True\n",
    "\n",
    "            # Only include files that overlap the time range\n",
    "            if track_start and track_stop:\n",
    "                info[idx] = ((idx, start_idx), (idx, stop_idx))\n",
    "\n",
    "        indices = []\n",
    "        for dataset_idx, (start, stop) in info.items():\n",
    "            for i in range(start[1], stop[1]+1):\n",
    "                indices.append((start[0], i))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fcst_datetime)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(self, \n",
    "                                     num_replicas=num_workers*self.world_size, \n",
    "                                     rank=self.rank*num_workers+worker_id, \n",
    "                                     shuffle=False)\n",
    "        for index in sampler:\n",
    "            # get time indices for inputs\n",
    "            data_lookup = self.find_start_stop_indices(index)\n",
    "            for k, (file_key, time_key) in enumerate(data_lookup):\n",
    "                if k == 0:\n",
    "                    output_dict = {}\n",
    "                    # get all inputs (upper air, surface, forcing, static ) in one xr.Dataset\n",
    "                    sliced_x = self.load_zarr_as_input(file_key, time_key)\n",
    "                    \n",
    "                    # Check if additional data from the next file is needed\n",
    "                    if len(sliced_x['time']) < self.history_len + 1:\n",
    "                        \n",
    "                        # Load excess data from the next file\n",
    "                        next_file_idx = self.filenames.index(self.filenames[file_key]) + 1\n",
    "                        \n",
    "                        if next_file_idx == len(self.filenames):\n",
    "                            # not enough input data to support this forecast\n",
    "                            raise OSError(\"You have reached the end of the available data. Exiting.\")\n",
    "                            \n",
    "                        else:\n",
    "                            # time_key = 0 because we need the beginning of the next file only\n",
    "                            sliced_x_next = self.load_zarr_as_input(next_file_idx, 0)\n",
    "                            \n",
    "                            # Concatenate excess data from the next file with the current data\n",
    "                            sliced_x = xr.concat([sliced_x, sliced_x_next], dim='time')\n",
    "                            sliced_x = sliced_x.isel(time=slice(0, self.history_len+1))\n",
    "                                                     \n",
    "                    # key 'historical_ERA5_images' is recongnized as input in credit.transform\n",
    "                    sample_x = {\n",
    "                        'historical_ERA5_images': sliced_x.isel(time=slice(0, self.history_len))\n",
    "                    }\n",
    "                    \n",
    "                    if self.transform:\n",
    "                        sample_x = self.transform(sample_x)\n",
    "                        \n",
    "                    for key in sample_x.keys():\n",
    "                        output_dict[key] = sample_x[key]\n",
    "                        \n",
    "                    output_dict['forecast_hour'] = k + 1\n",
    "                    # Adjust stopping condition\n",
    "                    output_dict['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))\n",
    "                    output_dict['datetime'] = sliced_x.time.values.astype('datetime64[s]').astype(int)[-1]\n",
    "                    print(sliced_x['time'])\n",
    "                else:\n",
    "                    output_dict['forecast_hour'] = k + 1\n",
    "                     # Adjust stopping condition\n",
    "                    output_dict['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1)) \n",
    "                yield output_dict\n",
    "\n",
    "                if output_dict['stop_forecast']:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec89c07a-a572-444b-b97c-1e34cf0d144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rank and world size for GPU-based rollout\n",
    "if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "    setup(rank, world_size, conf[\"trainer\"][\"mode\"])\n",
    "\n",
    "# infer device id from rank\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\")\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# config settings\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "# number of input time frames \n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "\n",
    "# transform and ToTensor class\n",
    "transform = load_transforms(conf)\n",
    "if conf[\"data\"][\"scaler_type\"] == 'std_new':\n",
    "    state_transformer = Normalize_ERA5_and_Forcing(conf)\n",
    "else:\n",
    "    print('Scaler type {} not supported'.format(conf[\"data\"][\"scaler_type\"]))\n",
    "    raise\n",
    "# ----------------------------------------------------------------- #\n",
    "# parse varnames and save_locs from config\n",
    "if 'lead_time_periods' in conf['data']:\n",
    "    lead_time_periods = conf['data']['lead_time_periods']\n",
    "else:\n",
    "    lead_time_periods = 1\n",
    "\n",
    "## upper air variables\n",
    "all_ERA_files = sorted(glob(conf[\"data\"][\"save_loc\"]))\n",
    "varname_upper_air = conf['data']['variables']\n",
    "\n",
    "## surface variables\n",
    "if \"save_loc_surface\" in conf[\"data\"]:\n",
    "    surface_files = sorted(glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "    varname_surface = conf['data']['surface_variables']\n",
    "else:\n",
    "    surface_files = None\n",
    "    varname_surface = None \n",
    "\n",
    "## forcing variables\n",
    "if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "    forcing_files = conf['data']['save_loc_forcing']\n",
    "    varname_forcing = conf['data']['forcing_variables']\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "\n",
    "## static variables\n",
    "if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "    static_files = conf['data']['save_loc_static']\n",
    "    varname_static = conf['data']['static_variables']\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3f45d7d4-9c81-438e-8aed-568858113ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datetime(start_time, end_time, interval_hr):\n",
    "    # Define the time interval (e.g., every hour)\n",
    "    interval = datetime.timedelta(hours=interval_hr)\n",
    "    \n",
    "    # Generate the list of datetime objects\n",
    "    datetime_list = []\n",
    "    current_time = start_time\n",
    "    while current_time <= end_time:\n",
    "        datetime_list.append(current_time)\n",
    "        current_time += interval\n",
    "    return datetime_list\n",
    "\n",
    "\n",
    "def hour_to_nanoseconds(input_hr):\n",
    "    # hr * min_per_hr * sec_per_min * nanosec_per_sec\n",
    "    return input_hr*60 * 60 * 1000000000\n",
    "\n",
    "def nanoseconds_to_year(nanoseconds_value):\n",
    "    return np.datetime64(nanoseconds_value, 'ns').astype('datetime64[Y]').astype(int) + 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eb6f3877-b300-457e-a2ac-461ea304021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_read_and_subset(filename, time_start, time_end, varnames):\n",
    "    sliced_x = xr.open_zarr(filename, consolidated=True)\n",
    "    sliced_x = sliced_x.isel(time=slice(time_start, time_end))\n",
    "    sliced_x = drop_var_from_dataset(sliced_x, varnames)\n",
    "    return sliced_x\n",
    "\n",
    "filenames = all_ERA_files\n",
    "filename_surface = surface_files\n",
    "filename_forcing = forcing_files\n",
    "filename_static = static_files\n",
    "\n",
    "def load_zarr_as_input(i_file, i_init_start, i_init_end):\n",
    "    # get the needed file from a list of zarr files\n",
    "    # open the zarr file as xr.dataset and subset based on the needed time\n",
    "    \n",
    "    # sliced_x: the final output, starts with an upper air xr.dataset\n",
    "    sliced_x = ds_read_and_subset(filenames[i_file], \n",
    "                                       i_init_start, \n",
    "                                       i_init_end+1, \n",
    "                                       varname_upper_air)\n",
    "    # surface variables\n",
    "    if varname_surface is not None:\n",
    "        sliced_surface = ds_read_and_subset(filename_surface[i_file], \n",
    "                                                 i_init_start, \n",
    "                                                 i_init_end+1, \n",
    "                                                 varname_surface)\n",
    "        # merge surface to sliced_x\n",
    "        sliced_surface['time'] = sliced_x['time']\n",
    "        sliced_x = sliced_x.merge(sliced_surface)\n",
    "        \n",
    "    # forcing / static\n",
    "    if filename_forcing is not None:\n",
    "        sliced_forcing = xr.open_dataset(filename_forcing)\n",
    "        sliced_forcing = drop_var_from_dataset(sliced_forcing, varname_forcing)\n",
    "\n",
    "        # See also `ERA5_and_Forcing_Dataset`\n",
    "        # =============================================================================== #\n",
    "        # matching month, day, hour between forcing and upper air [time]\n",
    "        # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "        month_day_forcing = extract_month_day_hour(np.array(sliced_forcing['time']))\n",
    "        month_day_inputs = extract_month_day_hour(np.array(sliced_x['time']))\n",
    "        # indices to subset\n",
    "        ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "        sliced_forcing = sliced_forcing.isel(time=ind_forcing)\n",
    "        # forcing and upper air have different years but the same mon/day/hour\n",
    "        # safely replace forcing time with upper air time\n",
    "        sliced_forcing['time'] = sliced_x['time']\n",
    "        # =============================================================================== #\n",
    "        \n",
    "        # merge forcing to sliced_x\n",
    "        sliced_x = sliced_x.merge(sliced_forcing)\n",
    "        \n",
    "    if filename_static is not None:\n",
    "        sliced_static = xr.open_dataset(filename_static)\n",
    "        sliced_static = drop_var_from_dataset(sliced_static, varname_static)\n",
    "        sliced_static = sliced_static.expand_dims(dim={\"time\": len(sliced_x['time'])})\n",
    "        sliced_static['time'] = sliced_x['time']\n",
    "        # merge static to sliced_x\n",
    "        sliced_x = sliced_x.merge(sliced_static)\n",
    "    return sliced_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911a194e-0f86-407b-be57-2d50d6f8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time_periods = conf['data']['lead_time_periods']\n",
    "skip_periods = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe954278-00ca-4929-bb14-7469fe6b66e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2020-01-01 00:00:00', '2020-01-02 18:00:00'],\n",
       " ['2020-01-01 12:00:00', '2020-01-03 06:00:00']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_forecasts(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d54092a-5da2-40e1-8204-59694ad8d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for fn in all_ERA_files:\n",
    "    # drop variables if they are not in the config\n",
    "    xarray_dataset = get_forward_data(filename=fn)\n",
    "    xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "    # collect yearly datasets within a list\n",
    "    all_files.append(xarray_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7249f26-e1dc-4634-a535-20b00c2e2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_datetime = load_forecasts(conf)\n",
    "index = 0\n",
    "# ============================================================================ #\n",
    "# shift hours for history_len > 1, becuase more than one init times are needed\n",
    "# <--- !! it MAY NOT work when self.skip_period != 1\n",
    "shifted_hours = lead_time_periods * skip_periods * (history_len-1)\n",
    "# ============================================================================ #\n",
    "# subtrack shifted_hour form the 1st & last init times\n",
    "# convert to datetime object\n",
    "init_datetime[index][0] = datetime.datetime.strptime(\n",
    "    init_datetime[index][0], '%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=shifted_hours)\n",
    "init_datetime[index][1] = datetime.datetime.strptime(\n",
    "    init_datetime[index][1], '%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=shifted_hours)\n",
    "\n",
    "# convert the 1st & last init times to a list of init times\n",
    "init_datetime[index] = generate_datetime(init_datetime[index][0], init_datetime[index][1], lead_time_periods)\n",
    "# convert datetime obj to nanosecondes\n",
    "init_time_list_dt = [np.datetime64(date.strftime('%Y-%m-%d %H:%M:%S')) for date in init_datetime[index]]\n",
    "init_time_list_np = [np.datetime64(str(dt_obj) + '.000000000').astype(datetime.datetime) for dt_obj in init_time_list_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cfd73df5-1023-452f-8c8e-e9b71015ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_file, ds in enumerate(all_files):\n",
    "    # get the year of the current file\n",
    "    ds_year = int(np.datetime_as_string(ds['time'][0].values, unit='Y'))\n",
    "\n",
    "    # get the first and last years of init times\n",
    "    init_year0 = nanoseconds_to_year(init_time_list_np[0])\n",
    "    \n",
    "    # found the right yearly file\n",
    "    if init_year0 == ds_year:\n",
    "        \n",
    "        # convert ds['time'] to a list of nanosecondes\n",
    "        ds_time_list = [np.datetime64(ds_time.values).astype(datetime.datetime) for ds_time in ds['time']]\n",
    "        ds_start_time = ds_time_list[0]\n",
    "        ds_end_time = ds_time_list[-1]\n",
    "        \n",
    "        init_time_start = init_time_list_np[0]\n",
    "        # if initalization time is within this (yearly) xr.Dataset\n",
    "        if ds_start_time <= init_time_start <= ds_end_time:\n",
    "\n",
    "            # try getting the index of the first initalization time \n",
    "            i_init_start = ds_time_list.index(init_time_start)\n",
    "            \n",
    "            # for multiple init time inputs (history_len > 1), init_end is different for init_start\n",
    "            init_time_end = init_time_start + hour_to_nanoseconds(shifted_hours)\n",
    "\n",
    "            # see if init_time_end is alos in this file\n",
    "            if ds_start_time <= init_time_end <= ds_end_time:\n",
    "                \n",
    "                # try getting the index\n",
    "                i_init_end = ds_time_list.index(init_time_end)\n",
    "            else:\n",
    "                # this set of initalizations have crossed years\n",
    "                # get the last element of the current file\n",
    "                # we have anthoer section that checks additional input data\n",
    "                i_init_end = len(ds_time_list) - 1\n",
    "                \n",
    "            info = [i_file, i_init_start, i_init_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d0814914-410a-4d19-b1d6-fe343b3a9333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2020-01-01\n"
     ]
    }
   ],
   "source": [
    "data_lookup = info\n",
    "\n",
    "for k, _ in enumerate(init_time_list_np):\n",
    "    # the first initialization time: get initalization from data\n",
    "    if k == 0:\n",
    "        i_file, i_init_start, i_init_end = data_lookup\n",
    "        output_dict = {}\n",
    "        # get all inputs (upper air, surface, forcing, static ) in one xr.Dataset\n",
    "        sliced_x = load_zarr_as_input(i_file, i_init_start, i_init_end)\n",
    "        \n",
    "        # Check if additional data from the next file is needed\n",
    "        if len(sliced_x['time']) < history_len:\n",
    "            \n",
    "            # Load excess data from the next file\n",
    "            next_file_idx = filenames.index(filenames[i_file]) + 1\n",
    "            \n",
    "            if next_file_idx >= len(filenames):\n",
    "                # not enough input data to support this forecast\n",
    "                raise OSError(\"You have reached the end of the available data. Exiting.\")\n",
    "                \n",
    "            else:\n",
    "                # i_init_start = 0 because we need the beginning of the next file only\n",
    "                sliced_x_next = load_zarr_as_input(next_file_idx, 0, history_len)\n",
    "                \n",
    "                # Concatenate excess data from the next file with the current data\n",
    "                sliced_x = xr.concat([sliced_x, sliced_x_next], dim='time')\n",
    "                sliced_x = sliced_x.isel(time=slice(0, history_len))\n",
    "                                         \n",
    "        # key 'historical_ERA5_images' is recongnized as input in credit.transform\n",
    "        # if len(sliced_x['time']) > history_len:\n",
    "        #     sliced_x = sliced_x.isel(time=slice(0, history_len))\n",
    "        sample_x = {'historical_ERA5_images': sliced_x}\n",
    "        print(sliced_x['time'])\n",
    "        if transform:\n",
    "            sample_x = transform(sample_x)\n",
    "            \n",
    "        for key in sample_x.keys():\n",
    "            output_dict[key] = sample_x[key]\n",
    "\n",
    "        # <--- !! 'forecast_hour' is actually \"forecast_step\" but named by assuming hourly\n",
    "        output_dict['forecast_hour'] = k + 1 \n",
    "        # Adjust stopping condition\n",
    "        output_dict['stop_forecast'] = k == (len(init_time_list_np) - 1)\n",
    "        output_dict['datetime'] = sliced_x.time.values.astype('datetime64[s]').astype(int)[-1]\n",
    "        \n",
    "    # other later initialization time: the same initalization as in k=0, but add more forecast steps\n",
    "    else:\n",
    "        output_dict['forecast_hour'] = k + 1\n",
    "         # Adjust stopping condition\n",
    "        output_dict['stop_forecast'] = k == (len(init_time_list_np) - 1)\n",
    "    #yield output_dict\n",
    "    \n",
    "    if output_dict['stop_forecast']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "eda78e6c-d028-4995-b19b-e06d97556f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2020-01-01T00:00:00')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_x.time.values.astype('datetime64[s]')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fcc1be1a-77a5-4edb-b45e-34c24a48062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['forecast_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8e1d2676-e963-4114-928a-74481b70c278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_forcing_static', 'x_surf', 'x'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "727e9a5e-b64b-4165-ac94-b7c512f54c88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_x\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_x' is not defined"
     ]
    }
   ],
   "source": [
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d742d7cb-74f8-4356-bd3f-564d63dd0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283996800000000000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4172c20a-1413-4043-87fc-0288a00de2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec928d45-d304-4415-b3b1-dacd14ff8026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729e14b-b39c-4cb6-89e7-71c28c49c2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553d788-dd4d-43c8-ba92-78bcacab6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6e6bf8-21ab-40ad-a643-a4da048a4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------- #\\\n",
    "# get dataset\n",
    "dataset = Predict_Dataset(\n",
    "    conf, \n",
    "    varname_upper_air,\n",
    "    varname_surface,\n",
    "    varname_forcing,\n",
    "    varname_static,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    fcst_datetime=load_forecasts(conf),\n",
    "    history_len=history_len,\n",
    "    rank=rank,\n",
    "    world_size=world_size,\n",
    "    transform=transform,\n",
    "    rollout_p=0.0,\n",
    "    which_forecast=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15344cb-c7c7-44ff-aa20-9926e912145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2020-01-01 00:00:00', '2020-01-02 18:00:00'],\n",
       " ['2020-01-01 12:00:00', '2020-01-03 06:00:00']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_forecasts(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b64877-3327-4fb3-8caf-eff515abc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_next_new = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b581f27c-290d-4a8a-bdb6-b3171718a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the dataloder\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac59de8-9811-4554-9c6b-435b07252b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 2)> Size: 16B\n",
      "array(['2019-12-31T18:00:00.000000000', '2020-01-01T00:00:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 16B 2019-12-31T18:00:00 2020-01-01\n"
     ]
    }
   ],
   "source": [
    "torch_next_new = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4048b93f-a521-45e3-9d38-e4a693169126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(torch_next_new['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ec2b9e-c04e-4a15-bc4c-bf7c9dd4ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forcing is the last in the new dataset class\n",
    "# batch size = 1, history_len, vars, lat, lon\n",
    "TOA_new = torch_next_new['x_forcing_static'][0, 0, 0, ...].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3056eae6-a955-4fbe-9438-277bb0756a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the normalized TSI\n",
    "ds_forcing = xr.open_dataset('/glade/campaign/cisl/aiml/ksha/CREDIT/forcing_norm_6h.nc')\n",
    "TSI = np.array(ds_forcing['TSI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb39c171-191c-435b-9d15-328143f080a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(TSI[-1, ...] - TOA_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8911b6a7-977d-4ef3-8bba-c9b32798e8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;time&#x27; ()&gt; Size: 8B\n",
       "array(&#x27;2000-12-31T18:00:00.000000000&#x27;, dtype=&#x27;datetime64[ns]&#x27;)\n",
       "Coordinates:\n",
       "    time     datetime64[ns] 8B 2000-12-31T18:00:00</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'time'</div></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-ed685a69-8874-4aac-919c-5ac0f84420e1' class='xr-array-in' type='checkbox' checked><label for='section-ed685a69-8874-4aac-919c-5ac0f84420e1' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>2000-12-31T18:00:00</span></div><div class='xr-array-data'><pre>array(&#x27;2000-12-31T18:00:00.000000000&#x27;, dtype=&#x27;datetime64[ns]&#x27;)</pre></div></div></li><li class='xr-section-item'><input id='section-58608ae9-7f41-4938-932f-c2d6be31847e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-58608ae9-7f41-4938-932f-c2d6be31847e' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>time</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2000-12-31T18:00:00</div><input id='attrs-6e22bf18-b9b9-4bf3-a440-7a1315109ba7' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-6e22bf18-b9b9-4bf3-a440-7a1315109ba7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f580f978-6b43-42ac-ad46-fe535568b07f' class='xr-var-data-in' type='checkbox'><label for='data-f580f978-6b43-42ac-ad46-fe535568b07f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(&#x27;2000-12-31T18:00:00.000000000&#x27;, dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-e713b7db-fa8f-4eea-8082-41352b40b5af' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e713b7db-fa8f-4eea-8082-41352b40b5af' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-a3f5429b-41f6-4bda-98c6-f56b1a0a22cf' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-a3f5429b-41f6-4bda-98c6-f56b1a0a22cf' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'time' ()> Size: 8B\n",
       "array('2000-12-31T18:00:00.000000000', dtype='datetime64[ns]')\n",
       "Coordinates:\n",
       "    time     datetime64[ns] 8B 2000-12-31T18:00:00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_forcing['time'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a37499d9-16d9-4e07-b196-0e98b44a3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lat/lons from x-array\n",
    "latlons = xr.open_dataset(conf[\"loss\"][\"latitude_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a9046-ba8f-464d-9a85-30813a0e2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_count = 0\n",
    "\n",
    "# y_pred allocation\n",
    "y_pred = None\n",
    "static = None\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01dce604-bdc0-4826-9072-3fa87fb02c71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for k, batch in enumerate(data_loader):\n",
    "    # get the datetime and forecasted hours\n",
    "    date_time = batch[\"datetime\"].item()\n",
    "    forecast_hour = batch[\"forecast_hour\"].item()\n",
    "\n",
    "    if forecast_hour == 1:\n",
    "        # Initialize x and x_surf with the first time step\n",
    "        if \"x_surf\" in batch:\n",
    "            # combine x and x_surf\n",
    "            # input: (batch_num, time, var, level, lat, lon), (batch_num, time, var, lat, lon) \n",
    "            # output: (batch_num, var, time, lat, lon), 'x' first and then 'x_surf'\n",
    "            x = concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(device).float()\n",
    "        else:\n",
    "            # no x_surf\n",
    "            x = reshape_only(batch[\"x\"]).to(device).float()\n",
    "\n",
    "        init_datetime_str = datetime.utcfromtimestamp(date_time)\n",
    "        init_datetime_str = init_datetime_str.strftime('%Y-%m-%dT%HZ')\n",
    "\n",
    "\n",
    "        # -------------------------------------------------------------------------------------- #\n",
    "        # add forcing and static variables (regardless of fcst hours)\n",
    "        if 'x_forcing_static' in batch:\n",
    "            \n",
    "            # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "            x_forcing_batch = batch['x_forcing_static'].to(device).permute(0, 2, 1, 3, 4).float()\n",
    "\n",
    "            # concat on var dimension\n",
    "            x = torch.cat((x, x_forcing_batch), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29911f31-bc3d-4bfa-8dd4-aebdf387729b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358bb29-7e04-4d33-a139-050caa8cd88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281d3da-e2c2-46f2-9ba9-82dc368e585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Rollout\n",
    "    with torch.no_grad():\n",
    "        # forecast count = a constant for each run\n",
    "\n",
    "    \n",
    "        # model inference loop\n",
    "        for k, batch in enumerate(data_loader):\n",
    "    \n",
    "\n",
    "            # initialization on the first forecast hour\n",
    "            if forecast_hour == 1:\n",
    "                \n",
    "                # Initialize x and x_surf with the first time step\n",
    "                if \"x_surf\" in batch:\n",
    "                    # combine x and x_surf\n",
    "                    # input: (batch_num, time, var, level, lat, lon), (batch_num, time, var, lat, lon) \n",
    "                    # output: (batch_num, var, time, lat, lon), 'x' first and then 'x_surf'\n",
    "                    x = concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(device).float()\n",
    "                else:\n",
    "                    # no x_surf\n",
    "                    x = reshape_only(batch[\"x\"]).to(device).float()\n",
    "\n",
    "                init_datetime_str = datetime.utcfromtimestamp(date_time)\n",
    "                init_datetime_str = init_datetime_str.strftime('%Y-%m-%dT%HZ')\n",
    "\n",
    "            # -------------------------------------------------------------------------------------- #\n",
    "            # add forcing and static variables (regardless of fcst hours)\n",
    "            if 'x_forcing_static' in batch:\n",
    "                \n",
    "                # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "                x_forcing_batch = batch['x_forcing_static'].to(device).permute(0, 2, 1, 3, 4).float()\n",
    "\n",
    "                # concat on var dimension\n",
    "                x = torch.cat((x, x_forcing_batch), dim=1)\n",
    "\n",
    "            # -------------------------------------------------------------------------------------- #\n",
    "            # start prediction\n",
    "            y_pred = model(x)\n",
    "            y_pred = state_transformer.inverse_transform(y_pred.cpu())\n",
    "            \n",
    "            if (\"use_laplace_filter\" in conf[\"predict\"] and conf[\"predict\"][\"use_laplace_filter\"]):\n",
    "                y_pred = (\n",
    "                    dpf.diff_lap2d_filt(y_pred.to(device).squeeze())\n",
    "                    .unsqueeze(0)\n",
    "                    .unsqueeze(2)\n",
    "                    .cpu()\n",
    "                )\n",
    "    \n",
    "            # Save the current forecast hour data in parallel\n",
    "            utc_datetime = datetime.utcfromtimestamp(date_time) + timedelta(hours=lead_time_periods*forecast_hour)\n",
    "    \n",
    "            # convert the current step result as x-array\n",
    "            darray_upper_air, darray_single_level = make_xarray(\n",
    "                y_pred,\n",
    "                utc_datetime,\n",
    "                latlons.latitude.values,\n",
    "                latlons.longitude.values,\n",
    "                conf,\n",
    "            )\n",
    "            \n",
    "            # Save the current forecast hour data in parallel\n",
    "            result = p.apply_async(\n",
    "                save_netcdf_increment,\n",
    "                (\n",
    "                    darray_upper_air, \n",
    "                     darray_single_level, \n",
    "                     init_datetime_str, \n",
    "                     lead_time_periods*forecast_hour, \n",
    "                     meta_data, \n",
    "                     conf\n",
    "                )\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update the input\n",
    "            # setup for next iteration, transform to z-space and send to device\n",
    "            y_pred = state_transformer.transform_array(y_pred).to(device)\n",
    "    \n",
    "            if history_len == 1:\n",
    "                x = y_pred.detach()\n",
    "            else:\n",
    "                # use multiple past forecast steps as inputs\n",
    "                # static channels will get updated on next pass\n",
    "                static_dim_size = abs(x.shape[1] - y_pred.shape[1])\n",
    "                \n",
    "                # if static_dim_size=0 then :0 gives empty range\n",
    "                x_detach = x[:, :-static_dim_size, 1:].detach() if static_dim_size else x[:, :, 1:].detach()  \n",
    "                x = torch.cat([x_detach, y_pred.detach()], dim=2)\n",
    "    \n",
    "            # Explicitly release GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "            if batch[\"stop_forecast\"][0]:\n",
    "                # Wait for all processes to finish in order\n",
    "                for result in results:\n",
    "                    result.get()\n",
    "    \n",
    "                # Now merge all the files into one and delete leftovers\n",
    "                # merge_netcdf_files(init_datetime_str, conf)\n",
    "    \n",
    "                # forecast count = a constant for each run\n",
    "                forecast_count += 1\n",
    "    \n",
    "                # update lists\n",
    "                results = []\n",
    "    \n",
    "                # y_pred allocation\n",
    "                y_pred = None\n",
    "    \n",
    "                gc.collect()\n",
    "    \n",
    "                if distributed:\n",
    "                    torch.distributed.barrier()\n",
    "    \n",
    "    if distributed:\n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfe66b-4672-4cba-b692-fd800fa92e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5921a64-8766-4d94-9edb-07db9225d9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d497f02-1d91-46a1-bc10-dabcae1b16dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45150b09-6026-456d-8ba9-bc4ddd6f1c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6bc5e-7caf-421e-a73f-3248123a4534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268967d-3eb8-49f3-bf0c-85e24565c61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ef12d-8ff9-4e41-96f3-e372e474b0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4539a5-4dbb-4d34-8b6e-9916b5e423b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
