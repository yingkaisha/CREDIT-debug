{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb99dd3-1f7c-403f-ac16-f312ba51ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- #\n",
    "# System\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import warnings\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ---------- #\n",
    "# Numerics\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------- #\n",
    "# AI libs\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "# import wandb\n",
    "\n",
    "# ---------- #\n",
    "# credit\n",
    "from credit.data import ERA5Dataset, ERA5_and_Forcing_Dataset, get_forward_data, concat_and_reshape, drop_var_from_dataset\n",
    "from credit.models import load_model\n",
    "from credit.transforms import load_transforms\n",
    "from credit.seed import seed_everything\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.pol_lapdiff_filt import Diffusion_and_Pole_Filter\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "from credit.models.checkpoint import load_model_state\n",
    "from credit.solar import TOADataLoader\n",
    "from credit.output import split_and_reshape, load_metadata, make_xarray, save_netcdf_increment\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5d0f4f-f072-4eb1-9e11-a8cd7fbc201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/glade/u/home/ksha/miles-credit/results/fuxi_norm/model.yml'\n",
    "#config_name = '/glade/work/ksha/repos/global/miles-credit/results/fuxi_6h/model_rollout.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702a45fc-8dd3-49f8-88f5-6c0f0ad110ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictForecast(torch.utils.data.IterableDataset):\n",
    "    def __init__(self,\n",
    "                 filenames,\n",
    "                 forecasts,\n",
    "                 history_len,\n",
    "                 skip_periods,\n",
    "                 rank,\n",
    "                 world_size,\n",
    "                 shuffle=False,\n",
    "                 transform=None,\n",
    "                 rollout_p=0.0,\n",
    "                 which_forecast=None):\n",
    "\n",
    "        self.dataset = ERA5Dataset(\n",
    "            filenames=filenames,\n",
    "            history_len=history_len,\n",
    "            forecast_len=1,\n",
    "            skip_periods=skip_periods,\n",
    "            transform=transform\n",
    "        )\n",
    "        self.meta_data_dict = self.dataset.meta_data_dict\n",
    "        self.all_files = self.dataset.all_fils\n",
    "        self.history_len = history_len\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.skip_periods = skip_periods\n",
    "        self.current_epoch = 0\n",
    "        self.rollout_p = rollout_p\n",
    "        self.forecasts = forecasts\n",
    "        self.skip_periods = skip_periods if skip_periods is not None else 1\n",
    "        self.which_forecast = which_forecast\n",
    "\n",
    "    def find_start_stop_indices(self, index):\n",
    "        start_time = self.forecasts[index][0]\n",
    "        date_object = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        shifted_hours = self.skip_periods * self.history_len\n",
    "        date_object = date_object - datetime.timedelta(hours=shifted_hours)\n",
    "        self.forecasts[index][0] = date_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        datetime_objs = [np.datetime64(date) for date in self.forecasts[index]]\n",
    "        start_time, stop_time = [str(datetime_obj) + '.000000000' for datetime_obj in datetime_objs]\n",
    "        self.start_time = np.datetime64(start_time).astype(datetime.datetime)\n",
    "        self.stop_time = np.datetime64(stop_time).astype(datetime.datetime)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        for idx, dataset in enumerate(self.all_files):\n",
    "            start_time = np.datetime64(dataset['time'].min().values).astype(datetime.datetime)\n",
    "            stop_time = np.datetime64(dataset['time'].max().values).astype(datetime.datetime)\n",
    "            track_start = False\n",
    "            track_stop = False\n",
    "\n",
    "            if start_time <= self.start_time <= stop_time:\n",
    "                # Start time is in this file, use start time index\n",
    "                dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                start_idx = np.searchsorted(dataset, self.start_time)\n",
    "                start_idx = max(0, min(start_idx, len(dataset)-1))\n",
    "                track_start = True\n",
    "\n",
    "            elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                start_idx = 0\n",
    "                track_start = True\n",
    "\n",
    "            if start_time <= self.stop_time <= stop_time:\n",
    "                # Stop time is in this file, use stop time index\n",
    "                if isinstance(dataset, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                stop_idx = np.searchsorted(dataset, self.stop_time)\n",
    "                stop_idx = max(0, min(stop_idx, len(dataset)-1))\n",
    "                track_stop = True\n",
    "\n",
    "            elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                stop_idx = len(dataset) - 1\n",
    "                track_stop = True\n",
    "\n",
    "            # Only include files that overlap the time range\n",
    "            if track_start and track_stop:\n",
    "                info[idx] = ((idx, start_idx), (idx, stop_idx))\n",
    "\n",
    "        indices = []\n",
    "        for dataset_idx, (start, stop) in info.items():\n",
    "            for i in range(start[1], stop[1]+1):\n",
    "                indices.append((start[0], i))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.forecasts)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers*self.world_size, rank=self.rank*num_workers+worker_id, shuffle=self.shuffle)\n",
    "\n",
    "        for index in sampler:\n",
    "\n",
    "            data_lookup = self.find_start_stop_indices(index)\n",
    "\n",
    "            for k, (file_key, time_key) in enumerate(data_lookup):\n",
    "\n",
    "                if k == 0:\n",
    "                    concatenated_samples = {'x': [], 'x_surf': []}\n",
    "                    sliced_x = xr.open_zarr(self.filenames[file_key], consolidated=True).isel(time=slice(time_key, time_key+self.history_len+1))\n",
    "\n",
    "                    # Check if additional data from the next file is needed\n",
    "                    if len(sliced_x['time']) < self.history_len + 1:\n",
    "                        # Load excess data from the next file\n",
    "                        next_file_idx = self.filenames.index(self.filenames[file_key]) + 1\n",
    "                        if next_file_idx == len(self.filenames):\n",
    "                            raise OSError(\"You have reached the end of the available data. Exiting.\")\n",
    "                        sliced_x_next = xr.open_zarr(\n",
    "                            self.filenames[next_file_idx],\n",
    "                            consolidated=True).isel(time=slice(0, self.history_len+1-len(sliced_x['time'])))\n",
    "\n",
    "                        # Concatenate excess data from the next file with the current data\n",
    "                        sliced_x = xr.concat([sliced_x, sliced_x_next], dim='time')\n",
    "\n",
    "                    sample_x = {\n",
    "                        'x': sliced_x.isel(time=slice(0, self.history_len))\n",
    "                    }\n",
    "\n",
    "                    if self.transform:\n",
    "                        sample_x = self.transform(sample_x)\n",
    "                        # Add static vars, if any, to the return dictionary\n",
    "                        if \"static\" in sample_x:\n",
    "                            concatenated_samples[\"static\"] = []\n",
    "                        if \"TOA\" in sample_x:\n",
    "                            concatenated_samples[\"TOA\"] = []\n",
    "\n",
    "                    for key in concatenated_samples.keys():\n",
    "                        concatenated_samples[key] = sample_x[key].squeeze(0) if self.history_len == 1 else sample_x[key]\n",
    "\n",
    "                    concatenated_samples['forecast_hour'] = k + 1\n",
    "                    concatenated_samples['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))  # Adjust stopping condition\n",
    "                    concatenated_samples['datetime'] = sliced_x.time.values.astype('datetime64[s]').astype(int)[-1]\n",
    "\n",
    "                else:\n",
    "                    concatenated_samples['forecast_hour'] = k + 1\n",
    "                    concatenated_samples['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))  # Adjust stopping condition\n",
    "\n",
    "                yield concatenated_samples\n",
    "\n",
    "                if concatenated_samples['stop_forecast']:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623cff9b-ec0f-4682-81fb-b8376a1c1115",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4960156b-8c62-407c-af14-758d829003b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "    setup(rank, world_size, conf[\"trainer\"][\"mode\"])\n",
    "\n",
    "# infer device id from rank\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\")\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Config settings\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "time_step = conf[\"data\"][\"time_step\"] if \"time_step\" in conf[\"data\"] else None\n",
    "\n",
    "# Load paths to all ERA5 data available\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# <--- !! works for 'std_new' only\n",
    "transform = load_transforms(conf)\n",
    "\n",
    "dataset = PredictForecast(\n",
    "    filenames=all_ERA_files,\n",
    "    forecasts=load_forecasts(conf),\n",
    "    history_len=history_len,\n",
    "    skip_periods=time_step,\n",
    "    transform=transform,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bca21b8-0610-49f5-ae9c-0870868827b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2018-06-01 00:00:00', '2018-06-01 02:00:00']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_forecasts(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887d5d90-6cfc-4b50-bb56-3bb2b60d99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab34edc6-f6df-4c01-abf9-1913ac96925f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1052, -0.1064, -0.1076,  ..., -0.1018, -0.1028, -0.1040],\n",
       "        [-0.1237, -0.1249, -0.1261,  ..., -0.1201, -0.1213, -0.1225],\n",
       "        [-0.1413, -0.1425, -0.1437,  ..., -0.1378, -0.1390, -0.1402],\n",
       "        ...,\n",
       "        [-1.2384, -1.2350, -1.2317,  ..., -1.2483, -1.2450, -1.2417],\n",
       "        [-1.2458, -1.2425, -1.2394,  ..., -1.2554, -1.2522, -1.2490],\n",
       "        [-1.2406, -1.2375, -1.2344,  ..., -1.2496, -1.2466, -1.2436]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x'][0, 0, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035801aa-9f4a-4c64-8bd3-7afff098e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the dataloder for this process\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9237fa15-4bb9-4632-9ff1-e88b27a7ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n"
     ]
    }
   ],
   "source": [
    "for test in data_loader:\n",
    "    print(test['x'][0, 0, 0, 0, 320, 640])\n",
    "    print('-----------')\n",
    "    print(test['x_surf'][0, 1, 6, 320, 640])\n",
    "    print('-----------')\n",
    "    print(test['TOA'][0, 0, 320, 640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d4402-5bf6-4176-987c-27b8389727b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
