{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38751e61-33e1-48d3-9875-dce03b812741",
   "metadata": {},
   "source": [
    "# Data loader workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7435be5-66b7-447b-ba1b-e80d2b835385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "import wandb\n",
    "import optuna\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "from echo.src.base_objective import BaseObjective\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "\n",
    "from credit.seed import seed_everything\n",
    "from credit.loss import VariableTotalLoss2D\n",
    "from credit.data import ERA5Dataset, ERA5_and_Forcing_Dataset, Dataset_BridgeScaler\n",
    "from credit.transforms import load_transforms\n",
    "from credit.scheduler import load_scheduler, annealed_probability\n",
    "from credit.trainer import Trainer\n",
    "from credit.metrics import LatWeightedMetrics\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.models import load_model\n",
    "from credit.models.checkpoint import (\n",
    "    FSDPOptimizerWrapper,\n",
    "    TorchFSDPCheckpointIO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49987e0a-8dcf-4202-8691-3552341fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785166b9-8673-4c1d-b5dd-ed8aecbc263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "os.environ['NCCL_SHM_DISABLE'] = '1'\n",
    "os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab33ce14-334c-47e0-b980-8f99ddeb9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_name = '/glade/u/home/ksha/miles-credit/config/fuxi_baseline_ksha_cpu.yml' #\n",
    "#config_name = '/glade/u/home/ksha/miles-credit/config/example_for_data_checks.yml'\n",
    "config_name = '/glade/work/ksha/CREDIT_runs/diag_o_tcw/model.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f457e9-7a04-426f-b12d-d6a7ed0561d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f2eea09-49a7-46b3-8405-a1b83340c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size, mode):\n",
    "    logging.info(f\"Running {mode.upper()} on rank {rank} with world_size {world_size}.\")\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06e49705-2de7-49fa-af10-dc8c86def756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_sampler(conf, files, world_size, rank, is_train, seed=42):\n",
    "\n",
    "    # convert $USER to the actual user name\n",
    "    conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "    # number of previous lead time inputs\n",
    "    history_len = conf[\"data\"][\"history_len\"]\n",
    "    valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "    history_len = history_len if is_train else valid_history_len\n",
    "\n",
    "    # number of lead times to forecast\n",
    "    forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "    valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "    forecast_len = forecast_len if is_train else valid_forecast_len\n",
    "\n",
    "    # optional setting: max_forecast_len\n",
    "    max_forecast_len = None if \"max_forecast_len\" not in conf[\"data\"] else conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "    # optional setting: skip_periods\n",
    "    skip_periods = None if \"skip_periods\" not in conf[\"data\"] else conf[\"data\"][\"skip_periods\"]\n",
    "\n",
    "    # optional setting: one_shot\n",
    "    one_shot = None if \"one_shot\" not in conf[\"data\"] else conf[\"data\"][\"one_shot\"]\n",
    "\n",
    "    # shufle dataloader if training\n",
    "    shuffle = is_train\n",
    "    name = \"Train\" if is_train else \"Valid\"\n",
    "\n",
    "    # data preprocessing utils\n",
    "    transforms = load_transforms(conf)\n",
    "\n",
    "    # quantile transform using BridgeScaler\n",
    "    if conf[\"data\"][\"scaler_type\"] == \"quantile-cached\":\n",
    "        dataset = Dataset_BridgeScaler(\n",
    "            conf,\n",
    "            conf_dataset='bs_years_train' if is_train else 'bs_years_val',\n",
    "            transform=transforms\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Z-score\n",
    "        dataset = ERA5Dataset(\n",
    "            filenames=files,\n",
    "            history_len=history_len,\n",
    "            forecast_len=forecast_len,\n",
    "            skip_periods=skip_periods,\n",
    "            one_shot=one_shot,\n",
    "            max_forecast_len=max_forecast_len,\n",
    "            transform=transforms\n",
    "        )\n",
    "\n",
    "    # Pytorch sampler\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        seed=seed,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True\n",
    "    )\n",
    "    logging.info(f\" Loaded a {name} ERA dataset, and a distributed sampler (forecast length = {forecast_len + 1})\")\n",
    "\n",
    "    return dataset, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "026b8433-a503-4ac1-a1da-ed44bbbba9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_sampler_zscore_only(conf, all_ERA_files, surface_files, diagnostic_files, world_size, rank, is_train, seed=42):\n",
    "\n",
    "    # convert $USER to the actual user name\n",
    "    conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "    # ======================================================== #\n",
    "    # parse intputs\n",
    "    \n",
    "    # file names\n",
    "    varname_upper_air = conf['data']['variables']\n",
    "    \n",
    "    if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "        forcing_files = conf['data']['save_loc_forcing']\n",
    "        varname_forcing = conf['data']['forcing_variables']\n",
    "    else:\n",
    "        forcing_files = None\n",
    "        varname_forcing = None\n",
    "    \n",
    "    if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "        static_files = conf['data']['save_loc_static']\n",
    "        varname_static = conf['data']['static_variables']\n",
    "    else:\n",
    "        static_files = None\n",
    "        varname_static = None\n",
    "    \n",
    "    if surface_files is not None:\n",
    "        varname_surface = conf['data']['surface_variables']\n",
    "    else:\n",
    "        varname_surface = None\n",
    "        \n",
    "    if diagnostic_files is not None:\n",
    "        varname_diagnostic = conf['data']['diagnostic_variables']\n",
    "    else:\n",
    "        varname_diagnostic = None\n",
    "        \n",
    "    # number of previous lead time inputs\n",
    "    history_len = conf[\"data\"][\"history_len\"]\n",
    "    valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "    # number of lead times to forecast\n",
    "    forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "    valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "    \n",
    "    if is_train:\n",
    "        history_len = history_len\n",
    "        forecast_len = forecast_len\n",
    "        # print out training / validation\n",
    "        name = \"training\"\n",
    "    else:\n",
    "        history_len = valid_history_len\n",
    "        forecast_len = valid_forecast_len\n",
    "        name = 'validation'\n",
    "        \n",
    "    # max_forecast_len\n",
    "    if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "        max_forecast_len = None\n",
    "    else:\n",
    "        max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "    # skip_periods\n",
    "    if \"skip_periods\" not in conf[\"data\"]:\n",
    "        skip_periods = None\n",
    "    else:\n",
    "        skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "        \n",
    "    # one_shot\n",
    "    if \"one_shot\" not in conf[\"data\"]:\n",
    "        one_shot = None\n",
    "    else:\n",
    "        one_shot = conf[\"data\"][\"one_shot\"]\n",
    "\n",
    "    # shufle\n",
    "    shuffle = is_train\n",
    "    \n",
    "    # data preprocessing utils\n",
    "    transforms = load_transforms(conf)\n",
    "\n",
    "    # Z-score\n",
    "    dataset = ERA5_and_Forcing_Dataset(\n",
    "        varname_upper_air=varname_upper_air,\n",
    "        varname_surface=varname_surface,\n",
    "        varname_forcing=varname_forcing,\n",
    "        varname_static=varname_static,\n",
    "        varname_diagnostic=varname_diagnostic,\n",
    "        filenames=all_ERA_files,\n",
    "        filename_surface=surface_files,\n",
    "        filename_forcing=forcing_files,\n",
    "        filename_static=static_files,\n",
    "        filename_diagnostic=diagnostic_files,\n",
    "        history_len=history_len,\n",
    "        forecast_len=forecast_len,\n",
    "        skip_periods=skip_periods,\n",
    "        one_shot=one_shot,\n",
    "        max_forecast_len=max_forecast_len,\n",
    "        transform=transforms\n",
    "    )\n",
    "    \n",
    "    # Pytorch sampler\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        seed=seed,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    logging.info(f\" Loaded a {name} ERA dataset, and a distributed sampler (forecast length = {forecast_len + 1})\")\n",
    "\n",
    "    return dataset, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd191ec5-ab81-4e87-8d3e-3aaa43139b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60eefa86-8c67-4645-878d-26633425ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert $USER to the actual user name\n",
    "conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "    setup(rank, world_size, conf[\"trainer\"][\"mode\"])\n",
    "\n",
    "# infer device id from rank\n",
    "device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "# Config settings\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "train_batch_size = conf['trainer']['train_batch_size']\n",
    "valid_batch_size = conf['trainer']['valid_batch_size']\n",
    "thread_workers = conf['trainer']['thread_workers']\n",
    "valid_thread_workers = conf['trainer']['valid_thread_workers'] if 'valid_thread_workers' in conf['trainer'] else thread_workers\n",
    "\n",
    "# get file names\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "\n",
    "    if \"save_loc_surface\" in conf[\"data\"]:\n",
    "        surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "    else:\n",
    "        surface_files = None\n",
    "\n",
    "    if \"save_loc_diagnostic\" in conf[\"data\"]:\n",
    "        diagnostic_files = sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "    else:\n",
    "        diagnostic_files = None\n",
    "        \n",
    "# ============================================================================== #\n",
    "# Space for customized train/test split\n",
    "# filenames = list(map(os.path.basename, all_ERA_files))\n",
    "# all_years = sorted([re.findall(r'(?:_)(\\d{4})', fn)[0] for fn in filenames])\n",
    "\n",
    "# Specify the years for each set\n",
    "# if conf[\"data\"][train_test_split]:\n",
    "#    normalized_split = conf[\"data\"][train_test_split] / sum(conf[\"data\"][train_test_split])\n",
    "#    n_years = len(all_years)\n",
    "#    train_years, sklearn.model_selection.train_test_split\n",
    "\n",
    "# ============================================================================== #\n",
    "# hard coded split\n",
    "train_years = [str(year) for year in range(1979, 2014)]\n",
    "valid_years = [str(year) for year in range(2014, 2018)]\n",
    "\n",
    "# Filter the files for each set\n",
    "train_files = [file for file in all_ERA_files if any(year in file for year in train_years)]\n",
    "valid_files = [file for file in all_ERA_files if any(year in file for year in valid_years)]\n",
    "\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    train_surface_files = [file for file in surface_files if any(year in file for year in train_years)]\n",
    "    valid_surface_files = [file for file in surface_files if any(year in file for year in valid_years)]\n",
    "    \n",
    "    train_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in train_years)]\n",
    "    valid_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# load dataset and sampler\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    \n",
    "    train_dataset, train_sampler = load_dataset_and_sampler_zscore_only(conf, \n",
    "                                                                        train_files, \n",
    "                                                                        train_surface_files, \n",
    "                                                                        train_diagnostic_files, \n",
    "                                                                        world_size, rank, is_train=True)\n",
    "    \n",
    "    valid_dataset, valid_sampler = load_dataset_and_sampler_zscore_only(conf, \n",
    "                                                                        valid_files, \n",
    "                                                                        valid_surface_files, \n",
    "                                                                        valid_diagnostic_files,\n",
    "                                                                        world_size, rank, is_train=False)\n",
    "else:\n",
    "    train_dataset, train_sampler = load_dataset_and_sampler(conf, train_files, world_size, rank, is_train=True)\n",
    "    valid_dataset, valid_sampler = load_dataset_and_sampler(conf, valid_files, world_size, rank, is_train=False)\n",
    "\n",
    "# setup the dataloder\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if thread_workers > 0 else False,\n",
    "        num_workers=thread_workers,\n",
    "        drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=valid_batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=valid_sampler,\n",
    "        pin_memory=False,\n",
    "        num_workers=valid_thread_workers,\n",
    "        drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f32e1300-ba8d-4f4a-ab6b-555761361059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_reshape(x1, x2):\n",
    "    x1 = x1.view(x1.shape[0], x1.shape[1], x1.shape[2] * x1.shape[3], x1.shape[4], x1.shape[5])\n",
    "    x_concat = torch.cat((x1, x2), dim=2)\n",
    "    return x_concat.permute(0, 2, 1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6d9d70e-fdb4-4645-aeea-565b92c9d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_only(x1):\n",
    "    x1 = x1.view(x1.shape[0], x1.shape[1], x1.shape[2] * x1.shape[3], x1.shape[4], x1.shape[5])\n",
    "    return x1.permute(0, 2, 1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b7c76ac-49ae-4fa1-8922-33705d20f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'std_new'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['data']['scaler_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ff6171f-df84-45e9-9ed3-4806e7ffda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34b1161d-99de-46ba-962e-cc24f2946d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_surf', 'x', 'x_forcing_static', 'y_diag', 'y_surf', 'y', 'index'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32b34ffd-a6e7-495e-a66e-4f60c818ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 2, 640, 1280])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_only(test['x']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3945b633-1d98-4618-a0db-654912bfec6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6509)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x_surf'][0, 1, -1, 300, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a4d8167-4700-4fc5-a1d7-af0490be25c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 1, 640, 1280])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_and_reshape(test['y'], test['y_surf']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75eb63a1-b7e6-4bb5-b476-f272159197bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 640, 1280])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['y_diag'].permute(0, 2, 1, 3, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b33f456a-d3ad-464d-8f33-9a215b3752a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 640, 1280])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x_forcing_static'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eef794-561f-4f25-a53d-22c169cde9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c967e24-99ff-431d-b667-5ae84f0885ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39235df7-54bb-4db1-aef8-4a31a234bdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_surf', 'x', 'y_surf', 'y', 'TOA', 'datetime', 'static', 'index'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "136bd905-accb-49b3-9b7e-abac4f39bdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 7, 640, 1280])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x_surf'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b48b2227-f3ac-4f34-aa1c-96451af0a91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6509)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x_surf'][0, 1, -1, 300, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123228d-8458-4a94-8a41-126d49ccc760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665b9bc-6edc-4418-8ffd-a22312459562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd316ba-009f-4f5b-879b-feb4f7427d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf070b3-dc35-49a8-a176-12f3a4650ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b12cd5-62d2-4014-ad44-11a5bf5cb6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebfd5b6b-8b6c-414b-bf21-cad7ba510346",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_merge = concat_and_reshape(test[\"x\"], test[\"x_surf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb51246-8ea0-4ccb-9fb2-19889f8eda18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 2, 640, 1280])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a00a1e19-591d-4087-849f-4cd3b6dad839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 640, 1280])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['forcing_static'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11826cc6-bd47-4355-baca-7df4b96a4673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 640, 1280])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['forcing_static'].permute(0, 2, 1, 3, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c62adbed-a233-4b4f-9581-81988da6e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 70, 2, 640, 1280])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45af1e21-d7bf-46d9-b542-58d737ad50d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1295)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_merge[0, 59, 0, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1656b4a0-c06f-48a8-ae35-dc30feab8d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1295)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"x\"][0, 0, -1, -1, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b042b65-2a6e-47b5-8f88-f71fac20e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_surf_new = test['x_surf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150f7e1-b5f3-4e81-acdc-0bb145b467b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOA_new = test['forcing_static'][0, :, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6d5c95e-104a-4508-874e-bda3328eaabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 640, 1280])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOA_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49805230-9289-488b-930c-258ed240a8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58a52a-2783-4277-9c38-fca23fdc0c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673614ee-fb90-4996-8960-3fbfb5a2eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd48ba9-2d55-4877-ab7a-4f594a2c4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_surf_old = test['x_surf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8744fa7d-9fb6-4ae6-b6e5-8adecacbaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOA_old = test['TOA'][0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ab065ed-2914-449e-90b2-1bec253e579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 640, 1280])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOA_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64d03cc2-288c-4893-af9f-4f320de2d9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TOA_new - TOA_old).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb57740-2eef-4cd8-abd6-b7e9df2ed2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354dfc0-d831-4d65-93eb-f19a93043183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1ca02-fb0f-454e-8650-1d35fdfd959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84807b-687b-4cc8-b358-6418148dcf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc5d88-0603-4737-b513-985ebb3f487f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cb431-cf8e-4ab3-9ddb-424905714934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a458aeb-543b-43c2-a160-c6a7199f9966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
