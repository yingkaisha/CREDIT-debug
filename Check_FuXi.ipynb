{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c879399c-a741-46b2-ab5e-4bcd6b442cfd",
   "metadata": {},
   "source": [
    "# FuXi architecture, model parameters, and I/O size debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef640e1-f422-4db0-80de-6648d6bd0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "\n",
    "from credit.models import load_model\n",
    "from credit.models.unet import SegmentationModel\n",
    "from credit.models.crossformer import CrossFormer\n",
    "from credit.models.fuxi import Fuxi\n",
    "from credit.parser import credit_main_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3d1bbe-97ae-4de6-8b70-82b39e4ff56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_DIR = '/glade/u/home/ksha/miles-physics/config/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6433070-0871-45aa-9a4f-fb46c745115c",
   "metadata": {},
   "source": [
    "## FuXi unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e651d63b-47b7-49f7-b2c2-090e02780524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fuxi(): \n",
    "    # load config\n",
    "    config = os.path.join(CONFIG_FILE_DIR, 'fuxi_1deg_test.yml')\n",
    "    with open(config) as cf:\n",
    "        conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    # handle config args\n",
    "    conf = credit_main_parser(conf)\n",
    "    \n",
    "    image_height = conf[\"model\"][\"image_height\"]\n",
    "    image_width = conf[\"model\"][\"image_width\"]\n",
    "    channels = conf[\"model\"][\"channels\"]\n",
    "    levels = conf[\"model\"][\"levels\"]\n",
    "    surface_channels = conf[\"model\"][\"surface_channels\"]\n",
    "    input_only_channels = conf[\"model\"][\"input_only_channels\"]\n",
    "    output_only_channels = conf[\"model\"][\"output_only_channels\"]\n",
    "    frames = conf[\"model\"][\"frames\"]\n",
    "    \n",
    "    in_channels = channels * levels + surface_channels + input_only_channels\n",
    "    out_channels = channels * levels + surface_channels + output_only_channels\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_tensor = torch.randn(1, in_channels, frames, image_height, image_width).to(device)\n",
    "    \n",
    "    model = load_model(conf).to(device)\n",
    "    assert isinstance(model, Fuxi)\n",
    "    \n",
    "    y_pred = model(input_tensor)\n",
    "    assert y_pred.shape == torch.Size([1, out_channels, 1, image_height, image_width])\n",
    "    assert not torch.isnan(y_pred).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7436ee04-b196-4c1f-8243-09fb3eaae6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "test_fuxi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e64679-9997-492a-afed-e68e9ac094fa",
   "metadata": {},
   "source": [
    "## FuXi dev section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59da63e-fdcf-46b5-b19e-d41054e929d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from timm.layers.helpers import to_2tuple\n",
    "from timm.models.swin_transformer_v2 import SwinTransformerV2Stage\n",
    "import logging\n",
    "\n",
    "from credit.postblock import PostBlock\n",
    "from credit.models.base_model import BaseModel\n",
    "from credit.boundary_padding import TensorPadding\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def apply_spectral_norm(model):\n",
    "    \"\"\"\n",
    "    add spectral norm to all the conv and linear layers\n",
    "    \"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "            nn.utils.spectral_norm(module)\n",
    "\n",
    "\n",
    "def get_pad3d(input_resolution, window_size):\n",
    "    \"\"\"\n",
    "    Estimate the size of padding based on the given window size and the original input size.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): (Pl, Lat, Lon)\n",
    "        window_size (tuple[int]): (Pl, Lat, Lon)\n",
    "\n",
    "    Returns:\n",
    "        padding (tuple[int]): (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back)\n",
    "    \"\"\"\n",
    "    Pl, Lat, Lon = input_resolution\n",
    "    win_pl, win_lat, win_lon = window_size\n",
    "\n",
    "    padding_left = padding_right = padding_top = padding_bottom = padding_front = (\n",
    "        padding_back\n",
    "    ) = 0\n",
    "    pl_remainder = Pl % win_pl\n",
    "    lat_remainder = Lat % win_lat\n",
    "    lon_remainder = Lon % win_lon\n",
    "\n",
    "    if pl_remainder:\n",
    "        pl_pad = win_pl - pl_remainder\n",
    "        padding_front = pl_pad // 2\n",
    "        padding_back = pl_pad - padding_front\n",
    "    if lat_remainder:\n",
    "        lat_pad = win_lat - lat_remainder\n",
    "        padding_top = lat_pad // 2\n",
    "        padding_bottom = lat_pad - padding_top\n",
    "    if lon_remainder:\n",
    "        lon_pad = win_lon - lon_remainder\n",
    "        padding_left = lon_pad // 2\n",
    "        padding_right = lon_pad - padding_left\n",
    "\n",
    "    return (\n",
    "        padding_left,\n",
    "        padding_right,\n",
    "        padding_top,\n",
    "        padding_bottom,\n",
    "        padding_front,\n",
    "        padding_back,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pad2d(input_resolution, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Lat, Lon\n",
    "        window_size (tuple[int]): Lat, Lon\n",
    "\n",
    "    Returns:\n",
    "        padding (tuple[int]): (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    \"\"\"\n",
    "    input_resolution = [2] + list(input_resolution)\n",
    "    window_size = [2] + list(window_size)\n",
    "    padding = get_pad3d(input_resolution, window_size)\n",
    "    return padding[:4]\n",
    "\n",
    "\n",
    "class CubeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_size: T, Lat, Lon\n",
    "        patch_size: T, Lat, Lon\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size, patch_size, in_chans, embed_dim, norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # input size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # number of patches after embedding (T_num, Lat_num, Lon_num)\n",
    "        patches_resolution = [\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "            img_size[2] // patch_size[2],\n",
    "        ]\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # number of embedded dimension after patching\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Conv3d-based patching\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        # layer norm\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # example size: [Batch, 67, 2, 640, 1280]\n",
    "        B, T, C, Lat, Lon = x.shape\n",
    "\n",
    "        # Conv3d-based patching and embedding\n",
    "        # output size: [B, 1024, 1, 40, 80]\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # combine T, Lat, Lon dimensions\n",
    "        # output size: [B, 1024, 3200]\n",
    "        x = x.reshape(B, self.embed_dim, -1)\n",
    "\n",
    "        # switch to channel-last for normalization\n",
    "        # output size: [B, 3200, 1024]\n",
    "        x = x.transpose(1, 2)  # B T*Lat*Lon C\n",
    "\n",
    "        # Layer norm (channel last)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        # switch back to channel first\n",
    "        # output size: [B, 1024, 3200]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # recover T, Lat, Lon dimensions\n",
    "        # output size: [B, 1024, 1, 40, 80]\n",
    "        x = x.reshape(B, self.embed_dim, *self.patches_resolution)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_chans: int, out_chans: int, num_groups: int, num_residuals: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # down-sampling with Conv2d\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chans, out_chans, kernel_size=(3, 3), stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        # blocks of residual path\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            blk.append(\n",
    "                nn.Conv2d(out_chans, out_chans, kernel_size=3, stride=1, padding=1)\n",
    "            )\n",
    "            blk.append(nn.GroupNorm(num_groups, out_chans))\n",
    "            blk.append(nn.SiLU())\n",
    "        self.b = nn.Sequential(*blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # down-sampling\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # skip-connection\n",
    "        shortcut = x\n",
    "\n",
    "        # residual path\n",
    "        x = self.b(x)\n",
    "\n",
    "        # additive residual connection\n",
    "        return x + shortcut\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, num_groups, num_residuals=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # down-sampling with Transpose Conv\n",
    "        self.conv = nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2)\n",
    "\n",
    "        # blocks of residual path\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            blk.append(\n",
    "                nn.Conv2d(out_chans, out_chans, kernel_size=3, stride=1, padding=1)\n",
    "            )\n",
    "            blk.append(nn.GroupNorm(num_groups, out_chans))\n",
    "            blk.append(nn.SiLU())\n",
    "        self.b = nn.Sequential(*blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # up-sampling\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # skip-connection\n",
    "        shortcut = x\n",
    "\n",
    "        # residual path\n",
    "        x = self.b(x)\n",
    "\n",
    "        # additive residual connection\n",
    "        return x + shortcut\n",
    "\n",
    "\n",
    "class UTransformer(nn.Module):\n",
    "    \"\"\"U-Transformer\n",
    "    Args:\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        num_groups (int | tuple[int]): number of groups to separate the channels into.\n",
    "        input_resolution (tuple[int]): Lat, Lon.\n",
    "        num_heads (int): Number of attention heads in different layers.\n",
    "        window_size (int | tuple[int]): Window size.\n",
    "        depth (int): Number of blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim, \n",
    "        num_groups, \n",
    "        input_resolution, \n",
    "        num_heads, \n",
    "        window_size, \n",
    "        depth,\n",
    "        drop_path\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_groups = to_2tuple(num_groups)\n",
    "        window_size = to_2tuple(window_size)  # convert window_size[int] to tuple\n",
    "\n",
    "        # padding input tensors so they are divided by the window size\n",
    "        padding = get_pad2d(input_resolution, window_size)  # <--- Accepts tuple only\n",
    "        padding_left, padding_right, padding_top, padding_bottom = padding\n",
    "        self.padding = padding\n",
    "        self.pad = nn.ZeroPad2d(padding)\n",
    "\n",
    "        # input resolution after padding\n",
    "        input_resolution = list(input_resolution)\n",
    "        input_resolution[0] = input_resolution[0] + padding_top + padding_bottom\n",
    "        input_resolution[1] = input_resolution[1] + padding_left + padding_right\n",
    "\n",
    "        # down-sampling block\n",
    "        self.down = DownBlock(embed_dim, embed_dim, num_groups[0])\n",
    "\n",
    "        # SwinT block\n",
    "        self.layer = SwinTransformerV2Stage(\n",
    "            embed_dim, \n",
    "            embed_dim, \n",
    "            input_resolution, \n",
    "            depth, \n",
    "            num_heads, \n",
    "            window_size[0],\n",
    "            drop_path=drop_path\n",
    "        )  # <--- window_size[0] get window_size[int] from tuple\n",
    "\n",
    "        # up-sampling block\n",
    "        self.up = UpBlock(embed_dim * 2, embed_dim, num_groups[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, Lat, Lon = x.shape\n",
    "        padding_left, padding_right, padding_top, padding_bottom = self.padding\n",
    "        x = self.down(x)\n",
    "        shortcut = x\n",
    "\n",
    "        # pad\n",
    "        x = self.pad(x)\n",
    "        _, _, pad_lat, pad_lon = x.shape\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)  # B Lat Lon C\n",
    "        x = self.layer(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # crop\n",
    "        x = x[\n",
    "            :,\n",
    "            :,\n",
    "            padding_top : pad_lat - padding_bottom,\n",
    "            padding_left : pad_lon - padding_right,\n",
    "        ]\n",
    "\n",
    "        # concat\n",
    "        x = torch.cat([shortcut, x], dim=1)  # B 2*C Lat Lon\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Fuxi(BaseModel):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_size (Sequence[int], optional): T, Lat, Lon.\n",
    "        patch_size (Sequence[int], optional): T, Lat, Lon.\n",
    "        in_chans (int, optional): number of input channels.\n",
    "        out_chans (int, optional): number of output channels.\n",
    "        dim (int, optional): number of embed channels.\n",
    "        num_groups (Sequence[int] | int, optional): number of groups to separate the channels into.\n",
    "        num_heads (int, optional): Number of attention heads.\n",
    "        window_size (int | tuple[int], optional): Local window size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_height=640,  # 640\n",
    "        patch_height=16,\n",
    "        image_width=1280,  # 1280\n",
    "        patch_width=16,\n",
    "        levels=15,\n",
    "        frames=2,\n",
    "        frame_patch_size=2,\n",
    "        dim=1536,\n",
    "        num_groups=32,\n",
    "        channels=4,\n",
    "        surface_channels=7,\n",
    "        input_only_channels=0,\n",
    "        output_only_channels=0,\n",
    "        num_heads=8,\n",
    "        depth=48,\n",
    "        window_size=7,\n",
    "        use_spectral_norm=True,\n",
    "        interp=True,\n",
    "        drop_path=0,\n",
    "        padding_conf=None,\n",
    "        post_conf=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_interp = interp\n",
    "        self.use_spectral_norm = use_spectral_norm\n",
    "        if padding_conf is None:\n",
    "            padding_conf = {\"activate\": False}\n",
    "        self.use_padding = padding_conf[\"activate\"]\n",
    "        if post_conf is None:\n",
    "            post_conf = {\"activate\": False}\n",
    "        self.use_post_block = post_conf[\"activate\"]\n",
    "\n",
    "        # input tensor size (time, lat, lon)\n",
    "        if self.use_padding:\n",
    "            pad_lat = padding_conf[\"pad_lat\"]\n",
    "            pad_lon = padding_conf[\"pad_lon\"]\n",
    "            image_height_pad = image_height + pad_lat[0] + pad_lat[1]\n",
    "            image_width_pad = image_width + pad_lon[0] + pad_lon[1]\n",
    "            img_size = (frames, image_height_pad, image_width_pad)\n",
    "            self.img_size_original = (frames, image_height, image_width)\n",
    "        else:\n",
    "            img_size = (frames, image_height, image_width)\n",
    "            self.img_size_original = img_size\n",
    "\n",
    "        # the size of embedded patches\n",
    "        patch_size = (frame_patch_size, patch_height, patch_width)\n",
    "\n",
    "        # number of channels = levels * varibales per level + surface variables\n",
    "        # in_chans = out_chans = levels * channels + surface_channels\n",
    "\n",
    "        in_chans = channels * levels + surface_channels + input_only_channels\n",
    "        out_chans = channels * levels + surface_channels + output_only_channels\n",
    "\n",
    "        # input resolution = number of embedded patches / 2\n",
    "        # divide by two because \"u_trasnformer\" has a down-sampling block\n",
    "\n",
    "        input_resolution = (\n",
    "            round(img_size[1] / patch_size[1] / 2),\n",
    "            round(img_size[2] / patch_size[2] / 2),\n",
    "        )\n",
    "        # FuXi cube embedding layer\n",
    "        self.cube_embedding = CubeEmbedding(img_size, patch_size, in_chans, dim)\n",
    "\n",
    "        # Downsampling --> SwinTransformerV2 stacks --> Upsampling\n",
    "        self.u_transformer = UTransformer(\n",
    "            dim, num_groups, \n",
    "            input_resolution, \n",
    "            num_heads, \n",
    "            window_size, \n",
    "            depth=depth,\n",
    "            drop_path=drop_path\n",
    "        )\n",
    "\n",
    "        # dense layer applied on channel dmension\n",
    "        # channel * patch_size beucase dense layer recovers embedded dimensions to the input dimensions\n",
    "        self.fc = nn.Linear(dim, out_chans * patch_size[1] * patch_size[2])\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.patch_size = patch_size\n",
    "        self.input_resolution = input_resolution\n",
    "        self.out_chans = out_chans\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.channels = channels\n",
    "        self.surface_channels = surface_channels\n",
    "        self.levels = levels\n",
    "\n",
    "        if self.use_padding:\n",
    "            self.padding_opt = TensorPadding(**padding_conf)\n",
    "\n",
    "        if self.use_spectral_norm:\n",
    "            logger.info(\"Adding spectral norm to all conv and linear layers\")\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            # Move the model to the device\n",
    "            self.to(device)\n",
    "            apply_spectral_norm(self)\n",
    "\n",
    "        if self.use_post_block:\n",
    "            self.postblock = PostBlock(post_conf)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # copy tensor to feed into postblock later\n",
    "        x_copy = None\n",
    "        if self.use_post_block:\n",
    "            x_copy = x.clone().detach()\n",
    "\n",
    "        if self.use_padding:\n",
    "            x = self.padding_opt.pad(x)\n",
    "\n",
    "        # Tensor dims: Batch, Variables, Time, Lat grids, Lon grids\n",
    "        B, _, _, _, _ = x.shape\n",
    "\n",
    "        _, patch_lat, patch_lon = self.patch_size\n",
    "\n",
    "        # Get the number of patches after embedding\n",
    "        Lat, Lon = self.input_resolution\n",
    "        Lat, Lon = Lat * 2, Lon * 2\n",
    "\n",
    "        # Cube Embedding and squeese the time dimension\n",
    "        # (the model produce single forecast lead time only)\n",
    "\n",
    "        # x: input size = (Batch, Variables, Time, Lat grids, Lon grids)\n",
    "        x = self.cube_embedding(x).squeeze(2)  # B C Lat Lon\n",
    "        # x: output size = (Batch, Embedded dimension, time, number of patches, number of patches)\n",
    "\n",
    "        # u_transformer stage\n",
    "        # the size of x does notchange\n",
    "        x = self.u_transformer(x)\n",
    "\n",
    "        # recover embeddings to lat/lon grids with dense layer and reshape operation.\n",
    "        x = self.fc(x.permute(0, 2, 3, 1))  # B Lat Lon C\n",
    "        x = x.reshape(B, Lat, Lon, patch_lat, patch_lon, self.out_chans).permute(\n",
    "            0, 1, 3, 2, 4, 5\n",
    "        )\n",
    "        # B, lat, patch_lat, lon, patch_lon, C\n",
    "        x = x.reshape(B, Lat * patch_lat, Lon * patch_lon, self.out_chans)\n",
    "        x = x.permute(0, 3, 1, 2)  # B C Lat Lon\n",
    "\n",
    "        if self.use_padding:\n",
    "            x = self.padding_opt.unpad(x)\n",
    "\n",
    "        if self.use_interp:\n",
    "            img_size = list(self.img_size_original)\n",
    "            x = F.interpolate(x, size=img_size[1:], mode=\"bilinear\")\n",
    "\n",
    "        x = x.unsqueeze(2)\n",
    "\n",
    "        if self.use_post_block:\n",
    "            x = {\n",
    "                \"y_pred\": x,\n",
    "                \"x\": x_copy,\n",
    "            }\n",
    "            x = self.postblock(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c061c5c-d78a-43ff-8895-7f4b704e4fb1",
   "metadata": {},
   "source": [
    "## FuXi param check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cf486f-0fb0-4b03-88e1-e9971ec1c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old rollout config\n",
    "#config_name = '/glade/u/home/ksha/miles-credit/config/example_physics_single.yml'\n",
    "\n",
    "config_name = '/glade/work/ksha/CREDIT_runs/fuxi_dry_tune/model_single.yml'\n",
    "\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fc7a466-9569-4f49-be21-90e626c7fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = credit_main_parser(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75f7915-36fe-4d0a-9f07-114c74da6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf['model']['post_conf']['activate'] = False\n",
    "# conf['model']['interp'] = False\n",
    "# conf['model']['padding_conf']['pad_lat'] = [21, 22]\n",
    "# conf['model']['padding_conf']['pad_lon'] = [44, 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8617fc75-0227-479b-bba5-be85d981e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = conf['model']['image_height']\n",
    "image_width = conf['model']['image_width']\n",
    "levels = conf['model']['levels']\n",
    "frames = conf['model']['frames']\n",
    "channels = conf['model']['channels']\n",
    "surface_channels = conf['model']['surface_channels']\n",
    "input_only_channels = conf['model']['input_only_channels']\n",
    "output_only_channels = conf['model']['output_only_channels']\n",
    "\n",
    "# ============================================================= #\n",
    "# build the model\n",
    "model = Fuxi(**conf['model']).to(\"cuda\")\n",
    "\n",
    "# ============================================================= #\n",
    "# test the model\n",
    "\n",
    "# pass an input tensor to test the graph\n",
    "input_tensor = torch.randn(1, channels * levels + surface_channels + input_only_channels, \n",
    "                           frames, image_height, image_width).to(\"cuda\")    \n",
    "\n",
    "y_pred = model(input_tensor.to(\"cuda\"))\n",
    "\n",
    "print('Input shape: {}'.format(input_tensor.shape))\n",
    "print(\"Predicted shape: {}\".format(y_pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec25aac-599b-4529-9f79-75603ea47e42",
   "metadata": {},
   "source": [
    "## FuXi I/O size and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352fffb-f9f0-4680-97b1-efc958703a5c",
   "metadata": {},
   "source": [
    "**Auto-detect pad size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9014e26e-6571-45d7-967d-802eac6fa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4bc66a20-eae1-45a3-83bc-cac42d817311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_padding_sizes(image_height, image_width, window_size, patch_height, patch_width, base_val=40, N_lat_add=0, N_lon_add=0):\n",
    "    \"\"\"\n",
    "    Computes the required padding sizes pad_lat and pad_lon for given image dimensions,\n",
    "    window size, and patch sizes, such that after padding:\n",
    "    - The input resolutions used in the model are divisible by window_size.\n",
    "    - After unpadding and any interpolation, the output tensor size is the same as the original.\n",
    "    - The padding sizes are as close as possible to [base_val, base_val].\n",
    "    - Allows adjustment of required input resolutions by adding multiples of window_size.\n",
    "    \n",
    "    Args:\n",
    "        image_height (int): Original image height.\n",
    "        image_width (int): Original image width.\n",
    "        window_size (int): Window size used in the model.\n",
    "        patch_height (int): Patch height.\n",
    "        patch_width (int): Patch width.\n",
    "        base_val (int): Desired base padding value for each side.\n",
    "        N_lat_add (int): Additional window sizes to add to required input resolution for latitude.\n",
    "        N_lon_add (int): Additional window sizes to add to required input resolution for longitude.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: pad_lat (list[int]), pad_lon (list[int])\n",
    "               Padding sizes for latitude and longitude ([top, bottom], [left, right]).\n",
    "    \"\"\"\n",
    "    frames = 2\n",
    "    frame_patch_size = 2\n",
    "\n",
    "    # Calculate initial input resolutions without padding\n",
    "    input_resolution_lat = image_height / patch_height / 2\n",
    "    input_resolution_lon = image_width / patch_width / 2\n",
    "\n",
    "    # Calculate minimal required input resolutions that are divisible by window_size\n",
    "    N_lat_min = math.ceil(input_resolution_lat / window_size)\n",
    "    N_lon_min = math.ceil(input_resolution_lon / window_size)\n",
    "\n",
    "    # Adjust required input resolutions by adding additional window sizes\n",
    "    N_lat = N_lat_min + N_lat_add\n",
    "    N_lon = N_lon_min + N_lon_add\n",
    "\n",
    "    required_input_resolution_lat = N_lat * window_size\n",
    "    required_input_resolution_lon = N_lon * window_size\n",
    "\n",
    "    # Adjusted image dimensions after padding\n",
    "    image_height_padded = required_input_resolution_lat * patch_height * 2\n",
    "    image_width_padded = required_input_resolution_lon * patch_width * 2\n",
    "\n",
    "    # Calculate total padding required\n",
    "    pad_lat_total = int(image_height_padded - image_height)\n",
    "    pad_lon_total = int(image_width_padded - image_width)\n",
    "\n",
    "    # Check if total padding is non-negative\n",
    "    if pad_lat_total < 0 or pad_lon_total < 0:\n",
    "        return None, None\n",
    "\n",
    "    # Distribute padding for latitude (height)\n",
    "    pad_lat = distribute_padding(pad_lat_total, base_val)\n",
    "\n",
    "    # Distribute padding for longitude (width)\n",
    "    pad_lon = distribute_padding(pad_lon_total, base_val)\n",
    "\n",
    "    return pad_lat, pad_lon\n",
    "\n",
    "def distribute_padding(total_padding, base_val):\n",
    "    if total_padding == 0:\n",
    "        return [0, 0]\n",
    "    \n",
    "    # Distribute padding evenly\n",
    "    pad_first = total_padding // 2\n",
    "    pad_second = total_padding - pad_first\n",
    "\n",
    "    # Adjust padding to be as close as possible to base_val\n",
    "    if pad_first > base_val:\n",
    "        pad_first = base_val\n",
    "        pad_second = total_padding - pad_first\n",
    "    if pad_second > base_val:\n",
    "        pad_second = base_val\n",
    "        pad_first = total_padding - pad_second\n",
    "\n",
    "    # Ensure total padding matches\n",
    "    if pad_first + pad_second != total_padding:\n",
    "        pad_second += total_padding - (pad_first + pad_second)\n",
    "\n",
    "    return [pad_first, pad_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5a9e6244-ff7d-479a-80bf-a552e9158b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([59, 60], [64, 64])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_padding_sizes(721, 1440, 7, 4, 4, base_val=200, N_lat_add=2, N_lon_add=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4e5d4-528e-44dd-b476-0e33d728a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
