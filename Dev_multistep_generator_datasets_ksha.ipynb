{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5b32b4-9512-42bf-b712-d9c997035fe0",
   "metadata": {},
   "source": [
    "# Develop and test Pytorch IterableDataset for multi-step training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d60ecb-0d9f-4ddc-bd9e-5f3174601d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import logging\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor as Pool\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from credit.data import (Sample, find_key_for_number, get_forward_data, \n",
    "                         drop_var_from_dataset, extract_month_day_hour, find_common_indices)\n",
    "\n",
    "from credit.transforms import load_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c658fe-fda3-403b-8ef4-53cdb47df3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9192aa9-f968-4a0e-9e87-28c92a30c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new rollout config\n",
    "config_name = '/glade/work/ksha/repos/global/miles-credit/results/wxformer_6h/model.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf_dyn = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f124bb33-deb7-40a0-9a68-d4f78319a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = conf_dyn\n",
    "is_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc27c7-216d-4ed1-911f-baf56ce14082",
   "metadata": {},
   "source": [
    "## Load transforms and single-step / one-shot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f43baf7-d7c9-410c-9036-fa2f9a04e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting surface files\n",
      "collecting dynamic forcing files\n"
     ]
    }
   ],
   "source": [
    "if 'train_years' in conf['data']:\n",
    "    train_years_range = conf['data']['train_years']\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if 'valid_years' in conf['data']:\n",
    "    valid_years_range = conf['data']['valid_years']\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# get file names\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# <------------------------------------------ std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "\n",
    "    # check and glob surface files\n",
    "    if ('surface_variables' in conf['data']) and (len(conf['data']['surface_variables']) > 0):\n",
    "        \n",
    "        print('collecting surface files')\n",
    "        surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "        \n",
    "    else:\n",
    "        surface_files = None\n",
    "\n",
    "    # check and glob dyn forcing files\n",
    "    if ('dynamic_forcing_variables' in conf['data']) and (len(conf['data']['dynamic_forcing_variables']) > 0):\n",
    "\n",
    "        print('collecting dynamic forcing files')\n",
    "        dyn_forcing_files = sorted(glob.glob(conf[\"data\"][\"save_loc_dynamic_forcing\"]))\n",
    "        \n",
    "    else:\n",
    "        dyn_forcing_files = None\n",
    "\n",
    "    # check and glob diagnostic files\n",
    "    if ('diagnostic_variables' in conf['data']) and (len(conf['data']['diagnostic_variables']) > 0):\n",
    "\n",
    "        print('collecting diagnostic files')\n",
    "        diagnostic_files = sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "        \n",
    "    else:\n",
    "        diagnostic_files = None\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [file for file in all_ERA_files if any(year in file for year in train_years)]\n",
    "valid_files = [file for file in all_ERA_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    \n",
    "    if surface_files is not None:\n",
    "        \n",
    "        train_surface_files = [file for file in surface_files if any(year in file for year in train_years)]\n",
    "        valid_surface_files = [file for file in surface_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_surface_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [surface files] and [upper-air files]'\n",
    "        assert len(valid_surface_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [surface files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_surface_files = None\n",
    "        valid_surface_files = None\n",
    "\n",
    "    if dyn_forcing_files is not None:\n",
    "        \n",
    "        train_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in train_years)]\n",
    "        valid_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_dyn_forcing_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [dynamic forcing files] and [upper-air files]'\n",
    "        assert len(valid_dyn_forcing_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [dynamic forcing files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_dyn_forcing_files = None\n",
    "        valid_dyn_forcing_files = None\n",
    "        \n",
    "    if diagnostic_files is not None:\n",
    "        \n",
    "        train_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in train_years)]\n",
    "        valid_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_diagnostic_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [diagnostic files] and [upper-air files]'\n",
    "        assert len(valid_diagnostic_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [diagnostic files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_diagnostic_files = None\n",
    "        valid_diagnostic_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e60466-1858-4d88-8e04-54ffaa713424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names\n",
    "varname_all = []\n",
    "\n",
    "# upper air\n",
    "varname_upper_air = conf['data']['variables']\n",
    "\n",
    "if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "    forcing_files = conf['data']['save_loc_forcing']\n",
    "    varname_forcing = conf['data']['forcing_variables']\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "    \n",
    "if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "    static_files = conf['data']['save_loc_static']\n",
    "    varname_static = conf['data']['static_variables']\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None\n",
    "\n",
    "# get surface variable names\n",
    "if surface_files is not None:\n",
    "    varname_surface = conf['data']['surface_variables']\n",
    "else:\n",
    "    varname_surface = None\n",
    "\n",
    "# get dynamic forcing variable names\n",
    "if dyn_forcing_files is not None:\n",
    "    varname_dyn_forcing = conf['data']['dynamic_forcing_variables']\n",
    "else:\n",
    "    varname_dyn_forcing = None\n",
    "\n",
    "# get diagnostic variable names\n",
    "if diagnostic_files is not None:\n",
    "    varname_diagnostic = conf['data']['diagnostic_variables']\n",
    "else:\n",
    "    varname_diagnostic = None\n",
    "        \n",
    "# number of previous lead time inputs\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "# number of lead times to forecast\n",
    "forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "\n",
    "if is_train:\n",
    "    history_len = history_len\n",
    "    forecast_len = forecast_len\n",
    "    # print out training / validation\n",
    "    name = \"training\"\n",
    "else:\n",
    "    history_len = valid_history_len\n",
    "    forecast_len = valid_forecast_len\n",
    "    name = 'validation'\n",
    "    \n",
    "# max_forecast_len\n",
    "if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "    max_forecast_len = None\n",
    "else:\n",
    "    max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "# skip_periods\n",
    "if \"skip_periods\" not in conf[\"data\"]:\n",
    "    skip_periods = None\n",
    "else:\n",
    "    skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "    \n",
    "# one_shot\n",
    "if \"one_shot\" not in conf[\"data\"]:\n",
    "    one_shot = None\n",
    "else:\n",
    "    one_shot = conf[\"data\"][\"one_shot\"]\n",
    "\n",
    "# shufle\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05581783-78fd-4737-8b9c-41055ecf3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing utils\n",
    "transforms = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590db17-a67e-4d15-b9e7-73ff53459703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ffbb75-488d-4c4f-98e8-dbfab8d6392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedSequentialDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air: List[str],\n",
    "        varname_surface: List[str],\n",
    "        varname_dyn_forcing: List[str],\n",
    "        varname_forcing: List[str],\n",
    "        varname_static: List[str],\n",
    "        varname_diagnostic: List[str],\n",
    "        filenames: List[str],\n",
    "        filename_surface: Optional[List[str]] = None,\n",
    "        filename_dyn_forcing: Optional[List[str]] = None,\n",
    "        filename_forcing: Optional[str] = None,\n",
    "        filename_static: Optional[str] = None,\n",
    "        filename_diagnostic: Optional[List[str]] = None,\n",
    "        rank: int = 0,\n",
    "        world_size: int = 1,\n",
    "        history_len: int = 2,\n",
    "        forecast_len: int = 0,\n",
    "        transform: Optional[Callable] = None,\n",
    "        seed: int = 42,\n",
    "        skip_periods: Optional[int] = None,\n",
    "        max_forecast_len: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "\n",
    "        '''\n",
    "        Initialize the DistributedSequentialDatasetV2.\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - rank (int, optional): Rank of the current process. Default is 0.\n",
    "        - world_size (int, optional): Total number of processes. Default is 1.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "        - num_workers (int, optional): Number of worker processes. Default is 0.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical ERA5 images, target ERA5 images, datetime index, and additional information.\n",
    "        '''\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_epoch = 0\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        logger.info(f\"Using {num_workers} workers in the iterable dataset\")\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # ERA5 operations\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "        \n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "            \n",
    "        self.all_files = all_files\n",
    "        \n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {} # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [len(ERA5_xarray['time']),\n",
    "                                                ind_start,\n",
    "                                                ind_start + len(ERA5_xarray['time'])]\n",
    "            ind_start += len(ERA5_xarray['time']) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "        \n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "        \n",
    "            for fn in filename_surface:\n",
    "                \n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "                \n",
    "                surface_files.append(xarray_dataset)\n",
    "                \n",
    "            self.surface_files = surface_files\n",
    "            \n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "        \n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "        \n",
    "            for fn in filename_dyn_forcing:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_dyn_forcing)\n",
    "                \n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "                \n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "            \n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "        \n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "        \n",
    "        if self.filename_diagnostic is not None:\n",
    "\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "            \n",
    "            for fn in filename_diagnostic:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_diagnostic)\n",
    "                \n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "                \n",
    "            self.diagnostic_files = diagnostic_files\n",
    "            \n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "        \n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(filename_forcing), 'Cannot find forcing file [{}]'.format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "            \n",
    "            self.xarray_forcing = xarray_dataset\n",
    "            \n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(filename_static), 'Cannot find static file [{}]'.format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "            \n",
    "            self.xarray_static = xarray_dataset\n",
    "            \n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "            \n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray['time']) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # get worker info\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "\n",
    "        # distributed sampler with worker info\n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers * self.world_size,\n",
    "                                     rank=self.rank * num_workers + worker_id, shuffle=self.shuffle)\n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # worker process\n",
    "        process_index_partial = partial(\n",
    "            worker,\n",
    "            ERA5_indices=self.ERA5_indices,\n",
    "            all_files=self.all_files,\n",
    "            surface_files=self.surface_files,\n",
    "            dyn_forcing_files = self.dyn_forcing_files,\n",
    "            diagnostic_files=self.diagnostic_files,\n",
    "            xarray_forcing=self.xarray_forcing,\n",
    "            xarray_static=self.xarray_static,\n",
    "            history_len=self.history_len,\n",
    "            forecast_len=self.forecast_len,\n",
    "            skip_periods=self.skip_periods,\n",
    "            transform=self.transform\n",
    "        )\n",
    "\n",
    "        # Dont use multi-processing\n",
    "        if self.num_workers <= 1:\n",
    "            for index in iter(sampler):\n",
    "                # Explicit inner (time step) loop\n",
    "                indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "                for ind_start_current_step in indices:\n",
    "                    sample = process_index_partial((index, ind_start_current_step))\n",
    "                    yield sample\n",
    "                    if sample['stop_forecast']:\n",
    "                        break\n",
    "                        \n",
    "        else:  # use multi-processing\n",
    "            with Pool(self.num_workers) as p:\n",
    "                batch_size = 2 * self.num_workers  # limit the size of the \"queue\"\n",
    "                for index in iter(sampler):\n",
    "                    indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "\n",
    "                    # Process indices in batches to avoid potential memory problems if indices is very long\n",
    "                    for i in range(0, len(indices), batch_size):\n",
    "                        batch_indices = indices[i:i+batch_size]\n",
    "                        batch_tasks = [(index, ind_start_current_step) for ind_start_current_step in batch_indices]\n",
    "\n",
    "                        # Process the batch\n",
    "                        batch_results = p.map(process_index_partial, batch_tasks)\n",
    "\n",
    "                        # Yield results from the batch\n",
    "                        for sample in batch_results:\n",
    "                            yield sample\n",
    "                            if sample['stop_forecast']:\n",
    "                                return\n",
    "\n",
    "\n",
    "def worker(\n",
    "    tuple_index: Tuple[int, int],\n",
    "    ERA5_indices: Dict[str, List[int]],\n",
    "    all_files: List[Any],\n",
    "    surface_files: Optional[List[Any]],\n",
    "    dyn_forcing_files: Optional[List[Any]],\n",
    "    diagnostic_files: Optional[List[Any]],\n",
    "    xarray_forcing: Optional[Any],\n",
    "    xarray_static: Optional[Any],\n",
    "    history_len: int,\n",
    "    forecast_len: int,\n",
    "    skip_periods: int,\n",
    "    transform: Optional[Callable]\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    '''\n",
    "    Processes a given index to extract and transform data for a specific time slice.\n",
    "\n",
    "    Parameters:\n",
    "    - tuple_index (Tuple[int, int]): Tuple containing the current index and sub-index for processing.\n",
    "    - ERA5_indices (Dict[str, List[int]]): Dictionary containing ERA5 indices metadata.\n",
    "    - all_files (List[Any]): List of xarray datasets containing upper air data.\n",
    "    - surface_files (Optional[List[Any]]): List of xarray datasets containing surface data.\n",
    "    - dyn_forcing_files (Optional[List[Any]]): List of xarray datasets containing dynamic forcing data.\n",
    "    - diagnostic_files (Optional[List[Any]]): List of xarray datasets containing diagnostic data.\n",
    "    - history_len (int): Length of the history sequence.\n",
    "    - forecast_len (int): Length of the forecast sequence.\n",
    "    - skip_periods (int): Number of periods to skip between samples.\n",
    "    - xarray_forcing (Optional[Any]): xarray dataset containing forcing data.\n",
    "    - xarray_static (Optional[Any]): xarray dataset containing static data.\n",
    "\n",
    "    - transform (Optional[Callable]): Transformation function to apply to the data.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing historical ERA5 images, target ERA5 images, datetime index, and additional information.\n",
    "    '''\n",
    "\n",
    "    index, ind_start_current_step = tuple_index\n",
    "\n",
    "    try:\n",
    "        # select the ind_file based on the iter index\n",
    "        ind_file = find_key_for_number(ind_start_current_step, ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = ind_start_current_step - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(all_files[int(ind_file)]['time']) - (history_len + forecast_len + 1)\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension & load it to the memory\n",
    "        \n",
    "        ind_end_in_file = ind_start_in_file+history_len+forecast_len\n",
    "        \n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "        \n",
    "        if surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "            \n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(surface_subset) # <-- lazy merge, ERA5 and surface both not loaded\n",
    "        \n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with forcing and static\n",
    "        \n",
    "        # the ind_end of the ERA5_subset\n",
    "        # ind_end_time = len(ERA5_subset['time'])\n",
    "        \n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "        \n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "        \n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if dyn_forcing_files:\n",
    "            dyn_forcing_subset = dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "            \n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if xarray_forcing:\n",
    "            # =============================================================================== #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(np.array(xarray_forcing['time']))\n",
    "            month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time'])) # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = xarray_forcing.isel(time=ind_forcing).load() # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "            # =============================================================================== #\n",
    "        \n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset['time'])\n",
    "            static_subset_input = xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "        \n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "            # update \n",
    "            static_subset_input['time'] = historical_ERA5_images['time']\n",
    "        \n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "        \n",
    "        # get the next forecast step\n",
    "        target_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(history_len, history_len+skip_periods, skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "        ## merge diagnoisc input here:\n",
    "        if diagnostic_files:\n",
    "            \n",
    "            # subset diagnostic variables\n",
    "            diagnostic_subset = diagnostic_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            \n",
    "            # get the next forecast step\n",
    "            diagnostic_subset = diagnostic_subset.isel(\n",
    "                time=slice(history_len, history_len+skip_periods, skip_periods)\n",
    "            ).load() # <-- load into memory\n",
    "            \n",
    "            # merge into the target dataset\n",
    "            target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        # create a dict object with input/output tensors\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number\n",
    "        )\n",
    "\n",
    "        # data normalization\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "\n",
    "        sample[\"index\"] = index\n",
    "        stop_forecast = ((ind_start_current_step - index) == forecast_len)\n",
    "        sample['forecast_hour'] = ind_start_current_step - index + 1\n",
    "        sample['index'] = index\n",
    "        sample['stop_forecast'] = stop_forecast\n",
    "        sample[\"datetime\"] = [\n",
    "            int(historical_ERA5_images.time.values[0].astype('datetime64[s]').astype(int)),\n",
    "            int(target_ERA5_images.time.values[0].astype('datetime64[s]').astype(int))\n",
    "        ]\n",
    "\n",
    "        # # print out to check input and target datetimes\n",
    "        # print('Input time: {}'.format(np.array(historical_ERA5_images['time'])))\n",
    "        # print('Target time: {}'.format(np.array(target_ERA5_images['time'])))\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing index {tuple_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "class DistributedSequentialDatasetBasic(DistributedSequentialDataset):\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # get worker info\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "\n",
    "        # distributed sampler with worker info\n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers * self.world_size,\n",
    "                                     rank=self.rank * num_workers + worker_id, shuffle=self.shuffle)\n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "\n",
    "        for index in iter(sampler):\n",
    "\n",
    "            indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "            stop_forecast = False\n",
    "\n",
    "            for k, ind_start_current_step in enumerate(indices):\n",
    "\n",
    "                # select the ind_file based on the iter index\n",
    "                ind_file = find_key_for_number(ind_start_current_step, self.ERA5_indices)\n",
    "\n",
    "                # get the ind within the current file\n",
    "                ind_start = self.ERA5_indices[ind_file][1]\n",
    "                ind_start_in_file = ind_start_current_step - ind_start\n",
    "\n",
    "                # handle out-of-bounds\n",
    "                ind_largest = len(self.all_files[int(ind_file)]['time'])-(self.history_len+self.forecast_len+1)\n",
    "                if ind_start_in_file > ind_largest:\n",
    "                    ind_start_in_file = ind_largest\n",
    "                # ========================================================================== #\n",
    "                # subset xarray on time dimension & load it to the memory\n",
    "\n",
    "                ind_end_in_file = ind_start_in_file+self.history_len+self.forecast_len\n",
    "\n",
    "                ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "                ERA5_subset = self.all_files[int(ind_file)].isel(\n",
    "                    time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "\n",
    "                if self.surface_files:\n",
    "                    ## subset surface variables\n",
    "                    surface_subset = self.surface_files[int(ind_file)].isel(\n",
    "                        time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "\n",
    "                    ## merge upper-air and surface here:\n",
    "                    ERA5_subset = ERA5_subset.merge(surface_subset) # <-- lazy merge, ERA5 and surface both not loaded\n",
    "\n",
    "                # ==================================================== #\n",
    "                # split ERA5_subset into training inputs and targets + merge with forcing and static\n",
    "\n",
    "                # the ind_end of the ERA5_subset\n",
    "                # ind_end_time = len(ERA5_subset['time'])\n",
    "\n",
    "                # datetiem information as int number (used in some normalization methods)\n",
    "                datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "\n",
    "                # ==================================================== #\n",
    "                # xarray dataset as input\n",
    "                # historical_ERA5_images: the final input\n",
    "                \n",
    "                historical_ERA5_images = ERA5_subset.isel(\n",
    "                    time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "\n",
    "                # ========================================================================== #\n",
    "                # merge dynamic forcing inputs\n",
    "                if self.dyn_forcing_files:\n",
    "                    dyn_forcing_subset = self.dyn_forcing_files[int(ind_file)].isel(\n",
    "                        time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "                    dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                        time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "                    \n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "                # ========================================================================== #\n",
    "                # merge forcing inputs\n",
    "                if self.xarray_forcing:\n",
    "                    # =============================================================================== #\n",
    "                    # matching month, day, hour between forcing and upper air [time]\n",
    "                    # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "                    month_day_forcing = extract_month_day_hour(np.array(self.xarray_forcing['time']))\n",
    "                    month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time']))  # <-- upper air\n",
    "                    # indices to subset\n",
    "                    ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "                    forcing_subset_input = self.xarray_forcing.isel(time=ind_forcing).load()\n",
    "                    # forcing and upper air have different years but the same mon/day/hour\n",
    "                    # safely replace forcing time with upper air time\n",
    "                    forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "                    # =============================================================================== #\n",
    "\n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "                # ========================================================================== #\n",
    "                # merge static inputs\n",
    "                if self.xarray_static:\n",
    "                    # expand static var on time dim\n",
    "                    N_time_dims = len(ERA5_subset['time'])\n",
    "                    static_subset_input = self.xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "                    # assign coords 'time'\n",
    "                    static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "\n",
    "                    # slice + load to the GPU\n",
    "                    static_subset_input = static_subset_input.isel(\n",
    "                        time=slice(0, self.history_len, self.skip_periods)).load()\n",
    "\n",
    "                    # update\n",
    "                    static_subset_input['time'] = historical_ERA5_images['time']\n",
    "\n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "                # ==================================================== #\n",
    "                # xarray dataset as target\n",
    "                # target_ERA5_images: the final target\n",
    "                \n",
    "                # get the next forecast step\n",
    "                target_ERA5_images = ERA5_subset.isel(\n",
    "                    time=slice(self.history_len, self.history_len+self.skip_periods, self.skip_periods)\n",
    "                ).load() # <-- load into memory\n",
    "                \n",
    "                ## merge diagnoisc input here:\n",
    "                if self.diagnostic_files:\n",
    "                    \n",
    "                    # subset diagnostic variables\n",
    "                    diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                        time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "                    \n",
    "                    # get the next forecast step\n",
    "                    diagnostic_subset = diagnostic_subset.isel(\n",
    "                        time=slice(\n",
    "                            self.history_len, self.history_len+self.skip_periods, self.skip_periods)\n",
    "                    ).load() # <-- load into memory\n",
    "                    \n",
    "                    # merge into the target dataset\n",
    "                    target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "                # pipe xarray datasets to the sampler\n",
    "                sample = Sample(\n",
    "                    historical_ERA5_images=historical_ERA5_images,\n",
    "                    target_ERA5_images=target_ERA5_images,\n",
    "                    datetime_index=datetime_as_number\n",
    "                )\n",
    "\n",
    "                # ==================================== #\n",
    "                # data normalization\n",
    "                if self.transform:\n",
    "                    sample = self.transform(sample)\n",
    "\n",
    "                # assign sample index\n",
    "                sample[\"index\"] = index\n",
    "\n",
    "                stop_forecast = (k == self.forecast_len)\n",
    "\n",
    "                sample['forecast_hour'] = k + 1\n",
    "                sample['index'] = index\n",
    "                sample['stop_forecast'] = stop_forecast\n",
    "                sample[\"datetime\"] = [\n",
    "                    int(historical_ERA5_images.time.values[0].astype('datetime64[s]').astype(int)),\n",
    "                    int(target_ERA5_images.time.values[0].astype('datetime64[s]').astype(int))\n",
    "                ]\n",
    "\n",
    "                # # print out to check input and target datetimes\n",
    "                # print('Input time: {}'.format(np.array(historical_ERA5_images['time'])))\n",
    "                # print('Target time: {}'.format(np.array(target_ERA5_images['time'])))\n",
    "                \n",
    "                yield sample\n",
    "\n",
    "                if stop_forecast:\n",
    "                    break\n",
    "\n",
    "                if (k == self.forecast_len):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5518e3e4-a3d5-4d0a-8130-26b184db34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 3 # really its 4\n",
    "\n",
    "# Z-score\n",
    "dataset = DistributedSequentialDataset(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b17713-46d1-4d6c-b3c1-c6a8239b2e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089c5772-20c9-4673-b66b-88cac27de21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False torch.Size([1, 4, 15, 640, 1280])\n",
      "1 False torch.Size([1, 4, 15, 640, 1280])\n",
      "2 False torch.Size([1, 4, 15, 640, 1280])\n",
      "3 True torch.Size([1, 4, 15, 640, 1280])\n",
      "4 False torch.Size([1, 4, 15, 640, 1280])\n",
      "5 False torch.Size([1, 4, 15, 640, 1280])\n",
      "6 False torch.Size([1, 4, 15, 640, 1280])\n",
      "7 True torch.Size([1, 4, 15, 640, 1280])\n",
      "8 False torch.Size([1, 4, 15, 640, 1280])\n",
      "9 False torch.Size([1, 4, 15, 640, 1280])\n",
      "10 False torch.Size([1, 4, 15, 640, 1280])\n",
      "11 True torch.Size([1, 4, 15, 640, 1280])\n",
      "12 False torch.Size([1, 4, 15, 640, 1280])\n",
      "13 False torch.Size([1, 4, 15, 640, 1280])\n",
      "14 False torch.Size([1, 4, 15, 640, 1280])\n",
      "15 True torch.Size([1, 4, 15, 640, 1280])\n"
     ]
    }
   ],
   "source": [
    "for k, result in enumerate(dataset):\n",
    "    print(k, result['stop_forecast'], result['x'].shape)\n",
    "    if (k + 1) == forecast_len * 5 + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebc090-e0e8-4525-a0a8-463e331f1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b14b27a-a835-455f-bf3a-23ac157b276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.7914, 0.7915, 0.7916,  ..., 0.7912, 0.7913, 0.7914],\n",
       "        [0.7858, 0.7859, 0.7859,  ..., 0.7857, 0.7857, 0.7858],\n",
       "        [0.7803, 0.7803, 0.7803,  ..., 0.7802, 0.7802, 0.7803]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['x_forcing_static'][0, 0, ...]/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32734d8-8a6c-4bc2-a200-a78187d40185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc52e2-ba21-42a4-90b5-8e5a577e4ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f5a3c-4561-43bd-a551-67868825b3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bcde0e-b85b-4454-83f2-2da8bebe1584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5319f0c7-8e87-4b7d-838d-3c81046f7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 3 # really its 4\n",
    "\n",
    "# Z-score\n",
    "dataset = DistributedSequentialDatasetBasic(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=None,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e502e-86fb-4f90-afdd-0ab1950dbb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69a9e8d-6cd6-4c95-9647-8d11b72af4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dyn = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edbc165-0921-45ea-ad5e-0dac45608223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'historical_ERA5_images': <xarray.Dataset> Size: 233MB\n",
       " Dimensions:     (time: 1, level: 15, latitude: 640, longitude: 1280,\n",
       "                  half_level: 138)\n",
       " Coordinates:\n",
       "   * half_level  (half_level) int32 552B 1 2 3 4 5 6 ... 133 134 135 136 137 138\n",
       "   * latitude    (latitude) float64 5kB 89.78 89.51 89.23 ... -89.51 -89.78\n",
       "   * level       (level) int32 60B 10 30 40 50 60 70 ... 100 105 110 120 130 136\n",
       "   * longitude   (longitude) float64 10kB 0.0 0.2812 0.5625 ... 359.2 359.4 359.7\n",
       "   * time        (time) datetime64[ns] 8B 1979-01-01\n",
       " Data variables: (12/14)\n",
       "     Q           (time, level, latitude, longitude) float32 49MB -2.874 ... -1...\n",
       "     T           (time, level, latitude, longitude) float32 49MB 4.688 ... -1.385\n",
       "     U           (time, level, latitude, longitude) float32 49MB -1.259 ... -0...\n",
       "     V           (time, level, latitude, longitude) float32 49MB 2.979 ... -1.274\n",
       "     Q500        (time, latitude, longitude) float32 3MB -0.6879 ... -0.63\n",
       "     SP          (time, latitude, longitude) float32 3MB 0.6142 0.6142 ... -2.768\n",
       "     ...          ...\n",
       "     V500        (time, latitude, longitude) float32 3MB -1.128 ... -0.4917\n",
       "     Z500        (time, latitude, longitude) float32 3MB -1.455 ... -0.7947\n",
       "     t2m         (time, latitude, longitude) float32 3MB -1.634 -1.634 ... -1.373\n",
       "     tsi         (time, latitude, longitude) float32 3MB 0.0 0.0 ... 1.182e+07\n",
       "     Z_GDS4_SFC  (time, latitude, longitude) float64 7MB 0.06226 ... 0.06226\n",
       "     LSM         (time, latitude, longitude) float32 3MB 0.0 0.0 0.0 ... 1.0 1.0,\n",
       " 'target_ERA5_images': <xarray.Dataset> Size: 220MB\n",
       " Dimensions:     (time: 1, level: 15, latitude: 640, longitude: 1280,\n",
       "                  half_level: 138)\n",
       " Coordinates:\n",
       "   * half_level  (half_level) int32 552B 1 2 3 4 5 6 ... 133 134 135 136 137 138\n",
       "   * latitude    (latitude) float64 5kB 89.78 89.51 89.23 ... -89.51 -89.78\n",
       "   * level       (level) int32 60B 10 30 40 50 60 70 ... 100 105 110 120 130 136\n",
       "   * longitude   (longitude) float64 10kB 0.0 0.2812 0.5625 ... 359.2 359.4 359.7\n",
       "   * time        (time) datetime64[ns] 8B 1979-01-01T06:00:00\n",
       " Data variables:\n",
       "     Q           (time, level, latitude, longitude) float32 49MB -3.733 ... -1...\n",
       "     T           (time, level, latitude, longitude) float32 49MB 4.477 ... -1.429\n",
       "     U           (time, level, latitude, longitude) float32 49MB -1.359 ... -0...\n",
       "     V           (time, level, latitude, longitude) float32 49MB 1.778 ... -1.197\n",
       "     Q500        (time, latitude, longitude) float32 3MB -0.5641 ... -0.5983\n",
       "     SP          (time, latitude, longitude) float32 3MB 0.6396 0.6396 ... -2.768\n",
       "     T500        (time, latitude, longitude) float32 3MB -1.195 ... -0.6881\n",
       "     U500        (time, latitude, longitude) float32 3MB -2.403 -2.41 ... -0.3427\n",
       "     V500        (time, latitude, longitude) float32 3MB -1.802 ... -0.4628\n",
       "     Z500        (time, latitude, longitude) float32 3MB -1.166 ... -0.8005\n",
       "     t2m         (time, latitude, longitude) float32 3MB -1.574 -1.574 ... -1.399,\n",
       " 'datetime_index': array([283996800, 284018400, 284040000, 284061600, 284083200]),\n",
       " 'index': 0,\n",
       " 'forecast_hour': 1,\n",
       " 'stop_forecast': False,\n",
       " 'datetime': [283996800, 284018400]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca305ba-d3e9-4c31-96a6-7b3ae3b7db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.transforms import Normalize_ERA5_and_Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de2bee05-02be-4195-8b1d-14611165311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_scaler = Normalize_ERA5_and_Forcing(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4db91c-1edd-471f-9452-fffed2c850b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 233MB\n",
      "Dimensions:     (time: 1, level: 15, latitude: 640, longitude: 1280,\n",
      "                 half_level: 138)\n",
      "Coordinates:\n",
      "  * half_level  (half_level) int32 552B 1 2 3 4 5 6 ... 133 134 135 136 137 138\n",
      "  * latitude    (latitude) float64 5kB 89.78 89.51 89.23 ... -89.51 -89.78\n",
      "  * level       (level) int32 60B 10 30 40 50 60 70 ... 100 105 110 120 130 136\n",
      "  * longitude   (longitude) float64 10kB 0.0 0.2812 0.5625 ... 359.2 359.4 359.7\n",
      "  * time        (time) datetime64[ns] 8B 1979-01-01\n",
      "Data variables: (12/14)\n",
      "    Q           (time, level, latitude, longitude) float32 49MB -2.874 ... -1...\n",
      "    T           (time, level, latitude, longitude) float32 49MB 4.688 ... -1.385\n",
      "    U           (time, level, latitude, longitude) float32 49MB -1.259 ... -0...\n",
      "    V           (time, level, latitude, longitude) float32 49MB 2.979 ... -1.274\n",
      "    Q500        (time, latitude, longitude) float32 3MB -0.6879 ... -0.63\n",
      "    SP          (time, latitude, longitude) float32 3MB 0.6142 0.6142 ... -2.768\n",
      "    ...          ...\n",
      "    V500        (time, latitude, longitude) float32 3MB -1.128 ... -0.4917\n",
      "    Z500        (time, latitude, longitude) float32 3MB -1.455 ... -0.7947\n",
      "    t2m         (time, latitude, longitude) float32 3MB -1.634 -1.634 ... -1.373\n",
      "    tsi         (time, latitude, longitude) float32 3MB 0.0 0.0 ... 1.182e+07\n",
      "    Z_GDS4_SFC  (time, latitude, longitude) float64 7MB 0.06226 ... 0.06226\n",
      "    LSM         (time, latitude, longitude) float32 3MB 0.0 0.0 0.0 ... 1.0 1.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"No variable named 'tsi'. Variables on the dataset include ['half_level', 'Q', 'T', 'U', 'V', ..., 'T500', 'U500', 'Q500', 'V500', 'Z500']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tsi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   1576\u001b[0m                     \u001b[0;34mf\"No variable named {key!r}. Variables on the dataset include {shorten_list_repr(list(self.variables.keys()), max_items=10)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0msplit_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tsi'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/derecho/scratch/ksha/tmp/ipykernel_23048/59416147.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_dyn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/credit/transforms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sample, inverse)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# inverse transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/credit/transforms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mvarname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarname_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                             \u001b[0;31m# if forcing and static skip it, otherwise do z-score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvarname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarname_forcing_static\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                                 \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0;31m# put transformed xr.Dataset to the output dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                         \u001b[0mnormalized_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   1576\u001b[0m                     \u001b[0;34mf\"No variable named {key!r}. Variables on the dataset include {shorten_list_repr(list(self.variables.keys()), max_items=10)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m                 ) from e\n\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"No variable named 'tsi'. Variables on the dataset include ['half_level', 'Q', 'T', 'U', 'V', ..., 'T500', 'U500', 'Q500', 'V500', 'Z500']\""
     ]
    }
   ],
   "source": [
    "transform_scaler(samples_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a17bc-9210-46a2-afe4-fb30e10f5d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78115e69-6b0d-4c57-ab88-7f3b6631d9e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"No variable named 'tsi'. Variables on the dataset include ['half_level', 'Q', 'T', 'U', 'V', ..., 'T500', 'U500', 'Q500', 'V500', 'Z500']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tsi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   1576\u001b[0m                     \u001b[0;34mf\"No variable named {key!r}. Variables on the dataset include {shorten_list_repr(list(self.variables.keys()), max_items=10)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0msplit_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tsi'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/glade/derecho/scratch/ksha/tmp/ipykernel_21770/4190509979.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stop_forecast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mforecast_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/derecho/scratch/ksha/tmp/ipykernel_21770/1525587566.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;31m# ==================================== #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0;31m# data normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0;31m# assign sample index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/credit/transforms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sample, inverse)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# inverse transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/credit/transforms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mvarname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarname_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                             \u001b[0;31m# if forcing and static skip it, otherwise do z-score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvarname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarname_forcing_static\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                                 \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0;31m# put transformed xr.Dataset to the output dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                         \u001b[0mnormalized_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   1576\u001b[0m                     \u001b[0;34mf\"No variable named {key!r}. Variables on the dataset include {shorten_list_repr(list(self.variables.keys()), max_items=10)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m                 ) from e\n\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"No variable named 'tsi'. Variables on the dataset include ['half_level', 'Q', 'T', 'U', 'V', ..., 'T500', 'U500', 'Q500', 'V500', 'Z500']\""
     ]
    }
   ],
   "source": [
    "for k, result in enumerate(dataset):\n",
    "    print(k, result['stop_forecast'], result['x'].shape)\n",
    "    if (k + 1) == forecast_len * 5 + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df99e7c-0ec0-4eec-b5b9-f980c52a5bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142ad84-d763-4dc4-9295-7e2afece3bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fb02e-8bcf-4d97-ad43-0aef00d88c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d852ae9-545e-4a77-92e9-5bb40dd91004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
