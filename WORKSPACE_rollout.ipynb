{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb99dd3-1f7c-403f-ac16-f312ba51ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- #\n",
    "# System\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import warnings\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ---------- #\n",
    "# Numerics\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------- #\n",
    "# AI libs\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "# import wandb\n",
    "\n",
    "# ---------- #\n",
    "# credit\n",
    "from credit.data import Predict_Dataset\n",
    "#ERA5Dataset, ERA5_and_Forcing_Dataset, get_forward_data, concat_and_reshape, drop_var_from_dataset\n",
    "from credit.models import load_model\n",
    "from credit.transforms import load_transforms\n",
    "from credit.seed import seed_everything\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.pol_lapdiff_filt import Diffusion_and_Pole_Filter\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "from credit.models.checkpoint import load_model_state\n",
    "from credit.solar import TOADataLoader\n",
    "from credit.output import split_and_reshape, load_metadata, make_xarray, save_netcdf_increment\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5d0f4f-f072-4eb1-9e11-a8cd7fbc201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/glade/u/home/ksha/miles-credit/results/fuxi_norm/model_new.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe2702-039c-4f56-8a35-4e3f0ba53271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fda11a7-1efc-4e74-a56d-2e99f1118ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "time_step = conf[\"data\"][\"time_step\"] if \"time_step\" in conf[\"data\"] else None\n",
    "\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "transform = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fd29b3-9691-4373-8352-9867f347227b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a54c50-7994-4b48-bee7-6de0e8351f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b38a99-0721-4017-b15e-ee1d52ce83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Predict_Dataset(torch.utils.data.IterableDataset):\n",
    "#     def __init__(self,\n",
    "#                  conf, \n",
    "#                  varname_upper_air,\n",
    "#                  varname_surface,\n",
    "#                  varname_forcing,\n",
    "#                  varname_static,\n",
    "#                  filenames,\n",
    "#                  filename_surface,\n",
    "#                  filename_forcing,\n",
    "#                  filename_static,\n",
    "#                  fcst_datetime,\n",
    "#                  history_len,\n",
    "#                  rank,\n",
    "#                  world_size,\n",
    "#                  transform=None,\n",
    "#                  rollout_p=0.0,\n",
    "#                  which_forecast=None):\n",
    "        \n",
    "#         # ------------------------------------------------------------------------------ #\n",
    "        \n",
    "#         ## no diagnostics because they are output only\n",
    "#         varname_diagnostic = None\n",
    "        \n",
    "#         self.rank = rank\n",
    "#         self.world_size = world_size\n",
    "#         self.transform = transform\n",
    "#         self.history_len = history_len\n",
    "#         self.fcst_datetime = fcst_datetime\n",
    "#         self.which_forecast = which_forecast # <-- got from the old roll-out. Dont know \n",
    "        \n",
    "#         # -------------------------------------- #\n",
    "#         self.filenames = sorted(filenames)\n",
    "#         self.filename_surface = sorted(filename_surface)\n",
    "#         self.filename_forcing = forcing_files\n",
    "#         self.filename_static = static_files\n",
    "        \n",
    "#         # -------------------------------------- #\n",
    "#         self.varname_upper_air = varname_upper_air\n",
    "#         self.varname_surface = varname_surface\n",
    "#         self.varname_forcing = varname_forcing\n",
    "#         self.varname_static = varname_static\n",
    "\n",
    "#         # ====================================== #\n",
    "#         # import all upper air zarr files\n",
    "#         all_files = []\n",
    "#         for fn in self.filenames:\n",
    "#             # drop variables if they are not in the config\n",
    "#             xarray_dataset = get_forward_data(filename=fn)\n",
    "#             xarray_dataset = drop_var_from_dataset(xarray_dataset, self.varname_upper_air)\n",
    "#             # collect yearly datasets within a list\n",
    "#             all_files.append(xarray_dataset)\n",
    "#         self.all_files = all_files\n",
    "#         # ====================================== #\n",
    "\n",
    "#         # -------------------------------------- #\n",
    "#         # other settings\n",
    "#         self.current_epoch = 0\n",
    "#         self.rollout_p = rollout_p\n",
    "\n",
    "#     def ds_read_and_subset(self, filename, time_start, time_end, varnames):\n",
    "#         sliced_x = xr.open_zarr(filename, consolidated=True)\n",
    "#         sliced_x = sliced_x.isel(time=slice(time_start, time_end))\n",
    "#         sliced_x = drop_var_from_dataset(sliced_x, varnames)\n",
    "#         return sliced_x\n",
    "\n",
    "#     def load_zarr_as_input(self, file_key, time_key):\n",
    "#         # get the needed file from a list of zarr files\n",
    "#         # open the zarr file as xr.dataset and subset based on the needed time\n",
    "        \n",
    "#         # sliced_x: the final output, starts with an upper air xr.dataset\n",
    "#         sliced_x = self.ds_read_and_subset(self.filenames[file_key], \n",
    "#                                            time_key, \n",
    "#                                            time_key+self.history_len+1, \n",
    "#                                            self.varname_upper_air)\n",
    "#         # surface variables\n",
    "#         if self.varname_surface is not None:\n",
    "#             sliced_surface = self.ds_read_and_subset(self.filename_surface[file_key], \n",
    "#                                                      time_key, \n",
    "#                                                      time_key+self.history_len+1, \n",
    "#                                                      self.varname_surface)\n",
    "#             # merge surface to sliced_x\n",
    "#             sliced_surface['time'] = sliced_x['time']\n",
    "#             sliced_x = sliced_x.merge(sliced_surface)\n",
    "            \n",
    "#         # forcing / static\n",
    "#         if self.filename_forcing is not None:\n",
    "#             sliced_forcing = xr.open_dataset(self.filename_forcing)\n",
    "#             sliced_forcing = drop_var_from_dataset(sliced_forcing, self.varname_forcing)\n",
    "#             sliced_forcing = sliced_forcing.isel(time=slice(time_key, time_key+self.history_len+1))\n",
    "#             sliced_forcing['time'] = sliced_x['time']\n",
    "#             # merge forcing to sliced_x\n",
    "#             sliced_x = sliced_x.merge(sliced_forcing)\n",
    "            \n",
    "#         if self.filename_static is not None:\n",
    "#             sliced_static = xr.open_dataset(self.filename_static)\n",
    "#             sliced_static = drop_var_from_dataset(sliced_static, self.varname_static)\n",
    "#             sliced_static = sliced_static.expand_dims(dim={\"time\": len(sliced_x['time'])})\n",
    "#             sliced_static['time'] = sliced_x['time']\n",
    "#             # merge static to sliced_x\n",
    "#             sliced_x = sliced_x.merge(sliced_static)\n",
    "#         return sliced_x\n",
    "\n",
    "    \n",
    "#     def find_start_stop_indices(self, index):\n",
    "#         # convert the first forecasted time to initialization time\n",
    "#         # by subtracting the forecast length (assuming 1 step)\n",
    "#         # other later forecasted time are viewed as init time directly \n",
    "#         # becuase their previous step forecasted time are init times of the later forecasted time\n",
    "#         start_time = self.fcst_datetime[index][0] # string\n",
    "#         date_object = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "#         shifted_hours = self.history_len\n",
    "#         date_object = date_object - datetime.timedelta(hours=shifted_hours)\n",
    "#         self.fcst_datetime[index][0] = date_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#         # convert all strings to np.datetime64\n",
    "#         datetime_objs = [np.datetime64(date) for date in self.fcst_datetime[index]]\n",
    "#         start_time, stop_time = [str(datetime_obj) + '.000000000' for datetime_obj in datetime_objs]\n",
    "#         self.start_time = np.datetime64(start_time).astype(datetime.datetime)\n",
    "#         self.stop_time = np.datetime64(stop_time).astype(datetime.datetime)\n",
    "\n",
    "#         info = {}\n",
    "\n",
    "#         for idx, dataset in enumerate(self.all_files):\n",
    "#             start_time = np.datetime64(dataset['time'].min().values).astype(datetime.datetime)\n",
    "#             stop_time = np.datetime64(dataset['time'].max().values).astype(datetime.datetime)\n",
    "#             track_start = False\n",
    "#             track_stop = False\n",
    "\n",
    "#             if start_time <= self.start_time <= stop_time:\n",
    "#                 # Start time is in this file, use start time index\n",
    "#                 dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "#                 start_idx = np.searchsorted(dataset, self.start_time)\n",
    "#                 start_idx = max(0, min(start_idx, len(dataset)-1))\n",
    "#                 track_start = True\n",
    "\n",
    "#             elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "#                 # File overlaps time range, use full file\n",
    "#                 start_idx = 0\n",
    "#                 track_start = True\n",
    "\n",
    "#             if start_time <= self.stop_time <= stop_time:\n",
    "#                 # Stop time is in this file, use stop time index\n",
    "#                 if isinstance(dataset, np.ndarray):\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "#                 stop_idx = np.searchsorted(dataset, self.stop_time)\n",
    "#                 stop_idx = max(0, min(stop_idx, len(dataset)-1))\n",
    "#                 track_stop = True\n",
    "\n",
    "#             elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "#                 # File overlaps time range, use full file\n",
    "#                 stop_idx = len(dataset) - 1\n",
    "#                 track_stop = True\n",
    "\n",
    "#             # Only include files that overlap the time range\n",
    "#             if track_start and track_stop:\n",
    "#                 info[idx] = ((idx, start_idx), (idx, stop_idx))\n",
    "\n",
    "#         indices = []\n",
    "#         for dataset_idx, (start, stop) in info.items():\n",
    "#             for i in range(start[1], stop[1]+1):\n",
    "#                 indices.append((start[0], i))\n",
    "#         return indices\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.fcst_datetime)\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         worker_info = get_worker_info()\n",
    "#         num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "#         worker_id = worker_info.id if worker_info is not None else 0\n",
    "#         sampler = DistributedSampler(self, \n",
    "#                                      num_replicas=num_workers*self.world_size, \n",
    "#                                      rank=self.rank*num_workers+worker_id, \n",
    "#                                      shuffle=False)\n",
    "#         for index in sampler:\n",
    "#             # get time indices for inputs\n",
    "#             data_lookup = self.find_start_stop_indices(index)\n",
    "#             for k, (file_key, time_key) in enumerate(data_lookup):\n",
    "                \n",
    "#                 if k == 0:\n",
    "#                     output_dict = {}\n",
    "#                     # get all inputs (upper air, surface, forcing, static ) in one xr.Dataset\n",
    "#                     sliced_x = self.load_zarr_as_input(file_key, time_key)\n",
    "                    \n",
    "#                     # Check if additional data from the next file is needed\n",
    "#                     if len(sliced_x['time']) < self.history_len + 1:\n",
    "                        \n",
    "#                         # Load excess data from the next file\n",
    "#                         next_file_idx = self.filenames.index(self.filenames[file_key]) + 1\n",
    "                        \n",
    "#                         if next_file_idx == len(self.filenames):\n",
    "#                             # not enough input data to support this forecast\n",
    "#                             raise OSError(\"You have reached the end of the available data. Exiting.\")\n",
    "                            \n",
    "#                         else:\n",
    "#                             # time_key = 0 because we need the beginning of the next file only\n",
    "#                             sliced_x_next = self.load_zarr_as_input(next_file_idx, 0)\n",
    "                            \n",
    "#                             # Concatenate excess data from the next file with the current data\n",
    "#                             sliced_x = xr.concat([sliced_x, sliced_x_next], dim='time')\n",
    "\n",
    "#                     # key 'historical_ERA5_images' is recongnized as input in credit.transform\n",
    "#                     sample_x = {\n",
    "#                         'historical_ERA5_images': sliced_x.isel(time=slice(0, self.history_len))\n",
    "#                     }\n",
    "                    \n",
    "#                     if self.transform:\n",
    "#                         sample_x = self.transform(sample_x)\n",
    "                        \n",
    "#                     for key in sample_x.keys():\n",
    "#                         output_dict[key] = sample_x[key]\n",
    "                        \n",
    "#                     output_dict['forecast_hour'] = k + 1\n",
    "#                     output_dict['datetime'] = sliced_x.time.values.astype('datetime64[s]').astype(int)[-1]\n",
    "                    \n",
    "#                     # Adjust stopping condition\n",
    "#                     output_dict['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1)) \n",
    "#                 else:\n",
    "#                     output_dict['forecast_hour'] = k + 1\n",
    "#                     # Adjust stopping condition\n",
    "#                     output_dict['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))  \n",
    "                    \n",
    "#                 yield output_dict\n",
    "\n",
    "#                 if output_dict['stop_forecast']:\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81457272-7a50-4cbe-af8b-43b76f76c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1\n",
    "\n",
    "dataset = Predict_Dataset(\n",
    "    conf, \n",
    "    varname_upper_air,\n",
    "    varname_surface,\n",
    "    varname_forcing,\n",
    "    varname_static,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    fcst_datetime=load_forecasts(conf),\n",
    "    history_len=2,\n",
    "    rank=rank,\n",
    "    world_size=world_size,\n",
    "    transform=transform,\n",
    "    rollout_p=0.0,\n",
    "    which_forecast=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0964675e-e759-4cdb-bb3d-5b8069345ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2018-06-01 00:00:00', '2018-06-01 02:00:00']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_forecasts(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5271799c-0828-4819-9d65-0a4a9203a90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f539603c-e0d7-4c92-8784-3d1f79e1dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1052, -0.1064, -0.1076,  ..., -0.1018, -0.1028, -0.1040],\n",
       "        [-0.1237, -0.1249, -0.1261,  ..., -0.1201, -0.1213, -0.1225],\n",
       "        [-0.1413, -0.1425, -0.1437,  ..., -0.1378, -0.1390, -0.1402],\n",
       "        ...,\n",
       "        [-1.2384, -1.2350, -1.2317,  ..., -1.2483, -1.2450, -1.2417],\n",
       "        [-1.2458, -1.2425, -1.2394,  ..., -1.2554, -1.2522, -1.2490],\n",
       "        [-1.2406, -1.2375, -1.2344,  ..., -1.2496, -1.2466, -1.2436]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x'][0, 0, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d908121f-4bee-48fb-81e9-8945e8aa554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the dataloder for this process\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a0d2e2-6219-4b3a-9fc2-13e6b25d602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n",
      "tensor(-0.5057)\n",
      "-----------\n",
      "tensor(0.8152)\n",
      "-----------\n",
      "tensor(0.6407)\n"
     ]
    }
   ],
   "source": [
    "for test in data_loader:\n",
    "    print(test['x'][0, 0, 0, 0, 320, 640])\n",
    "    print('-----------')\n",
    "    print(test['x_surf'][0, 1, 6, 320, 640])\n",
    "    print('-----------')\n",
    "    print(test['x_forcing_static'][0, 0, 0, 320, 640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f89e8b-60ab-420c-86cf-52c170937fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123321f-ff81-461d-8634-9323db2db2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
