{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf47a0c-30e8-4753-b604-836211a1089d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d388269-02f7-4711-814e-df545405128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from credit.models.base_model import BaseModel\n",
    "from credit.postblock import PostBlock\n",
    "from credit.boundary_padding import TensorPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992f69b8-3dfa-40cf-a4bf-8181fad53609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.parser import (\n",
    "    CREDIT_main_parser,\n",
    "    training_data_check\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe4d1eb0-0297-473f-977b-07c91c68be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d09b3a-952b-4205-8f51-f436ba94c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03277f24-c598-41fb-b202-161c19eeb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from credit.boundary_padding import TensorPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760f0aa0-23df-4e11-b666-10140b6d8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_tuple(val, length=1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * length)\n",
    "\n",
    "def apply_spectral_norm(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "            nn.utils.spectral_norm(module)\n",
    "\n",
    "\n",
    "# cube embedding\n",
    "\n",
    "class CubeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_size: T, Lat, Lon\n",
    "        patch_size: T, Lat, Lon\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2] // patch_size[2]]\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C, Lat, Lon = x.shape\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # ----------------------------------- #\n",
    "        # Layer norm on T*lat*lon\n",
    "        x = x.reshape(B, self.embed_dim, -1).transpose(1, 2)  # B T*Lat*Lon C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        x = x.transpose(1, 2).reshape(B, self.embed_dim, *self.patches_resolution)\n",
    "\n",
    "        return x.squeeze(2)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, num_groups, num_residuals=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2)\n",
    "\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            blk.append(nn.Conv2d(out_chans, out_chans, kernel_size=3, stride=1, padding=1))\n",
    "            blk.append(nn.GroupNorm(num_groups, out_chans))\n",
    "            blk.append(nn.SiLU())\n",
    "\n",
    "        self.b = nn.Sequential(*blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.b(x)\n",
    "\n",
    "        return x + shortcut\n",
    "\n",
    "\n",
    "# cross embed layer\n",
    "\n",
    "class CrossEmbedLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        kernel_sizes,\n",
    "        stride=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        kernel_sizes = sorted(kernel_sizes)\n",
    "        num_scales = len(kernel_sizes)\n",
    "\n",
    "        # calculate the dimension at each scale\n",
    "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n",
    "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n",
    "\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n",
    "            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride=stride, padding=(kernel - stride) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n",
    "        return torch.cat(fmaps, dim=1)\n",
    "\n",
    "\n",
    "# dynamic positional bias\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(DynamicPositionBias, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1),\n",
    "            Rearrange('... () -> ...')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            LayerNorm(dim),\n",
    "            nn.Conv2d(dim, dim * mult, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim * mult, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        attn_type,\n",
    "        window_size,\n",
    "        dim_head=32,\n",
    "        dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert attn_type in {'short', 'long'}, 'attention type must be one of local or distant'\n",
    "        heads = dim // dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.attn_type = attn_type\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(inner_dim, dim, 1)\n",
    "\n",
    "        # positions\n",
    "\n",
    "        self.dpb = DynamicPositionBias(dim // 4)\n",
    "\n",
    "        # calculate and store indices for retrieving bias\n",
    "\n",
    "        pos = torch.arange(window_size)\n",
    "        grid = torch.stack(torch.meshgrid(pos, pos, indexing='ij'))\n",
    "        grid = rearrange(grid, 'c i j -> (i j) c')\n",
    "        rel_pos = grid[:, None] - grid[None, :]\n",
    "        rel_pos += window_size - 1\n",
    "        rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(dim=-1)\n",
    "\n",
    "        self.register_buffer('rel_pos_indices', rel_pos_indices, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        *_, height, width, heads, wsz, device = *x.shape, self.heads, self.window_size, x.device\n",
    "\n",
    "        # prenorm\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # rearrange for short or long distance attention\n",
    "\n",
    "        if self.attn_type == 'short':\n",
    "            x = rearrange(x, 'b d (h s1) (w s2) -> (b h w) d s1 s2', s1=wsz, s2=wsz)\n",
    "        elif self.attn_type == 'long':\n",
    "            x = rearrange(x, 'b d (l1 h) (l2 w) -> (b h w) d l1 l2', l1=wsz, l2=wsz)\n",
    "\n",
    "        # queries / keys / values\n",
    "\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=1)\n",
    "\n",
    "        # split heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> b h (x y) d', h=heads), (q, k, v))\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        # add dynamic positional bias\n",
    "\n",
    "        pos = torch.arange(-wsz, wsz + 1, device=device)\n",
    "        rel_pos = torch.stack(torch.meshgrid(pos, pos, indexing='ij'))\n",
    "        rel_pos = rearrange(rel_pos, 'c i j -> (i j) c')\n",
    "        biases = self.dpb(rel_pos.float())\n",
    "        rel_pos_bias = biases[self.rel_pos_indices]\n",
    "\n",
    "        sim = sim + rel_pos_bias\n",
    "\n",
    "        # attend\n",
    "\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # merge heads\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=wsz, y=wsz)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        # rearrange back for long or short distance attention\n",
    "\n",
    "        if self.attn_type == 'short':\n",
    "            out = rearrange(out, '(b h w) d s1 s2 -> b d (h s1) (w s2)', h=height // wsz, w=width // wsz)\n",
    "        elif self.attn_type == 'long':\n",
    "            out = rearrange(out, '(b h w) d l1 l2 -> b d (l1 h) (l2 w)', h=height // wsz, w=width // wsz)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        local_window_size,\n",
    "        global_window_size,\n",
    "        depth=4,\n",
    "        dim_head=32,\n",
    "        attn_dropout=0.,\n",
    "        ff_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, attn_type='short', window_size=local_window_size, dim_head=dim_head, dropout=attn_dropout),\n",
    "                FeedForward(dim, dropout=ff_dropout),\n",
    "                Attention(dim, attn_type='long', window_size=global_window_size, dim_head=dim_head, dropout=attn_dropout),\n",
    "                FeedForward(dim, dropout=ff_dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for short_attn, short_ff, long_attn, long_ff in self.layers:\n",
    "            x = short_attn(x) + x\n",
    "            x = short_ff(x) + x\n",
    "            x = long_attn(x) + x\n",
    "            x = long_ff(x) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "class CrossFormer(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_height=640,\n",
    "        patch_height=1,\n",
    "        image_width=1280,\n",
    "        patch_width=1,\n",
    "        frames=2,\n",
    "        channels=4,\n",
    "        surface_channels=7,\n",
    "        input_only_channels=3,\n",
    "        output_only_channels=0,\n",
    "        levels=15,\n",
    "        dim=(64, 128, 256, 512),\n",
    "        depth=(2, 2, 8, 2),\n",
    "        dim_head=32,\n",
    "        global_window_size=(5, 5, 2, 1),\n",
    "        local_window_size=10,\n",
    "        cross_embed_kernel_sizes=((4, 8, 16, 32), (2, 4), (2, 4), (2, 4)),\n",
    "        cross_embed_strides=(4, 2, 2, 2),\n",
    "        attn_dropout=0.,\n",
    "        ff_dropout=0.,\n",
    "        use_spectral_norm=True,\n",
    "        interp=True,\n",
    "        padding_conf=None,\n",
    "        post_conf=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        dim = tuple(dim)\n",
    "        depth = tuple(depth)\n",
    "        global_window_size = tuple(global_window_size)\n",
    "        cross_embed_kernel_sizes = tuple([tuple(_) for _ in cross_embed_kernel_sizes])\n",
    "        cross_embed_strides = tuple(cross_embed_strides)\n",
    "\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.patch_height = patch_height\n",
    "        self.patch_width = patch_width\n",
    "        self.frames = frames\n",
    "        self.channels = channels\n",
    "        self.surface_channels = surface_channels\n",
    "        self.levels = levels\n",
    "        self.use_spectral_norm = use_spectral_norm\n",
    "        self.use_interp = interp\n",
    "        if padding_conf is None:\n",
    "            padding_conf = {'activate': False}\n",
    "        self.use_padding =  padding_conf['activate']\n",
    "        if post_conf is None:\n",
    "            post_conf = {\"activate\": False}\n",
    "        self.use_post_block = post_conf['activate']\n",
    "        \n",
    "        # input channels\n",
    "        input_channels = channels * levels + surface_channels + input_only_channels\n",
    "\n",
    "        # output channels\n",
    "        output_channels = channels * levels + surface_channels + output_only_channels\n",
    "\n",
    "        dim = cast_tuple(dim, 4)\n",
    "        depth = cast_tuple(depth, 4)\n",
    "        global_window_size = cast_tuple(global_window_size, 4)\n",
    "        local_window_size = cast_tuple(local_window_size, 4)\n",
    "        cross_embed_kernel_sizes = cast_tuple(cross_embed_kernel_sizes, 4)\n",
    "        cross_embed_strides = cast_tuple(cross_embed_strides, 4)\n",
    "\n",
    "        assert len(dim) == 4\n",
    "        assert len(depth) == 4\n",
    "        assert len(global_window_size) == 4\n",
    "        assert len(local_window_size) == 4\n",
    "        assert len(cross_embed_kernel_sizes) == 4\n",
    "        assert len(cross_embed_strides) == 4\n",
    "\n",
    "        # dimensions\n",
    "        last_dim = dim[-1]\n",
    "        first_dim = input_channels if (patch_height == 1 and patch_width == 1) else dim[0]\n",
    "        dims = [first_dim, *dim]\n",
    "        dim_in_and_out = tuple(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # allocate cross embed layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        # loop through hyperparameters\n",
    "        for (dim_in, dim_out), num_layers, global_wsize, local_wsize, kernel_sizes, stride in zip(\n",
    "            dim_in_and_out,\n",
    "            depth,\n",
    "            global_window_size,\n",
    "            local_window_size,\n",
    "            cross_embed_kernel_sizes,\n",
    "            cross_embed_strides\n",
    "        ):\n",
    "            # create CrossEmbedLayer\n",
    "            cross_embed_layer = CrossEmbedLayer(\n",
    "                dim_in=dim_in,\n",
    "                dim_out=dim_out,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                stride=stride\n",
    "            )\n",
    "            \n",
    "            # create Transformer\n",
    "            transformer_layer = Transformer(\n",
    "                dim=dim_out,\n",
    "                local_window_size=local_wsize,\n",
    "                global_window_size=global_wsize,\n",
    "                depth=num_layers,\n",
    "                dim_head=dim_head,\n",
    "                attn_dropout=attn_dropout,\n",
    "                ff_dropout=ff_dropout\n",
    "            )\n",
    "            \n",
    "            # append everything\n",
    "            self.layers.append(\n",
    "                nn.ModuleList([\n",
    "                    cross_embed_layer,\n",
    "                    transformer_layer\n",
    "                ])\n",
    "            )\n",
    "        \n",
    "        if self.use_padding:\n",
    "            self.padding_opt = TensorPadding(**padding_conf)\n",
    "            \n",
    "        # define embedding layer using adjusted sizes\n",
    "        # if the original sizes were good, adjusted sizes should == original sizes\n",
    "        self.cube_embedding = CubeEmbedding(\n",
    "            (frames, image_height, image_width),\n",
    "            (frames, patch_height, patch_width),\n",
    "            input_channels,\n",
    "            dim[0]\n",
    "        )\n",
    "        \n",
    "        # =================================================================================== #\n",
    "\n",
    "        self.up_block1 = UpBlock(1 * last_dim, last_dim // 2, dim[0])\n",
    "        self.up_block2 = UpBlock(2 * (last_dim // 2), last_dim // 4, dim[0])\n",
    "        self.up_block3 = UpBlock(2 * (last_dim // 4), last_dim // 8, dim[0])\n",
    "        self.up_block4 = nn.ConvTranspose2d(2 * (last_dim // 8), output_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        if self.use_spectral_norm:\n",
    "            logger.info(\"Adding spectral norm to all conv and linear layers\")\n",
    "            apply_spectral_norm(self)\n",
    "            \n",
    "        if self.use_post_block:\n",
    "            self.postblock = PostBlock(post_conf)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_copy = None\n",
    "        if self.use_post_block:  # copy tensor to feed into postBlock later\n",
    "            x_copy = x.clone().detach()\n",
    "            \n",
    "        if self.use_padding:\n",
    "            x = self.padding_opt.pad(x)\n",
    "\n",
    "        if self.patch_width > 1 and self.patch_height > 1:\n",
    "            x = self.cube_embedding(x)\n",
    "        elif self.frames > 1:\n",
    "            x = F.avg_pool3d(x, kernel_size=(2, 1, 1)).squeeze(2)\n",
    "        else:  # case where only using one time-step as input\n",
    "            x = x.squeeze(2)\n",
    "\n",
    "        encodings = []\n",
    "        for cel, transformer in self.layers:\n",
    "            x = cel(x)\n",
    "            x = transformer(x)\n",
    "            encodings.append(x)\n",
    "\n",
    "        x = self.up_block1(x)\n",
    "        x = torch.cat([x, encodings[2]], dim=1)\n",
    "        x = self.up_block2(x)\n",
    "        x = torch.cat([x, encodings[1]], dim=1)\n",
    "        x = self.up_block3(x)\n",
    "        x = torch.cat([x, encodings[0]], dim=1)\n",
    "        x = self.up_block4(x)\n",
    "\n",
    "        if self.use_padding:\n",
    "            x = self.padding_opt.unpad(x)\n",
    "\n",
    "        if self.use_interp:\n",
    "            x = F.interpolate(x, size=(self.image_height, self.image_width), mode=\"bilinear\")\n",
    "            \n",
    "        x = x.unsqueeze(2)\n",
    "        \n",
    "        if self.use_post_block:\n",
    "            x = {\n",
    "                \"y_pred\": x,\n",
    "                \"x\": x_copy,\n",
    "            }\n",
    "            x = self.postblock(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def rk4(self, x):\n",
    "\n",
    "        def integrate_step(x, k, factor):\n",
    "            return self.forward(x + k * factor)\n",
    "\n",
    "        k1 = self.forward(x)  # State at i\n",
    "        k1 = torch.cat([x[:, :, -2:-1], k1], dim=2)\n",
    "        k2 = integrate_step(x, k1, 0.5)  # State at i + 0.5\n",
    "        k2 = torch.cat([x[:, :, -2:-1], k2], dim=2)\n",
    "        k3 = integrate_step(x, k2, 0.5)  # State at i + 0.5\n",
    "        k3 = torch.cat([x[:, :, -2:-1], k3], dim=2)\n",
    "        k4 = integrate_step(x, k3, 1.0)  # State at i + 1\n",
    "\n",
    "        return (k1 + 2 * k2 + 2 * k3 + k4) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0676d30c-25f1-4814-8278-48faf9e7d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/glade/derecho/scratch/akn7/miles-credit/results/crossformer_6h_2/res_181_360/model.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5915ca83-5aa6-48ce-a2e2-0140d92e4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = CREDIT_main_parser(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e9ce14c-cfcf-4d17-821c-c0a14902bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['model']['padding_conf']['pad_lat'] = [29, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1f202ff-7c86-4e17-be76-1dbe5f7b3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['model']['padding_conf']['pad_lon'] = [36, 36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26b12655-10b3-4439-af28-947ea85fdafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74, 1, 181, 360])\n",
      "Number of parameters in the model: 31657635\n",
      "Predicted shape: torch.Size([1, 71, 1, 181, 360])\n"
     ]
    }
   ],
   "source": [
    "image_height = conf['model']['image_height']\n",
    "image_width = conf['model']['image_width']\n",
    "levels = conf['model']['levels']\n",
    "frames = conf['model']['frames']\n",
    "channels = conf['model']['channels']\n",
    "surface_channels = conf['model']['surface_channels']\n",
    "input_only_channels = conf['model']['input_only_channels']\n",
    "frame_patch_size = conf['model']['frame_patch_size']\n",
    "dim = conf['model']['dim']\n",
    "depth = conf['model']['depth']\n",
    "global_window_size = conf['model']['global_window_size']\n",
    "cross_embed_kernel_sizes = conf['model']['cross_embed_kernel_sizes']\n",
    "cross_embed_strides = conf['model']['cross_embed_strides']\n",
    "local_window_size = conf['model']['local_window_size']\n",
    "\n",
    "# Define the model\n",
    "model = CrossFormer(**conf['model']).to(\"cuda\")\n",
    "\n",
    "input_tensor = torch.randn(1, \n",
    "                           channels * levels + surface_channels + input_only_channels, \n",
    "                           frames, \n",
    "                           image_height, \n",
    "                           image_width).to(\"cuda\")\n",
    "\n",
    "\n",
    "print(input_tensor.shape)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params}\")\n",
    "\n",
    "y_pred = model(input_tensor.to(\"cuda\"))\n",
    "print(\"Predicted shape:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c9183-f530-48bb-8055-46ff487fec38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
