{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb99dd3-1f7c-403f-ac16-f312ba51ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- #\n",
    "# System\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import warnings\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ---------- #\n",
    "# Numerics\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------- #\n",
    "# AI libs\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "# import wandb\n",
    "\n",
    "# ---------- #\n",
    "# credit\n",
    "from credit.data import ERA5Dataset, concat_and_reshape\n",
    "from credit.models import load_model\n",
    "from credit.transforms import ToTensor, NormalizeState, NormalizeState_Quantile\n",
    "from credit.seed import seed_everything\n",
    "from credit.pbs import launch_script, launch_script_mpi\n",
    "from credit.pol_lapdiff_filt import Diffusion_and_Pole_Filter\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.distributed import distributed_model_wrapper\n",
    "from credit.models.checkpoint import load_model_state\n",
    "from credit.solar import TOADataLoader\n",
    "from credit.output import split_and_reshape, load_metadata, make_xarray, save_netcdf_increment\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5d0f4f-f072-4eb1-9e11-a8cd7fbc201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = '/glade/u/home/ksha/miles-credit/results/fuxi_norm/model.yml'\n",
    "# Read YAML file\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a4daaa-6382-40f1-968c-069a6869c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_dataset('/glade/derecho/scratch/ksha/CREDIT/wx_former_6h/2020-01-01T00Z/pred_2020-01-01T00Z_006.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0f8f0-eecb-4f02-8f05-667ebe55c694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d77431-131b-4852-924c-948a4cb7767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae84dd0-59d2-4c0c-9473-252365756ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictForecast(torch.utils.data.IterableDataset):\n",
    "    def __init__(self,\n",
    "                 filenames,\n",
    "                 forecasts,\n",
    "                 history_len,\n",
    "                 skip_periods,\n",
    "                 rank,\n",
    "                 world_size,\n",
    "                 shuffle=False,\n",
    "                 transform=None,\n",
    "                 rollout_p=0.0,\n",
    "                 which_forecast=None):\n",
    "\n",
    "        self.dataset = ERA5Dataset(\n",
    "            filenames=filenames,\n",
    "            history_len=history_len,\n",
    "            forecast_len=1,\n",
    "            skip_periods=skip_periods,\n",
    "            transform=transform\n",
    "        )\n",
    "        self.meta_data_dict = self.dataset.meta_data_dict\n",
    "        self.all_files = self.dataset.all_fils\n",
    "        self.history_len = history_len\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.skip_periods = skip_periods\n",
    "        self.current_epoch = 0\n",
    "        self.rollout_p = rollout_p\n",
    "        self.forecasts = forecasts\n",
    "        self.skip_periods = skip_periods if skip_periods is not None else 1\n",
    "        self.which_forecast = which_forecast\n",
    "\n",
    "    def find_start_stop_indices(self, index):\n",
    "        start_time = self.forecasts[index][0]\n",
    "        date_object = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        shifted_hours = self.skip_periods * self.history_len\n",
    "        date_object = date_object - datetime.timedelta(hours=shifted_hours)\n",
    "        self.forecasts[index][0] = date_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        datetime_objs = [np.datetime64(date) for date in self.forecasts[index]]\n",
    "        start_time, stop_time = [str(datetime_obj) + '.000000000' for datetime_obj in datetime_objs]\n",
    "        self.start_time = np.datetime64(start_time).astype(datetime.datetime)\n",
    "        self.stop_time = np.datetime64(stop_time).astype(datetime.datetime)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        for idx, dataset in enumerate(self.all_files):\n",
    "            start_time = np.datetime64(dataset['time'].min().values).astype(datetime.datetime)\n",
    "            stop_time = np.datetime64(dataset['time'].max().values).astype(datetime.datetime)\n",
    "            track_start = False\n",
    "            track_stop = False\n",
    "\n",
    "            if start_time <= self.start_time <= stop_time:\n",
    "                # Start time is in this file, use start time index\n",
    "                dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                start_idx = np.searchsorted(dataset, self.start_time)\n",
    "                start_idx = max(0, min(start_idx, len(dataset)-1))\n",
    "                track_start = True\n",
    "\n",
    "            elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                start_idx = 0\n",
    "                track_start = True\n",
    "\n",
    "            if start_time <= self.stop_time <= stop_time:\n",
    "                # Stop time is in this file, use stop time index\n",
    "                if isinstance(dataset, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset = np.array([np.datetime64(x.values).astype(datetime.datetime) for x in dataset['time']])\n",
    "                stop_idx = np.searchsorted(dataset, self.stop_time)\n",
    "                stop_idx = max(0, min(stop_idx, len(dataset)-1))\n",
    "                track_stop = True\n",
    "\n",
    "            elif start_time < self.stop_time and stop_time > self.start_time:\n",
    "                # File overlaps time range, use full file\n",
    "                stop_idx = len(dataset) - 1\n",
    "                track_stop = True\n",
    "\n",
    "            # Only include files that overlap the time range\n",
    "            if track_start and track_stop:\n",
    "                info[idx] = ((idx, start_idx), (idx, stop_idx))\n",
    "\n",
    "        indices = []\n",
    "        for dataset_idx, (start, stop) in info.items():\n",
    "            for i in range(start[1], stop[1]+1):\n",
    "                indices.append((start[0], i))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.forecasts)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers*self.world_size, rank=self.rank*num_workers+worker_id, shuffle=self.shuffle)\n",
    "\n",
    "        for index in sampler:\n",
    "\n",
    "            data_lookup = self.find_start_stop_indices(index)\n",
    "\n",
    "            for k, (file_key, time_key) in enumerate(data_lookup):\n",
    "\n",
    "                if k == 0:\n",
    "                    concatenated_samples = {'x': [], 'x_surf': []}\n",
    "                    sliced_x = xr.open_zarr(self.filenames[file_key], consolidated=True).isel(time=slice(time_key, time_key+self.history_len+1))\n",
    "\n",
    "                    # Check if additional data from the next file is needed\n",
    "                    if len(sliced_x['time']) < self.history_len + 1:\n",
    "                        # Load excess data from the next file\n",
    "                        next_file_idx = self.filenames.index(self.filenames[file_key]) + 1\n",
    "                        if next_file_idx == len(self.filenames):\n",
    "                            raise OSError(\"You have reached the end of the available data. Exiting.\")\n",
    "                        sliced_x_next = xr.open_zarr(\n",
    "                            self.filenames[next_file_idx],\n",
    "                            consolidated=True).isel(time=slice(0, self.history_len+1-len(sliced_x['time'])))\n",
    "\n",
    "                        # Concatenate excess data from the next file with the current data\n",
    "                        sliced_x = xr.concat([sliced_x, sliced_x_next], dim='time')\n",
    "\n",
    "                    sample_x = {\n",
    "                        'x': sliced_x.isel(time=slice(0, self.history_len))\n",
    "                    }\n",
    "\n",
    "                    if self.transform:\n",
    "                        sample_x = self.transform(sample_x)\n",
    "                        # Add static vars, if any, to the return dictionary\n",
    "                        if \"static\" in sample_x:\n",
    "                            concatenated_samples[\"static\"] = []\n",
    "                        if \"TOA\" in sample_x:\n",
    "                            concatenated_samples[\"TOA\"] = []\n",
    "\n",
    "                    for key in concatenated_samples.keys():\n",
    "                        concatenated_samples[key] = sample_x[key].squeeze(0) if self.history_len == 1 else sample_x[key]\n",
    "\n",
    "                    concatenated_samples['forecast_hour'] = k + 1\n",
    "                    concatenated_samples['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))  # Adjust stopping condition\n",
    "                    concatenated_samples['datetime'] = sliced_x.time.values.astype('datetime64[s]').astype(int)[-1]\n",
    "\n",
    "                else:\n",
    "                    concatenated_samples['forecast_hour'] = k + 1\n",
    "                    concatenated_samples['stop_forecast'] = (k == (len(data_lookup)-self.history_len-1))  # Adjust stopping condition\n",
    "\n",
    "                yield concatenated_samples\n",
    "\n",
    "                if concatenated_samples['stop_forecast']:\n",
    "                    break\n",
    "\n",
    "\n",
    "def setup(rank, world_size, mode):\n",
    "    logging.info(f\"Running {mode.upper()} on rank {rank} with world_size {world_size}.\")\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acc937-dbe9-4786-903e-099e19181263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc94336-f5b0-4e20-8686-3aaa61873483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, toa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Predict and convert to real space for laplace filter and metrics\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    133\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m    134\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m state_transformer\u001b[38;5;241m.\u001b[39minverse_transform(y_pred\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "    setup(rank, world_size, conf[\"trainer\"][\"mode\"])\n",
    "\n",
    "# infer device id from rank\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\")\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Config settings\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "time_step = conf[\"data\"][\"time_step\"] if \"time_step\" in conf[\"data\"] else None\n",
    "\n",
    "# Load paths to all ERA5 data available\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# Preprocessing transformations\n",
    "if conf[\"data\"][\"scaler_type\"] == \"std\":\n",
    "    state_transformer = NormalizeState(conf)\n",
    "else:\n",
    "    state_transformer = NormalizeState_Quantile(conf)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        state_transformer,\n",
    "        ToTensor(conf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = PredictForecast(\n",
    "    filenames=all_ERA_files,\n",
    "    forecasts=load_forecasts(conf),\n",
    "    history_len=history_len,\n",
    "    skip_periods=time_step,\n",
    "    transform=transform,\n",
    "    rank=rank,\n",
    "    world_size=world_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# setup the dataloder for this process\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = load_model(conf, load_weights=True).to(device)\n",
    "\n",
    "# Warning -- see next line\n",
    "distributed = conf[\"trainer\"][\"mode\"] in [\"ddp\", \"fsdp\"]\n",
    "if distributed:  # A new field needs to be added to predict\n",
    "    model = distributed_model_wrapper(conf, model, device)\n",
    "    if conf[\"trainer\"][\"mode\"] == \"fsdp\":\n",
    "        # Load model weights (if any), an optimizer, scheduler, and gradient scaler\n",
    "        model = load_model_state(conf, model, device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# get lat/lons from x-array\n",
    "latlons = xr.open_dataset(conf[\"loss\"][\"latitude_weights\"])\n",
    "\n",
    "meta_data = load_metadata(conf)\n",
    "\n",
    "# Set up the diffusion and pole filters\n",
    "if (\n",
    "    \"use_laplace_filter\" in conf[\"predict\"]\n",
    "    and conf[\"predict\"][\"use_laplace_filter\"]\n",
    "):\n",
    "    dpf = Diffusion_and_Pole_Filter(\n",
    "        nlat=conf[\"model\"][\"image_height\"],\n",
    "        nlon=conf[\"model\"][\"image_width\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "# Rollout\n",
    "with torch.no_grad():\n",
    "    # forecast count = a constant for each run\n",
    "    forecast_count = 0\n",
    "\n",
    "    # y_pred allocation\n",
    "    y_pred = None\n",
    "    static = None\n",
    "    results = []\n",
    "\n",
    "    # model inference loop\n",
    "    for k, batch in enumerate(data_loader):\n",
    "\n",
    "        # get the datetime and forecasted hours\n",
    "        date_time = batch[\"datetime\"].item()\n",
    "        forecast_hour = batch[\"forecast_hour\"].item()\n",
    "\n",
    "        # initialization on the first forecast hour\n",
    "        if forecast_hour == 1:\n",
    "            # Initialize x and x_surf with the first time step\n",
    "            #x = model.concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(device)\n",
    "            x = concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(device)\n",
    "            \n",
    "            init_datetime_str = datetime.datetime.utcfromtimestamp(date_time)\n",
    "            init_datetime_str = init_datetime_str.strftime('%Y-%m-%dT%HZ')\n",
    "\n",
    "        # Add statics\n",
    "        if \"static\" in batch:\n",
    "            if static is None:\n",
    "                static = batch[\"static\"].to(device).unsqueeze(2).expand(-1, -1, x.shape[2], -1, -1).float()\n",
    "            x = torch.cat((x, static.clone()), dim=1)\n",
    "\n",
    "        # Add solar \"statics\"\n",
    "        if \"static_variables\" in conf[\"data\"] and \"tsi\" in conf[\"data\"][\"static_variables\"]:\n",
    "            if k == 0:\n",
    "                toaDL = TOADataLoader(conf)\n",
    "            elapsed_time = pd.Timedelta(hours=k)\n",
    "            tnow = pd.to_datetime(datetime.datetime.utcfromtimestamp(batch[\"datetime\"]))\n",
    "            tnow = tnow + elapsed_time\n",
    "            if history_len == 1:\n",
    "                current_times = [pd.to_datetime(datetime.datetime.utcfromtimestamp(_t)) + elapsed_time for _t in tnow]\n",
    "            else:\n",
    "                current_times = [tnow if hl == 0 else tnow - pd.Timedelta(hours=hl) for hl in range(history_len)]\n",
    "\n",
    "            toa = torch.cat([toaDL(_t) for _t in current_times], dim=0).to(device)\n",
    "            toa = toa.squeeze().unsqueeze(0)\n",
    "            x = torch.cat([x, toa.unsqueeze(1).to(device).float()], dim=1)\n",
    "\n",
    "        # Predict and convert to real space for laplace filter and metrics\n",
    "        raise\n",
    "        y_pred = model(x)\n",
    "        y_pred = state_transformer.inverse_transform(y_pred.cpu())\n",
    "\n",
    "        if (\"use_laplace_filter\" in conf[\"predict\"] and conf[\"predict\"][\"use_laplace_filter\"]):\n",
    "            y_pred = (\n",
    "                dpf.diff_lap2d_filt(y_pred.to(device).squeeze())\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(2)\n",
    "                .cpu()\n",
    "            )\n",
    "\n",
    "        # Save the current forecast hour data in parallel\n",
    "        utc_datetime = datetime.datetime.utcfromtimestamp(date_time) + datetime.timedelta(hours=forecast_hour)\n",
    "\n",
    "        # convert the current step result as x-array\n",
    "        darray_upper_air, darray_single_level = make_xarray(\n",
    "            y_pred,\n",
    "            utc_datetime,\n",
    "            latlons.latitude.values,\n",
    "            latlons.longitude.values,\n",
    "            conf,\n",
    "        )\n",
    "\n",
    "        # Save the current forecast hour data in parallel\n",
    "        result = p.apply_async(\n",
    "            save_netcdf_increment,\n",
    "            (darray_upper_air, darray_single_level, init_datetime_str, forecast_hour, meta_data, conf)\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "        # Update the input\n",
    "        # setup for next iteration, transform to z-space and send to device\n",
    "        y_pred = state_transformer.transform_array(y_pred).to(device)\n",
    "\n",
    "        if history_len == 1:\n",
    "            x = y_pred.detach()\n",
    "        else:\n",
    "            # use multiple past forecast steps as inputs\n",
    "            static_dim_size = abs(x.shape[1] - y_pred.shape[1])  # static channels will get updated on next pass\n",
    "            x_detach = x[:, :-static_dim_size, 1:].detach() if static_dim_size else x[:, :, 1:].detach()  # if static_dim_size=0 then :0 gives empty range\n",
    "            x = torch.cat([x_detach, y_pred.detach()], dim=2)\n",
    "\n",
    "        # Explicitly release GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if batch[\"stop_forecast\"][0]:\n",
    "            # Wait for all processes to finish in order\n",
    "            for result in results:\n",
    "                result.get()\n",
    "\n",
    "            # Now merge all the files into one and delete leftovers\n",
    "            # merge_netcdf_files(init_datetime_str, conf)\n",
    "\n",
    "            # forecast count = a constant for each run\n",
    "            forecast_count += 1\n",
    "\n",
    "            # update lists\n",
    "            results = []\n",
    "\n",
    "            # y_pred allocation\n",
    "            y_pred = None\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            if distributed:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "if distributed:\n",
    "    torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbb86a0-de80-47a8-a1f9-0dca68cd4677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5151, -0.5148, -0.5146,  ..., -0.5156, -0.5154, -0.5152],\n",
       "         [-0.4601, -0.4599, -0.4595,  ..., -0.4609, -0.4606, -0.4602],\n",
       "         [-0.4083, -0.4081, -0.4079,  ..., -0.4088, -0.4086, -0.4085],\n",
       "         ...,\n",
       "         [-0.6567, -0.6568, -0.6568,  ..., -0.6565, -0.6566, -0.6567],\n",
       "         [-0.6584, -0.6584, -0.6584,  ..., -0.6583, -0.6583, -0.6583],\n",
       "         [-0.6583, -0.6583, -0.6584,  ..., -0.6583, -0.6583, -0.6583]],\n",
       "\n",
       "        [[-0.5215, -0.5213, -0.5212,  ..., -0.5218, -0.5217, -0.5216],\n",
       "         [-0.4651, -0.4650, -0.4648,  ..., -0.4655, -0.4653, -0.4651],\n",
       "         [-0.4081, -0.4081, -0.4080,  ..., -0.4081, -0.4081, -0.4081],\n",
       "         ...,\n",
       "         [-0.6479, -0.6481, -0.6482,  ..., -0.6473, -0.6475, -0.6477],\n",
       "         [-0.6515, -0.6516, -0.6517,  ..., -0.6513, -0.6513, -0.6514],\n",
       "         [-0.6536, -0.6536, -0.6536,  ..., -0.6535, -0.6535, -0.6536]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, -4, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc827cb-99be-4b8b-9c1d-6f0ba3f320ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b0ff5-a8cb-4c77-bed8-ed89f5a2644f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d6a74-aafb-4982-ad30-8f45323510d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46c88d37-b4e3-4912-91e0-8ae3551822fc",
   "metadata": {},
   "source": [
    "### compare old & new rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a07b27-8815-484e-a405-be2bfca0c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'SP' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 't2m' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'V500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'U500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'T500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'Z500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'Q500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    }
   ],
   "source": [
    "new = xr.open_dataset('/glade/derecho/scratch/ksha/CREDIT/fuxi_norm_new/2018-06-01T00Z/pred_2018-06-01T00Z_001.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ce744e-164e-49ef-a0cd-7fdfd69c6b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'SP' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 't2m' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'V500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'U500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'T500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'Z500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/glade/work/ksha/miniconda3/envs/credit/lib/python3.10/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'Q500' has multiple fill values {9.999e+20, 9.999e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    }
   ],
   "source": [
    "old = xr.open_dataset('/glade/derecho/scratch/ksha/CREDIT/fuxi_norm_test/2018-06-01T00Z/pred_2018-06-01T00Z_001.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62bf7539-610b-4021-89a5-c02a32ccc89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.34469616, -0.3446092 , -0.3437317 , ..., -0.34757087,\n",
       "         -0.34641483, -0.34614974],\n",
       "        [-0.33615398, -0.3353314 , -0.33522156, ..., -0.3368566 ,\n",
       "         -0.3371188 , -0.33786282],\n",
       "        [-0.3358565 , -0.33543956, -0.3358222 , ..., -0.33780622,\n",
       "         -0.337941  , -0.33799165],\n",
       "        ...,\n",
       "        [-2.2178302 , -2.2207105 , -2.22325   , ..., -2.2133207 ,\n",
       "         -2.2163541 , -2.2175353 ],\n",
       "        [-2.234412  , -2.2354207 , -2.2362342 , ..., -2.2284808 ,\n",
       "         -2.2312403 , -2.2342846 ],\n",
       "        [-2.2077699 , -2.2087858 , -2.2077293 , ..., -2.202627  ,\n",
       "         -2.2074575 , -2.2098649 ]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new['t2m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0614d1bf-61bb-41d6-b3dc-10005a29a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.344811  , -0.3447332 , -0.34384775, ..., -0.34767446,\n",
       "         -0.34650114, -0.3462309 ],\n",
       "        [-0.336249  , -0.3354228 , -0.3353209 , ..., -0.3369275 ,\n",
       "         -0.33717674, -0.33792216],\n",
       "        [-0.335923  , -0.3355071 , -0.33589032, ..., -0.33785227,\n",
       "         -0.33797932, -0.33802745],\n",
       "        ...,\n",
       "        [-2.2178102 , -2.2206895 , -2.2232304 , ..., -2.2132993 ,\n",
       "         -2.2163353 , -2.2175174 ],\n",
       "        [-2.23439   , -2.2353997 , -2.2362142 , ..., -2.2284608 ,\n",
       "         -2.2312205 , -2.234268  ],\n",
       "        [-2.207747  , -2.2087667 , -2.2077122 , ..., -2.202608  ,\n",
       "         -2.2074409 , -2.209847  ]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(old['t2m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23c858-d46e-4b1b-8e8a-8c98e7e9a8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d88647-a4fc-4163-b84a-6390e625bb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434481b-1fc2-40a1-9d1d-46982ef517d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31784f97-c273-42e3-83be-b11b17cfcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.data import Sample\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aabd5a1-c35e-4dd3-99be-1adebe28c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c478a73-2691-4ae8-8c15-338cc97f6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ae2aaab-9706-42ba-91ef-dcbee184c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_transformer = Normalize_ERA5_and_Forcing(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99032b2-b973-415d-a99a-4f4926f8032a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce473241-1534-4a0c-859a-eceb0ac1a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = state_transformer.inverse_transform(y_pred.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "820cde84-c189-4532-b5d6-837670f4faf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 1, 640, 1280])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d87d75f-4e44-45b2-a6d3-264595cf9d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 9.6050e+00,  9.6030e+00,  9.5811e+00,  ...,  9.6841e+00,\n",
       "             9.6410e+00,  9.6423e+00],\n",
       "           [ 9.6263e+00,  9.5931e+00,  9.5919e+00,  ...,  9.6613e+00,\n",
       "             9.6555e+00,  9.6297e+00],\n",
       "           [ 9.7909e+00,  9.7778e+00,  9.7666e+00,  ...,  9.8450e+00,\n",
       "             9.8432e+00,  9.8334e+00],\n",
       "           ...,\n",
       "           [ 1.7022e-01,  3.5282e-01,  5.4143e-01,  ...,  5.4510e-02,\n",
       "             1.0519e-01,  1.4594e-01],\n",
       "           [ 2.9755e-01,  4.7897e-01,  6.9006e-01,  ...,  3.1048e-01,\n",
       "             2.8333e-01,  2.2607e-01],\n",
       "           [ 1.0609e-01,  3.1670e-01,  5.3178e-01,  ...,  1.3572e-01,\n",
       "             1.2455e-01,  5.1676e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 4.2611e+00,  4.2457e+00,  4.2450e+00,  ...,  4.2849e+00,\n",
       "             4.2833e+00,  4.2390e+00],\n",
       "           [ 4.4083e+00,  4.3953e+00,  4.3780e+00,  ...,  4.4362e+00,\n",
       "             4.4175e+00,  4.3946e+00],\n",
       "           [ 4.4866e+00,  4.4929e+00,  4.4759e+00,  ...,  4.5344e+00,\n",
       "             4.5213e+00,  4.5233e+00],\n",
       "           ...,\n",
       "           [-3.4631e+00, -3.4540e+00, -3.5487e+00,  ..., -3.4942e+00,\n",
       "            -3.5571e+00, -3.5442e+00],\n",
       "           [-3.6978e+00, -3.6711e+00, -3.7302e+00,  ..., -3.7283e+00,\n",
       "            -3.7462e+00, -3.7137e+00],\n",
       "           [-3.8344e+00, -3.8189e+00, -3.8602e+00,  ..., -3.8386e+00,\n",
       "            -3.8179e+00, -3.8797e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 4.1664e+00,  4.1688e+00,  4.1532e+00,  ...,  4.2076e+00,\n",
       "             4.1837e+00,  4.1803e+00],\n",
       "           [ 4.2132e+00,  4.2161e+00,  4.2145e+00,  ...,  4.2493e+00,\n",
       "             4.2443e+00,  4.2345e+00],\n",
       "           [ 4.2969e+00,  4.2910e+00,  4.2945e+00,  ...,  4.3309e+00,\n",
       "             4.3330e+00,  4.3323e+00],\n",
       "           ...,\n",
       "           [ 5.8587e+00,  5.8676e+00,  5.9106e+00,  ...,  5.9405e+00,\n",
       "             5.9140e+00,  5.9206e+00],\n",
       "           [ 5.6778e+00,  5.6813e+00,  5.7019e+00,  ...,  5.7697e+00,\n",
       "             5.7556e+00,  5.7781e+00],\n",
       "           [ 5.6048e+00,  5.6631e+00,  5.6585e+00,  ...,  5.6949e+00,\n",
       "             5.7019e+00,  5.7064e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 2.4940e+02,  2.4941e+02,  2.4941e+02,  ...,  2.4939e+02,\n",
       "             2.4940e+02,  2.4942e+02],\n",
       "           [ 2.4930e+02,  2.4931e+02,  2.4930e+02,  ...,  2.4929e+02,\n",
       "             2.4929e+02,  2.4931e+02],\n",
       "           [ 2.4930e+02,  2.4930e+02,  2.4930e+02,  ...,  2.4929e+02,\n",
       "             2.4929e+02,  2.4930e+02],\n",
       "           ...,\n",
       "           [ 2.3692e+02,  2.3690e+02,  2.3698e+02,  ...,  2.3671e+02,\n",
       "             2.3673e+02,  2.3676e+02],\n",
       "           [ 2.3687e+02,  2.3687e+02,  2.3698e+02,  ...,  2.3664e+02,\n",
       "             2.3669e+02,  2.3670e+02],\n",
       "           [ 2.3687e+02,  2.3690e+02,  2.3690e+02,  ...,  2.3664e+02,\n",
       "             2.3665e+02,  2.3667e+02]]],\n",
       "\n",
       "\n",
       "         [[[ 5.3307e+04,  5.3307e+04,  5.3306e+04,  ...,  5.3307e+04,\n",
       "             5.3307e+04,  5.3308e+04],\n",
       "           [ 5.3297e+04,  5.3297e+04,  5.3296e+04,  ...,  5.3297e+04,\n",
       "             5.3296e+04,  5.3295e+04],\n",
       "           [ 5.3283e+04,  5.3284e+04,  5.3281e+04,  ...,  5.3283e+04,\n",
       "             5.3281e+04,  5.3280e+04],\n",
       "           ...,\n",
       "           [ 5.1009e+04,  5.1024e+04,  5.1022e+04,  ...,  5.0953e+04,\n",
       "             5.0960e+04,  5.0965e+04],\n",
       "           [ 5.0996e+04,  5.1014e+04,  5.1017e+04,  ...,  5.0942e+04,\n",
       "             5.0939e+04,  5.0952e+04],\n",
       "           [ 5.0992e+04,  5.0993e+04,  5.1004e+04,  ...,  5.0922e+04,\n",
       "             5.0922e+04,  5.0931e+04]]],\n",
       "\n",
       "\n",
       "         [[[ 5.4526e-04,  5.4434e-04,  5.4487e-04,  ...,  5.4880e-04,\n",
       "             5.4711e-04,  5.4423e-04],\n",
       "           [ 5.4635e-04,  5.4549e-04,  5.4688e-04,  ...,  5.4866e-04,\n",
       "             5.4790e-04,  5.4682e-04],\n",
       "           [ 5.5487e-04,  5.5425e-04,  5.5409e-04,  ...,  5.5624e-04,\n",
       "             5.5475e-04,  5.5519e-04],\n",
       "           ...,\n",
       "           [ 3.0184e-04,  3.1398e-04,  3.1892e-04,  ...,  3.2617e-04,\n",
       "             3.2722e-04,  3.2497e-04],\n",
       "           [ 3.1588e-04,  3.1566e-04,  3.0542e-04,  ...,  3.2111e-04,\n",
       "             3.2845e-04,  3.2563e-04],\n",
       "           [ 3.2588e-04,  3.2292e-04,  3.1165e-04,  ...,  3.1763e-04,\n",
       "             3.3331e-04,  3.4118e-04]]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8136649-cb33-4976-bf3e-e6a99951a693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123321f-ff81-461d-8634-9323db2db2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
