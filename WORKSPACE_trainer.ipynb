{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38751e61-33e1-48d3-9875-dce03b812741",
   "metadata": {},
   "source": [
    "# Trainer workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac84807b-687b-4cc8-b358-6418148dcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.fft\n",
    "import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import IterableDataset\n",
    "import optuna\n",
    "from credit.models.checkpoint import TorchFSDPCheckpointIO\n",
    "from credit.solar import TOADataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bc5d88-0603-4737-b513-985ebb3f487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dc6c8c-2b10-433c-b19a-2bff2794b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def accum_log(log, new_logs):\n",
    "    for key, new_value in new_logs.items():\n",
    "        old_value = log.get(key, 0.)\n",
    "        log[key] = old_value + new_value\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adec30e-a325-46df-89cc-d1adebf9d432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cb431-cf8e-4ab3-9ddb-424905714934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, rank, module=False):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.rank = rank\n",
    "        self.device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        if module:\n",
    "            self.model = self.model.module\n",
    "\n",
    "    # Training function.\n",
    "    def train_one_epoch(\n",
    "        self,\n",
    "        epoch,\n",
    "        conf,\n",
    "        trainloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "        metrics\n",
    "    ):\n",
    "        # training hyperparameters\n",
    "        batches_per_epoch = conf['trainer']['batches_per_epoch']\n",
    "        grad_accum_every = conf['trainer']['grad_accum_every']\n",
    "        history_len = conf[\"data\"][\"history_len\"]\n",
    "        forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "        amp = conf['trainer']['amp']\n",
    "        distributed = True if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"] else False\n",
    "\n",
    "        if 'stop_rollout' not in conf['trainer']:\n",
    "            rollout_p = 1.0 \n",
    "        else:\n",
    "            rollout_p = conf['trainer']['stop_rollout']\n",
    "\n",
    "        if \"total_time_steps\" in conf[\"data\"]:\n",
    "            total_time_steps = conf[\"data\"][\"total_time_steps\"]\n",
    "        else:\n",
    "            total_time_steps = forecast_len\n",
    "\n",
    "        if 'diagnostic_variables' in conf[\"data\"]:\n",
    "            varnum_diag = len(conf[\"data\"]['diagnostic_variables'])\n",
    "            \n",
    "        # update the learning rate if epoch-by-epoch updates that dont depend on a metric\n",
    "        if conf['trainer']['use_scheduler'] and conf['trainer']['scheduler']['scheduler_type'] == \"lambda\":\n",
    "            scheduler.step()\n",
    "\n",
    "        # set up a custom tqdm\n",
    "        if isinstance(trainloader.dataset, IterableDataset):\n",
    "            # we sample forecast termination with probability p during training\n",
    "            trainloader.dataset.set_rollout_prob(rollout_p)\n",
    "        else:\n",
    "            batches_per_epoch = (\n",
    "                batches_per_epoch if 0 < batches_per_epoch < len(trainloader) else len(trainloader)\n",
    "            )\n",
    "\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(trainloader),\n",
    "            total=batches_per_epoch,\n",
    "            leave=True,\n",
    "            disable=True if self.rank > 0 else False\n",
    "        )\n",
    "        \n",
    "        results_dict = defaultdict(list)\n",
    "\n",
    "        for i, batch in batch_group_generator:\n",
    "            # training log\n",
    "            logs = {}\n",
    "            # loss\n",
    "            commit_loss = 0.0\n",
    "\n",
    "            with autocast(enabled=amp):\n",
    "\n",
    "                # --------------------------------------------------------------------------------- #\n",
    "\n",
    "                if \"x_surf\" in batch:\n",
    "                    # combine x and x_surf\n",
    "                    # input: (batch_num, time, var, level, lat, lon), (batch_num, time, var, lat, lon) \n",
    "                    # output: (batch_num, var, time, lat, lon), 'x' first and then 'x_surf'\n",
    "                    x = self.model.concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(self.device).float()\n",
    "                else:\n",
    "                    # no x_surf\n",
    "                    x = self.model.reshape_only(batch[\"x\"]).to(self.device).float()\n",
    "\n",
    "                # --------------------------------------------------------------------------------- #\n",
    "                # add forcing and static variables\n",
    "                if 'x_forcing_static' in batch:\n",
    "                    \n",
    "                    # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "                    x_forcing_batch = batch['x_forcing_static'].to(self.device).permute(0, 2, 1, 3, 4).float()\n",
    "\n",
    "                    # concat on var dimension\n",
    "                    x = torch.cat((x, x_forcing_batch), dim=1)\n",
    "                    \n",
    "                # --------------------------------------------------------------------------------- #\n",
    "                # combine y and y_surf\n",
    "                if \"y_surf\" in batch:\n",
    "                    y = self.model.concat_and_reshape(batch[\"y\"], batch[\"y_surf\"]).to(self.device)\n",
    "                else:\n",
    "                    y = self.model.reshape_only(batch[\"y\"]).to(self.device)\n",
    "                \n",
    "                if 'y_diag' in batch:\n",
    "\n",
    "                    # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "                    y_diag_batch = batch['y_diag'].to(self.device).permute(0, 2, 1, 3, 4).float()\n",
    "                    \n",
    "                    # concat on var dimension\n",
    "                    y = torch.cat((y, y_diag_batch), dim=1)\n",
    "                \n",
    "                k = 0\n",
    "                while True:\n",
    "\n",
    "                    with torch.no_grad() if k != total_time_steps else torch.enable_grad():\n",
    "\n",
    "                        self.model.eval() if k != total_time_steps else self.model.train()\n",
    "\n",
    "                        if getattr(self.model, 'use_codebook', False):\n",
    "                            y_pred, cm_loss = self.model(x)\n",
    "                            commit_loss += cm_loss\n",
    "                        else:\n",
    "                            y_pred = self.model(x)\n",
    "\n",
    "                        if k == total_time_steps:\n",
    "                            break\n",
    "\n",
    "                        k += 1\n",
    "\n",
    "                        if history_len > 1:\n",
    "\n",
    "                            # detach and throw away the oldest time dim\n",
    "                            x_detach = x.detach()[:, :, 1:, ...]\n",
    "                            \n",
    "                            # use y_pred as the next-hour input\n",
    "                            if 'y_diag' in batch:\n",
    "                                # drop diagnostic variables\n",
    "                                y_pred = y_pred.detach()[:, :-varnum_diag, ...]\n",
    "\n",
    "                            # add forcing and static vars to y_pred\n",
    "                            if 'x_forcing_static' in batch:\n",
    "\n",
    "                                # detach and throw away the oldest time dim (same idea as x_detach)\n",
    "                                x_forcing_detach = x_forcing_batch.detach()[:, :, 1:, ...]\n",
    "\n",
    "                                # concat forcing and static to y_pred, y_pred will be the next input\n",
    "                                y_pred = torch.cat((y_pred, x_forcing_detach), dim=1)\n",
    "                            \n",
    "                            x = torch.cat([x_detach, y_pred], dim=2).detach()\n",
    "                            \n",
    "                        else:\n",
    "                            # use y_pred as the next-hour inputs\n",
    "                            x = y_pred.detach()\n",
    "                            \n",
    "                            if 'y_diag' in batch:\n",
    "                                x = x[:, :-varnum_diag, ...]\n",
    "                                \n",
    "                            # add forcing and static vars to y_pred\n",
    "                            if 'x_forcing_static' in batch:\n",
    "                                x = torch.cat((x, x_forcing_batch), dim=1)\n",
    "\n",
    "                y = y.to(device=self.device, dtype=y_pred.dtype)\n",
    "                \n",
    "                loss = criterion(y, y_pred)\n",
    "\n",
    "                # Metrics\n",
    "                metrics_dict = metrics(y_pred.float(), y.float())\n",
    "                \n",
    "                for name, value in metrics_dict.items():\n",
    "                    value = torch.Tensor([value]).cuda(self.device, non_blocking=True)\n",
    "                    if distributed:\n",
    "                        dist.all_reduce(value, dist.ReduceOp.AVG, async_op=False)\n",
    "                    results_dict[f\"train_{name}\"].append(value[0].item())\n",
    "\n",
    "                loss = loss.mean() + commit_loss\n",
    "\n",
    "                scaler.scale(loss / grad_accum_every).backward()\n",
    "\n",
    "            accum_log(logs, {'loss': loss.item() / grad_accum_every})\n",
    "\n",
    "            if distributed:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Handle batch_loss\n",
    "            batch_loss = torch.Tensor([logs[\"loss\"]]).cuda(self.device)\n",
    "            \n",
    "            if distributed:\n",
    "                dist.all_reduce(batch_loss, dist.ReduceOp.AVG, async_op=False)\n",
    "            results_dict[\"train_loss\"].append(batch_loss[0].item())\n",
    "\n",
    "            if 'forecast_hour' in batch:\n",
    "                forecast_hour_tensor = batch['forecast_hour'].to(self.device)\n",
    "                if distributed:\n",
    "                    dist.all_reduce(forecast_hour_tensor, dist.ReduceOp.AVG, async_op=False)\n",
    "                    forecast_hour_avg = forecast_hour_tensor[-1].item()\n",
    "                else:\n",
    "                    forecast_hour_avg = batch['forecast_hour'][-1].item()\n",
    "\n",
    "                results_dict[\"train_forecast_len\"].append(forecast_hour_avg + 1)\n",
    "            else:\n",
    "                results_dict[\"train_forecast_len\"].append(forecast_len + 1)\n",
    "\n",
    "            if not np.isfinite(np.mean(results_dict[\"train_loss\"])):\n",
    "                try:\n",
    "                    raise optuna.TrialPruned()\n",
    "                except Exception as E:\n",
    "                    raise E\n",
    "\n",
    "            # agg the results\n",
    "            to_print = \"Epoch: {} train_loss: {:.6f} train_acc: {:.6f} train_mae: {:.6f} forecast_len {:.6}\".format(\n",
    "                epoch,\n",
    "                np.mean(results_dict[\"train_loss\"]),\n",
    "                np.mean(results_dict[\"train_acc\"]),\n",
    "                np.mean(results_dict[\"train_mae\"]),\n",
    "                np.mean(results_dict[\"train_forecast_len\"])\n",
    "            )\n",
    "            \n",
    "            to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0][\"lr\"])\n",
    "            if self.rank == 0:\n",
    "                batch_group_generator.set_description(to_print)\n",
    "\n",
    "            if conf['trainer']['use_scheduler'] and conf['trainer']['scheduler']['scheduler_type'] == \"cosine-annealing\":\n",
    "                scheduler.step()\n",
    "\n",
    "            if i >= batches_per_epoch and i > 0:\n",
    "                break\n",
    "\n",
    "        #  Shutdown the progbar\n",
    "        batch_group_generator.close()\n",
    "\n",
    "        # clear the cached memory from the gpu\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "    def validate(\n",
    "        self,\n",
    "        epoch,\n",
    "        conf,\n",
    "        valid_loader,\n",
    "        criterion,\n",
    "        metrics\n",
    "    ):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        valid_batches_per_epoch = conf['trainer']['valid_batches_per_epoch']\n",
    "        history_len = conf[\"data\"][\"valid_history_len\"] if \"valid_history_len\" in conf[\"data\"] else conf[\"history_len\"]\n",
    "        forecast_len = conf[\"data\"][\"valid_forecast_len\"] if \"valid_forecast_len\" in conf[\"data\"] else conf[\"forecast_len\"]\n",
    "        distributed = True if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"] else False\n",
    "        total_time_steps = conf[\"data\"][\"total_time_steps\"] if \"total_time_steps\" in conf[\"data\"] else forecast_len\n",
    "\n",
    "        if 'diagnostic_variables' in conf[\"data\"]:\n",
    "            varnum_diag = len(conf[\"data\"]['diagnostic_variables'])\n",
    "        \n",
    "        results_dict = defaultdict(list)\n",
    "\n",
    "        # set up a custom tqdm\n",
    "        if isinstance(valid_loader.dataset, IterableDataset):\n",
    "            valid_batches_per_epoch = valid_batches_per_epoch\n",
    "        else:\n",
    "            valid_batches_per_epoch = (\n",
    "                valid_batches_per_epoch if 0 < valid_batches_per_epoch < len(valid_loader) else len(valid_loader)\n",
    "            )\n",
    "\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(valid_loader),\n",
    "            total=valid_batches_per_epoch,\n",
    "            leave=True,\n",
    "            disable=True if self.rank > 0 else False\n",
    "        )\n",
    "\n",
    "        for i, batch in batch_group_generator:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                commit_loss = 0.0\n",
    "\n",
    "                if \"x_surf\" in batch:\n",
    "                    # combine x and x_surf\n",
    "                    # input: (batch_num, time, var, level, lat, lon), (batch_num, time, var, lat, lon) \n",
    "                    # output: (batch_num, var, time, lat, lon), 'x' first and then 'x_surf'\n",
    "                    x = self.model.concat_and_reshape(batch[\"x\"], batch[\"x_surf\"]).to(self.device).float()\n",
    "                else:\n",
    "                    # no x_surf\n",
    "                    x = self.model.reshape_only(batch[\"x\"]).to(self.device).float()\n",
    "\n",
    "                # --------------------------------------------------------------------------------- #\n",
    "                # add forcing and static variables\n",
    "                if 'x_forcing_static' in batch:\n",
    "                    \n",
    "                    # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "                    x_forcing_batch = batch['x_forcing_static'].to(self.device).permute(0, 2, 1, 3, 4).float()\n",
    "\n",
    "                    # concat on var dimension\n",
    "                    x = torch.cat((x, x_forcing_batch), dim=1)\n",
    "                    \n",
    "                # --------------------------------------------------------------------------------- #\n",
    "                # combine y and y_surf\n",
    "                if \"y_surf\" in batch:\n",
    "                    y = self.model.concat_and_reshape(batch[\"y\"], batch[\"y_surf\"]).to(self.device)\n",
    "                else:\n",
    "                    y = self.model.reshape_only(batch[\"y\"]).to(self.device)\n",
    "                \n",
    "                if 'y_diag' in batch:\n",
    "\n",
    "                    # (batch_num, time, var, lat, lon) --> (batch_num, var, time, lat, lon)\n",
    "                    y_diag_batch = batch['y_diag'].to(self.device).permute(0, 2, 1, 3, 4).float()\n",
    "                    \n",
    "                    # concat on var dimension\n",
    "                    y = torch.cat((y, y_diag_batch), dim=1)\n",
    "\n",
    "                k = 0\n",
    "                while True:\n",
    "                    if getattr(self.model, 'use_codebook', False):\n",
    "                        y_pred, cm_loss = self.model(x)\n",
    "                        commit_loss += cm_loss\n",
    "                    else:\n",
    "                        y_pred = self.model(x)\n",
    "\n",
    "                    if k == total_time_steps:\n",
    "                        break\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "                    if history_len > 1:\n",
    "\n",
    "                        # detach and throw away the oldest time dim\n",
    "                        x_detach = x.detach()[:, :, 1:, ...]\n",
    "                        \n",
    "                        # use y_pred as the next-hour input\n",
    "                        if 'y_diag' in batch:\n",
    "                            # drop diagnostic variables\n",
    "                            y_pred = y_pred.detach()[:, :-varnum_diag, ...]\n",
    "\n",
    "                        # add forcing and static vars to y_pred\n",
    "                        if 'x_forcing_static' in batch:\n",
    "\n",
    "                            # detach and throw away the oldest time dim (same idea as x_detach)\n",
    "                            x_forcing_detach = x_forcing_batch.detach()[:, :, 1:, ...]\n",
    "\n",
    "                            # concat forcing and static to y_pred, y_pred will be the next input\n",
    "                            y_pred = torch.cat((y_pred, x_forcing_detach), dim=1)\n",
    "                        \n",
    "                        x = torch.cat([x_detach, y_pred], dim=2).detach()\n",
    "                        \n",
    "                    else:\n",
    "                        # use y_pred as the next-hour inputs\n",
    "                        x = y_pred.detach()\n",
    "                        \n",
    "                        if 'y_diag' in batch:\n",
    "                            x = x[:, :-varnum_diag, ...]\n",
    "                            \n",
    "                        # add forcing and static vars to y_pred\n",
    "                        if 'x_forcing_static' in batch:\n",
    "                            x = torch.cat((x, x_forcing_batch), dim=1)\n",
    "                            \n",
    "                loss = criterion(y.to(y_pred.dtype), y_pred)\n",
    "\n",
    "                # Metrics\n",
    "                metrics_dict = metrics(y_pred.float(), y.float())\n",
    "                \n",
    "                for name, value in metrics_dict.items():\n",
    "                    value = torch.Tensor([value]).cuda(self.device, non_blocking=True)\n",
    "                    \n",
    "                    if distributed:\n",
    "                        dist.all_reduce(value, dist.ReduceOp.AVG, async_op=False)\n",
    "                        \n",
    "                    results_dict[f\"valid_{name}\"].append(value[0].item())\n",
    "\n",
    "                batch_loss = torch.Tensor([loss.item()]).cuda(self.device)\n",
    "                \n",
    "                if distributed:\n",
    "                    torch.distributed.barrier()\n",
    "                    \n",
    "                results_dict[\"valid_loss\"].append(batch_loss[0].item())\n",
    "\n",
    "                # print to tqdm\n",
    "                to_print = \"Epoch: {} valid_loss: {:.6f} valid_acc: {:.6f} valid_mae: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    np.mean(results_dict[\"valid_loss\"]),\n",
    "                    np.mean(results_dict[\"valid_acc\"]),\n",
    "                    np.mean(results_dict[\"valid_mae\"])\n",
    "                )\n",
    "                \n",
    "                if self.rank == 0:\n",
    "                    batch_group_generator.set_description(to_print)\n",
    "\n",
    "                if i >= valid_batches_per_epoch and i > 0:\n",
    "                    break\n",
    "\n",
    "        # Shutdown the progbar\n",
    "        batch_group_generator.close()\n",
    "\n",
    "        # Wait for rank-0 process to save the checkpoint above\n",
    "        if distributed:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        # clear the cached memory from the gpu\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        conf,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        optimizer,\n",
    "        train_criterion,\n",
    "        valid_criterion,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "        metrics,\n",
    "        rollout_scheduler=None,\n",
    "        trial=False\n",
    "    ):\n",
    "        # convert $USER to the actual user name\n",
    "        conf['save_loc'] = save_loc = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "        # training hyperparameters\n",
    "        start_epoch = conf['trainer']['start_epoch']\n",
    "        epochs = conf['trainer']['epochs']\n",
    "        skip_validation = conf['trainer']['skip_validation'] if 'skip_validation' in conf['trainer'] else False\n",
    "\n",
    "        # Reload the results saved in the training csv if continuing to train\n",
    "        if start_epoch == 0:\n",
    "            results_dict = defaultdict(list)\n",
    "        else:\n",
    "            results_dict = defaultdict(list)\n",
    "            saved_results = pd.read_csv(os.path.join(save_loc, \"training_log.csv\"))\n",
    "\n",
    "            # Set start_epoch to the length of the training log and train for one epoch\n",
    "            # This is a manual override, you must use train_one_epoch = True\n",
    "            if \"train_one_epoch\" in conf[\"trainer\"] and conf[\"trainer\"][\"train_one_epoch\"]:\n",
    "                start_epoch = len(saved_results)\n",
    "                epochs = start_epoch + 1\n",
    "\n",
    "            for key in saved_results.columns:\n",
    "                if key == \"index\":\n",
    "                    continue\n",
    "                results_dict[key] = list(saved_results[key])\n",
    "\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "\n",
    "            logging.info(f\"Beginning epoch {epoch}\")\n",
    "\n",
    "            if not isinstance(train_loader.dataset, IterableDataset):\n",
    "                train_loader.sampler.set_epoch(epoch)\n",
    "            else:\n",
    "                train_loader.dataset.set_epoch(epoch)\n",
    "                if rollout_scheduler is not None:\n",
    "                    conf['trainer']['stop_rollout'] = rollout_scheduler(epoch, epochs)\n",
    "                    train_loader.dataset.set_rollout_prob(conf['trainer']['stop_rollout'])\n",
    "\n",
    "            ############\n",
    "            #\n",
    "            # Train\n",
    "            #\n",
    "            ############\n",
    "\n",
    "            train_results = self.train_one_epoch(\n",
    "                epoch,\n",
    "                conf,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                train_criterion,\n",
    "                scaler,\n",
    "                scheduler,\n",
    "                metrics\n",
    "            )\n",
    "\n",
    "            ############\n",
    "            #\n",
    "            # Validation\n",
    "            #\n",
    "            ############\n",
    "\n",
    "            if skip_validation:\n",
    "\n",
    "                valid_results = train_results\n",
    "\n",
    "            else:\n",
    "\n",
    "                valid_results = self.validate(\n",
    "                    epoch,\n",
    "                    conf,\n",
    "                    valid_loader,\n",
    "                    valid_criterion,\n",
    "                    metrics\n",
    "                )\n",
    "\n",
    "            #################\n",
    "            #\n",
    "            # Save results\n",
    "            #\n",
    "            #################\n",
    "\n",
    "            # update the learning rate if epoch-by-epoch updates\n",
    "\n",
    "            if conf['trainer']['use_scheduler'] and conf['trainer']['scheduler']['scheduler_type'] == \"plateau\":\n",
    "                scheduler.step(results_dict[\"valid_acc\"][-1])\n",
    "\n",
    "            # Put things into a results dictionary -> dataframe\n",
    "\n",
    "            results_dict[\"epoch\"].append(epoch)\n",
    "            for name in [\"loss\", \"acc\", \"mae\"]:\n",
    "                results_dict[f\"train_{name}\"].append(np.mean(train_results[f\"train_{name}\"]))\n",
    "                results_dict[f\"valid_{name}\"].append(np.mean(valid_results[f\"valid_{name}\"]))\n",
    "            results_dict['train_forecast_len'].append(np.mean(train_results['train_forecast_len']))\n",
    "            results_dict[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "\n",
    "            # Save the dataframe to disk\n",
    "\n",
    "            if trial:\n",
    "                df.to_csv(\n",
    "                    os.path.join(f\"{save_loc}\", \"trial_results\", f\"training_log_{trial.number}.csv\"),\n",
    "                    index=False,\n",
    "                )\n",
    "            else:\n",
    "                df.to_csv(os.path.join(f\"{save_loc}\", \"training_log.csv\"), index=False)\n",
    "\n",
    "            ############\n",
    "            #\n",
    "            # Checkpoint\n",
    "            #\n",
    "            ############\n",
    "\n",
    "            if not trial:\n",
    "\n",
    "                if conf[\"trainer\"][\"mode\"] != \"fsdp\":\n",
    "\n",
    "                    if self.rank == 0:\n",
    "\n",
    "                        # Save the current model\n",
    "\n",
    "                        logging.info(f\"Saving model, optimizer, grad scaler, and learning rate scheduler states to {save_loc}\")\n",
    "\n",
    "                        state_dict = {\n",
    "                            \"epoch\": epoch,\n",
    "                            \"model_state_dict\": self.model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict() if conf[\"trainer\"][\"use_scheduler\"] else None,\n",
    "                            'scaler_state_dict': scaler.state_dict()\n",
    "                        }\n",
    "                        torch.save(state_dict, f\"{save_loc}/checkpoint.pt\")\n",
    "\n",
    "                else:\n",
    "\n",
    "                    logging.info(f\"Saving FSDP model, optimizer, grad scaler, and learning rate scheduler states to {save_loc}\")\n",
    "\n",
    "                    # Initialize the checkpoint I/O handler\n",
    "\n",
    "                    checkpoint_io = TorchFSDPCheckpointIO()\n",
    "\n",
    "                    # Save model and optimizer checkpoints\n",
    "\n",
    "                    checkpoint_io.save_unsharded_model(\n",
    "                        self.model,\n",
    "                        os.path.join(save_loc, \"model_checkpoint.pt\"),\n",
    "                        gather_dtensor=True,\n",
    "                        use_safetensors=False,\n",
    "                        rank=self.rank\n",
    "                    )\n",
    "                    checkpoint_io.save_unsharded_optimizer(\n",
    "                        optimizer,\n",
    "                        os.path.join(save_loc, \"optimizer_checkpoint.pt\"),\n",
    "                        gather_dtensor=True,\n",
    "                        rank=self.rank\n",
    "                    )\n",
    "\n",
    "                    # Still need to save the scheduler and scaler states, just in another file for FSDP\n",
    "\n",
    "                    state_dict = {\n",
    "                        \"epoch\": epoch,\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if conf[\"trainer\"][\"use_scheduler\"] else None,\n",
    "                        'scaler_state_dict': scaler.state_dict()\n",
    "                    }\n",
    "\n",
    "                    torch.save(state_dict, os.path.join(save_loc, \"checkpoint.pt\"))\n",
    "\n",
    "                # This needs updated!\n",
    "                # valid_loss = np.mean(valid_results[\"valid_loss\"])\n",
    "                # # save if this is the best model seen so far\n",
    "                # if (self.rank == 0) and (np.mean(valid_loss) == min(results_dict[\"valid_loss\"])):\n",
    "                #     if conf[\"trainer\"][\"mode\"] == \"ddp\":\n",
    "                #         shutil.copy(f\"{save_loc}/checkpoint_{self.device}.pt\", f\"{save_loc}/best_{self.device}.pt\")\n",
    "                #     elif conf[\"trainer\"][\"mode\"] == \"fsdp\":\n",
    "                #         if os.path.exists(f\"{save_loc}/best\"):\n",
    "                #             shutil.rmtree(f\"{save_loc}/best\")\n",
    "                #         shutil.copytree(f\"{save_loc}/checkpoint\", f\"{save_loc}/best\")\n",
    "                #     else:\n",
    "                #         shutil.copy(f\"{save_loc}/checkpoint.pt\", f\"{save_loc}/best.pt\")\n",
    "\n",
    "            # clear the cached memory from the gpu\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            training_metric = \"train_loss\" if skip_validation else \"valid_loss\"\n",
    "\n",
    "            # Stop training if we have not improved after X epochs (stopping patience)\n",
    "            best_epoch = [\n",
    "                i\n",
    "                for i, j in enumerate(results_dict[training_metric])\n",
    "                if j == min(results_dict[training_metric])\n",
    "            ][0]\n",
    "            offset = epoch - best_epoch\n",
    "            if offset >= conf['trainer']['stopping_patience']:\n",
    "                logging.info(f\"Trial {trial.number} is stopping early\")\n",
    "                break\n",
    "\n",
    "            # Stop training if we get too close to the wall time\n",
    "            if 'stop_after_epoch' in conf['trainer']:\n",
    "                if conf['trainer']['stop_after_epoch']:\n",
    "                    break\n",
    "\n",
    "        training_metric = \"train_loss\" if skip_validation else \"valid_loss\"\n",
    "\n",
    "        best_epoch = [\n",
    "            i for i, j in enumerate(results_dict[training_metric]) if j == min(results_dict[training_metric])\n",
    "        ][0]\n",
    "\n",
    "        result = {k: v[best_epoch] for k, v in results_dict.items()}\n",
    "\n",
    "        if conf[\"trainer\"][\"mode\"] in [\"fsdp\", \"ddp\"]:\n",
    "            cleanup()\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a458aeb-543b-43c2-a160-c6a7199f9966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
